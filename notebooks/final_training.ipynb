{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6baa129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import shap\n",
    "import socket\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b75fc8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "835c5a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from imblearn.combine import SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "42325c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "edfab8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6d8e158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/raw/CIC_IoMT_2024_WiFi_MQTT_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "438d8ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    'Benign': ['Benign_train'],\n",
    "    \n",
    "    'Spoofing': ['ARP_Spoofing_train'],\n",
    "\n",
    "    'Reconnaissance': [\n",
    "        'Recon-OS_Scan_train',\n",
    "        'Recon-Ping_Sweep_train',\n",
    "        'Recon-VulScan_train',\n",
    "        'Recon-Port_Scan_train'\n",
    "    ],\n",
    "    \n",
    "    'MQTT': [\n",
    "        'MQTT-Malformed_Data_train',\n",
    "        'MQTT-DoS-Connect_Flood_train',\n",
    "        'MQTT-DDoS-Publish_Flood_train',\n",
    "        'MQTT-DoS-Publish_Flood_train',\n",
    "        'MQTT-DDoS-Connect_Flood_train'\n",
    "    ],\n",
    "\n",
    "    'DoS': [\n",
    "        'TCP_IP-DoS-TCP1_train', 'TCP_IP-DoS-TCP2_train', 'TCP_IP-DoS-TCP3_train', 'TCP_IP-DoS-TCP4_train',\n",
    "        'TCP_IP-DoS-UDP1_train', 'TCP_IP-DoS-UDP2_train', 'TCP_IP-DoS-UDP3_train', 'TCP_IP-DoS-UDP4_train',\n",
    "        'TCP_IP-DoS-ICMP1_train', 'TCP_IP-DoS-ICMP2_train', 'TCP_IP-DoS-ICMP3_train', 'TCP_IP-DoS-ICMP4_train',\n",
    "        'TCP_IP-DoS-SYN1_train', 'TCP_IP-DoS-SYN2_train', 'TCP_IP-DoS-SYN3_train', 'TCP_IP-DoS-SYN4_train'\n",
    "    ],\n",
    "\n",
    "    'DDoS': [\n",
    "        'TCP_IP-DDoS-TCP1_train', 'TCP_IP-DDoS-TCP2_train', 'TCP_IP-DDoS-TCP3_train', 'TCP_IP-DDoS-TCP4_train',\n",
    "        'TCP_IP-DDoS-UDP1_train', 'TCP_IP-DDoS-UDP2_train', 'TCP_IP-DDoS-UDP3_train', 'TCP_IP-DDoS-UDP4_train',\n",
    "        'TCP_IP-DDoS-UDP5_train', 'TCP_IP-DDoS-UDP6_train', 'TCP_IP-DDoS-UDP7_train', 'TCP_IP-DDoS-UDP8_train',\n",
    "        'TCP_IP-DDoS-ICMP1_train', 'TCP_IP-DDoS-ICMP2_train', 'TCP_IP-DDoS-ICMP3_train', 'TCP_IP-DDoS-ICMP4_train',\n",
    "        'TCP_IP-DDoS-ICMP5_train', 'TCP_IP-DDoS-ICMP6_train', 'TCP_IP-DDoS-ICMP7_train', 'TCP_IP-DDoS-ICMP8_train',\n",
    "        'TCP_IP-DDoS-SYN1_train', 'TCP_IP-DDoS-SYN2_train', 'TCP_IP-DDoS-SYN3_train', 'TCP_IP-DDoS-SYN4_train'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7b134e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_category(label):\n",
    "    for parent, sub_labels in label_mapping.items():\n",
    "        if label in sub_labels:\n",
    "            return parent\n",
    "    return 'Unknown'\n",
    "df['SuperClass'] = df['label'].apply(map_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0df439a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hierarchical Breakdown:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SuperClass</th>\n",
       "      <th>label</th>\n",
       "      <th>Records</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Benign</td>\n",
       "      <td>Benign_train</td>\n",
       "      <td>192732</td>\n",
       "      <td>2.691475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>DDoS</td>\n",
       "      <td>TCP_IP-DDoS-UDP2_train</td>\n",
       "      <td>207295</td>\n",
       "      <td>2.894846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>DDoS</td>\n",
       "      <td>TCP_IP-DDoS-UDP3_train</td>\n",
       "      <td>206604</td>\n",
       "      <td>2.885196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>DDoS</td>\n",
       "      <td>TCP_IP-DDoS-UDP4_train</td>\n",
       "      <td>206343</td>\n",
       "      <td>2.881551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>DDoS</td>\n",
       "      <td>TCP_IP-DDoS-UDP1_train</td>\n",
       "      <td>206170</td>\n",
       "      <td>2.879135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>DDoS</td>\n",
       "      <td>TCP_IP-DDoS-UDP5_train</td>\n",
       "      <td>205507</td>\n",
       "      <td>2.869876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>DDoS</td>\n",
       "      <td>TCP_IP-DDoS-UDP8_train</td>\n",
       "      <td>204105</td>\n",
       "      <td>2.850298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>DDoS</td>\n",
       "      <td>TCP_IP-DDoS-TCP3_train</td>\n",
       "      <td>204075</td>\n",
       "      <td>2.849879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DDoS</td>\n",
       "      <td>TCP_IP-DDoS-SYN2_train</td>\n",
       "      <td>203669</td>\n",
       "      <td>2.844209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>DDoS</td>\n",
       "      <td>TCP_IP-DDoS-TCP1_train</td>\n",
       "      <td>202311</td>\n",
       "      <td>2.825245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>DDoS</td>\n",
       "      <td>TCP_IP-DDoS-UDP6_train</td>\n",
       "      <td>202247</td>\n",
       "      <td>2.824351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>DDoS</td>\n",
       "      <td>TCP_IP-DDoS-SYN3_train</td>\n",
       "      <td>202023</td>\n",
       "      <td>2.821223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>DDoS</td>\n",
       "      <td>TCP_IP-DDoS-TCP4_train</td>\n",
       "      <td>200998</td>\n",
       "      <td>2.806909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DDoS</td>\n",
       "      <td>TCP_IP-DDoS-SYN1_train</td>\n",
       "      <td>199390</td>\n",
       "      <td>2.784453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>DDoS</td>\n",
       "      <td>TCP_IP-DDoS-UDP7_train</td>\n",
       "      <td>197685</td>\n",
       "      <td>2.760643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>DDoS</td>\n",
       "      <td>TCP_IP-DDoS-TCP2_train</td>\n",
       "      <td>197081</td>\n",
       "      <td>2.752209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DDoS</td>\n",
       "      <td>TCP_IP-DDoS-SYN4_train</td>\n",
       "      <td>196880</td>\n",
       "      <td>2.749402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DDoS</td>\n",
       "      <td>TCP_IP-DDoS-ICMP5_train</td>\n",
       "      <td>195032</td>\n",
       "      <td>2.723595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DDoS</td>\n",
       "      <td>TCP_IP-DDoS-ICMP1_train</td>\n",
       "      <td>194938</td>\n",
       "      <td>2.722282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DDoS</td>\n",
       "      <td>TCP_IP-DDoS-ICMP2_train</td>\n",
       "      <td>194818</td>\n",
       "      <td>2.720606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SuperClass                    label  Records  Percentage\n",
       "0      Benign             Benign_train   192732    2.691475\n",
       "18       DDoS   TCP_IP-DDoS-UDP2_train   207295    2.894846\n",
       "19       DDoS   TCP_IP-DDoS-UDP3_train   206604    2.885196\n",
       "20       DDoS   TCP_IP-DDoS-UDP4_train   206343    2.881551\n",
       "17       DDoS   TCP_IP-DDoS-UDP1_train   206170    2.879135\n",
       "21       DDoS   TCP_IP-DDoS-UDP5_train   205507    2.869876\n",
       "24       DDoS   TCP_IP-DDoS-UDP8_train   204105    2.850298\n",
       "15       DDoS   TCP_IP-DDoS-TCP3_train   204075    2.849879\n",
       "10       DDoS   TCP_IP-DDoS-SYN2_train   203669    2.844209\n",
       "13       DDoS   TCP_IP-DDoS-TCP1_train   202311    2.825245\n",
       "22       DDoS   TCP_IP-DDoS-UDP6_train   202247    2.824351\n",
       "11       DDoS   TCP_IP-DDoS-SYN3_train   202023    2.821223\n",
       "16       DDoS   TCP_IP-DDoS-TCP4_train   200998    2.806909\n",
       "9        DDoS   TCP_IP-DDoS-SYN1_train   199390    2.784453\n",
       "23       DDoS   TCP_IP-DDoS-UDP7_train   197685    2.760643\n",
       "14       DDoS   TCP_IP-DDoS-TCP2_train   197081    2.752209\n",
       "12       DDoS   TCP_IP-DDoS-SYN4_train   196880    2.749402\n",
       "5        DDoS  TCP_IP-DDoS-ICMP5_train   195032    2.723595\n",
       "1        DDoS  TCP_IP-DDoS-ICMP1_train   194938    2.722282\n",
       "2        DDoS  TCP_IP-DDoS-ICMP2_train   194818    2.720606"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SuperClass</th>\n",
       "      <th>Records</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Benign</td>\n",
       "      <td>192732</td>\n",
       "      <td>2.691475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DDoS</td>\n",
       "      <td>4779859</td>\n",
       "      <td>66.750060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DoS</td>\n",
       "      <td>1805529</td>\n",
       "      <td>25.213959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MQTT</td>\n",
       "      <td>262938</td>\n",
       "      <td>3.671892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reconnaissance</td>\n",
       "      <td>103726</td>\n",
       "      <td>1.448519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Spoofing</td>\n",
       "      <td>16047</td>\n",
       "      <td>0.224094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       SuperClass  Records  Percentage\n",
       "0          Benign   192732    2.691475\n",
       "1            DDoS  4779859   66.750060\n",
       "2             DoS  1805529   25.213959\n",
       "3            MQTT   262938    3.671892\n",
       "4  Reconnaissance   103726    1.448519\n",
       "5        Spoofing    16047    0.224094"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grouped = df.groupby(['SuperClass', 'label']).size().reset_index(name='Records')\n",
    "total = len(df)\n",
    "grouped['Percentage'] = (grouped['Records'] / total) * 100\n",
    "\n",
    "summary = grouped.groupby('SuperClass')['Records'].sum().reset_index()\n",
    "summary['Percentage'] = (summary['Records'] / total) * 100\n",
    "\n",
    "print(\"Hierarchical Breakdown:\")\n",
    "display(grouped.sort_values(['SuperClass', 'Records'], ascending=[True, False]).head(20))\n",
    "\n",
    "print(\"Category Summary:\")\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fe59164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_COLS = ['label', 'SuperClass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3815989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_COLS = ['Boot_Time_with_date', 'RTime', 'SrcAddr', 'DstAddr', 'SrcMac', 'DstMac', 'IMEI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3921eea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "RATE_COLS = ['Rate', 'Srate', 'Drate'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2fbbe388",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COUNTS = {\n",
    "    'DDoS': 500_000,\n",
    "    'DoS':  500_000,\n",
    "    'Benign': 300_000,\n",
    "    'MQTT': 300_000,\n",
    "    'Reconnaissance': 200_000,\n",
    "    'Spoofing': 200_000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "17c37776",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [c for c in DROP_COLS if c in df.columns]\n",
    "if cols_to_drop:\n",
    "    print(\"Dropping ID/time columns:\", cols_to_drop)\n",
    "    df = df.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ce778e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Header_Length</th>\n",
       "      <th>Protocol Type</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Rate</th>\n",
       "      <th>Srate</th>\n",
       "      <th>Drate</th>\n",
       "      <th>fin_flag_number</th>\n",
       "      <th>syn_flag_number</th>\n",
       "      <th>rst_flag_number</th>\n",
       "      <th>psh_flag_number</th>\n",
       "      <th>...</th>\n",
       "      <th>Tot size</th>\n",
       "      <th>IAT</th>\n",
       "      <th>Number</th>\n",
       "      <th>Magnitue</th>\n",
       "      <th>Radius</th>\n",
       "      <th>Covariance</th>\n",
       "      <th>Variance</th>\n",
       "      <th>Weight</th>\n",
       "      <th>label</th>\n",
       "      <th>SuperClass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>683.50</td>\n",
       "      <td>17.00</td>\n",
       "      <td>64.0</td>\n",
       "      <td>553148.440</td>\n",
       "      <td>553148.440</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>50.00</td>\n",
       "      <td>101635944.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>141.5</td>\n",
       "      <td>TCP_IP-DDoS-UDP2_train</td>\n",
       "      <td>DDoS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1745.92</td>\n",
       "      <td>16.12</td>\n",
       "      <td>64.0</td>\n",
       "      <td>29919.545</td>\n",
       "      <td>29919.545</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>52.28</td>\n",
       "      <td>84696616.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.05</td>\n",
       "      <td>2.152</td>\n",
       "      <td>11.785338</td>\n",
       "      <td>0.23</td>\n",
       "      <td>141.5</td>\n",
       "      <td>TCP_IP-DDoS-UDP2_train</td>\n",
       "      <td>DDoS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2830.36</td>\n",
       "      <td>15.79</td>\n",
       "      <td>64.0</td>\n",
       "      <td>27075.605</td>\n",
       "      <td>27075.605</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>...</td>\n",
       "      <td>53.25</td>\n",
       "      <td>84696616.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.35</td>\n",
       "      <td>10.555</td>\n",
       "      <td>113.675460</td>\n",
       "      <td>0.53</td>\n",
       "      <td>141.5</td>\n",
       "      <td>TCP_IP-DDoS-UDP2_train</td>\n",
       "      <td>DDoS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4087.50</td>\n",
       "      <td>16.89</td>\n",
       "      <td>64.0</td>\n",
       "      <td>19660.156</td>\n",
       "      <td>19660.156</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>50.16</td>\n",
       "      <td>84696616.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.03</td>\n",
       "      <td>1.236</td>\n",
       "      <td>4.490081</td>\n",
       "      <td>0.18</td>\n",
       "      <td>141.5</td>\n",
       "      <td>TCP_IP-DDoS-UDP2_train</td>\n",
       "      <td>DDoS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3916.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>64.0</td>\n",
       "      <td>270322.780</td>\n",
       "      <td>270322.780</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>50.00</td>\n",
       "      <td>84696616.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>141.5</td>\n",
       "      <td>TCP_IP-DDoS-UDP2_train</td>\n",
       "      <td>DDoS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Header_Length  Protocol Type  Duration        Rate       Srate  Drate  \\\n",
       "0         683.50          17.00      64.0  553148.440  553148.440      0   \n",
       "1        1745.92          16.12      64.0   29919.545   29919.545      0   \n",
       "2        2830.36          15.79      64.0   27075.605   27075.605      0   \n",
       "3        4087.50          16.89      64.0   19660.156   19660.156      0   \n",
       "4        3916.00          17.00      64.0  270322.780  270322.780      0   \n",
       "\n",
       "   fin_flag_number  syn_flag_number  rst_flag_number  psh_flag_number  ...  \\\n",
       "0              0.0              0.0              0.0             0.00  ...   \n",
       "1              0.0              0.0              0.0             0.04  ...   \n",
       "2              0.0              0.0              0.0             0.06  ...   \n",
       "3              0.0              0.0              0.0             0.00  ...   \n",
       "4              0.0              0.0              0.0             0.00  ...   \n",
       "\n",
       "   Tot size          IAT  Number  Magnitue  Radius  Covariance  Variance  \\\n",
       "0     50.00  101635944.0     9.5     10.00   0.000    0.000000      0.00   \n",
       "1     52.28   84696616.0     9.5     10.05   2.152   11.785338      0.23   \n",
       "2     53.25   84696616.0     9.5     10.35  10.555  113.675460      0.53   \n",
       "3     50.16   84696616.0     9.5     10.03   1.236    4.490081      0.18   \n",
       "4     50.00   84696616.0     9.5     10.00   0.000    0.000000      0.00   \n",
       "\n",
       "   Weight                   label  SuperClass  \n",
       "0   141.5  TCP_IP-DDoS-UDP2_train        DDoS  \n",
       "1   141.5  TCP_IP-DDoS-UDP2_train        DDoS  \n",
       "2   141.5  TCP_IP-DDoS-UDP2_train        DDoS  \n",
       "3   141.5  TCP_IP-DDoS-UDP2_train        DDoS  \n",
       "4   141.5  TCP_IP-DDoS-UDP2_train        DDoS  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "529b2b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SuperClass\n",
       "DDoS              4779859\n",
       "DoS               1805529\n",
       "MQTT               262938\n",
       "Benign             192732\n",
       "Reconnaissance     103726\n",
       "Spoofing            16047\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['SuperClass'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "40649277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count: 45\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [c for c in df.columns if c not in ID_COLS]\n",
    "print(f\"Feature count: {len(feature_cols)}\")\n",
    "\n",
    "X_raw = df[feature_cols].copy()\n",
    "y_raw = df['SuperClass'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5327c58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numeric = [c for c in X_raw.columns if not np.issubdtype(X_raw[c].dtype, np.number)]\n",
    "if non_numeric:\n",
    "    print(\"Non-numeric columns detected - converting to numeric (if possible) or dropping:\", non_numeric)\n",
    "    for c in non_numeric:\n",
    "        X_raw[c] = pd.to_numeric(X_raw[c], errors='coerce')\n",
    "    non_numeric_after = [c for c in X_raw.columns if not np.issubdtype(X_raw[c].dtype, np.number)]\n",
    "    if non_numeric_after:\n",
    "        print(\"Dropping columns still non-numeric:\", non_numeric_after)\n",
    "        X_raw = X_raw.drop(columns=non_numeric_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e7ba089e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying MinMax normalization to rate cols: ['Rate', 'Srate', 'Drate']\n"
     ]
    }
   ],
   "source": [
    "rate_cols_present = [c for c in RATE_COLS if c in X_raw.columns]\n",
    "if rate_cols_present:\n",
    "    print(\"Applying MinMax normalization to rate cols:\", rate_cols_present)\n",
    "    mms = MinMaxScaler()\n",
    "    X_raw[rate_cols_present] = mms.fit_transform(X_raw[rate_cols_present])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f2bb3dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_raw)\n",
    "del X_raw\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ed9c8564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7160831, 45)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b9398b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7160831,)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0e2a700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_raw, test_size=0.2, stratify=y_raw, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d45f9df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def hybrid_balance_safe(df, label_col='SuperClass', target_n=60000, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    balanced_parts = []\n",
    "    num_cols = [c for c in df.columns if c not in ['label', label_col]]\n",
    "\n",
    "    for cls, group in df.groupby(label_col):\n",
    "        n_avail = len(group)\n",
    "        print(f\"Class {cls} -> available: {n_avail}\")\n",
    "\n",
    "        if n_avail > target_n:\n",
    "            # Downsample\n",
    "            sampled = group.sample(n=target_n, random_state=random_state)\n",
    "\n",
    "        elif n_avail == target_n:\n",
    "            sampled = group\n",
    "\n",
    "        elif n_avail >= 5:\n",
    "            # Gaussian noise augmentation for numeric features\n",
    "            X = group[num_cols].copy()\n",
    "            n_to_gen = target_n - n_avail\n",
    "\n",
    "            # Compute per-column stds and add jitter\n",
    "            stds = X.std(axis=0, ddof=0).replace(0, 1e-6)\n",
    "            synth_samples = X.sample(n=n_to_gen, replace=True, random_state=random_state)\n",
    "            noise = np.random.normal(0, 0.01, synth_samples.shape) * stds.values\n",
    "            synth_samples = synth_samples + noise\n",
    "\n",
    "            synth_df = synth_samples.copy()\n",
    "            synth_df[label_col] = cls\n",
    "            synth_df['label'] = group['label'].sample(\n",
    "                n=n_to_gen, replace=True, random_state=random_state\n",
    "            ).values\n",
    "\n",
    "            sampled = pd.concat([group, synth_df], ignore_index=True)\n",
    "\n",
    "        else:\n",
    "            # Tiny class — replicate with replacement\n",
    "            sampled = group.sample(n=target_n, replace=True, random_state=random_state)\n",
    "\n",
    "        balanced_parts.append(sampled)\n",
    "\n",
    "    balanced_df = (\n",
    "        pd.concat(balanced_parts)\n",
    "        .sample(frac=1, random_state=random_state)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    print(\"\\n✅ Done! New distribution:\")\n",
    "    print(balanced_df[label_col].value_counts())\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "762223eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Benign -> available: 192732\n",
      "Class DDoS -> available: 4779859\n",
      "Class DoS -> available: 1805529\n",
      "Class MQTT -> available: 262938\n",
      "Class Reconnaissance -> available: 103726\n",
      "Class Spoofing -> available: 16047\n",
      "\n",
      "✅ Done! New distribution:\n",
      "SuperClass\n",
      "Spoofing          16047\n",
      "Benign            16047\n",
      "DDoS              16047\n",
      "Reconnaissance    16047\n",
      "DoS               16047\n",
      "MQTT              16047\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "subset_df = hybrid_balance_safe(df, label_col='SuperClass', target_n=16047)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d3025e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(4180)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "355d10e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "        Benign       0.97      0.96      0.97      3209\n",
      "          DDoS       1.00      1.00      1.00      3209\n",
      "           DoS       1.00      1.00      1.00      3210\n",
      "          MQTT       1.00      0.99      1.00      3210\n",
      "Reconnaissance       0.99      0.97      0.98      3210\n",
      "      Spoofing       0.94      0.98      0.96      3209\n",
      "\n",
      "      accuracy                           0.98     19257\n",
      "     macro avg       0.98      0.98      0.98     19257\n",
      "  weighted avg       0.98      0.98      0.98     19257\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = subset_df.drop(columns=['label','SuperClass'], errors='ignore')\n",
    "y = subset_df['SuperClass']\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_enc, test_size=0.2, stratify=y_enc, random_state=42)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0cf9e7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features and labels\n",
    "df_combined = pd.concat([X, y], axis=1)\n",
    "\n",
    "df_combined = df_combined.drop_duplicates()\n",
    "\n",
    "X_subset = df_combined.drop(\"SuperClass\", axis=1)\n",
    "y_subset = df_combined[\"SuperClass\"]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_subset, y_subset, stratify=y_subset, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9e7832aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "print(X_train.duplicated().sum(), X_test.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9c475cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73658, 45)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "83322feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_test_enc = le.transform(y_test)\n",
    "num_classes = len(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0d2f4209",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaledd = scaler.fit_transform(X_train)\n",
    "X_test_scaledd = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8c501b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = X_train_scaledd[..., np.newaxis]\n",
    "X_test_scaled = X_test_scaledd[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8fcd6a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "        Benign       0.97      0.96      0.96      3210\n",
      "          DDoS       1.00      1.00      1.00      2773\n",
      "           DoS       1.00      1.00      1.00      2962\n",
      "          MQTT       1.00      0.99      0.99      3209\n",
      "Reconnaissance       0.99      0.97      0.98      3059\n",
      "      Spoofing       0.93      0.98      0.95      3202\n",
      "\n",
      "      accuracy                           0.98     18415\n",
      "     macro avg       0.98      0.98      0.98     18415\n",
      "  weighted avg       0.98      0.98      0.98     18415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "067ed1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "importances = rf.feature_importances_\n",
    "feat_imp = pd.Series(importances, index=X_train.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=feat_imp[:20], y=feat_imp.index[:20])\n",
    "plt.title(\"Top 20 Important Features - Random Forest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c4bed33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    n_jobs=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "897b521c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=300, n_jobs=4,\n",
       "              num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>XGBClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.0.0/python/python_api.html#xgboost.XGBClassifier\">?<span>Documentation for XGBClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('objective',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">objective&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;multi:softprob&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('base_score',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">base_score&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('booster',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">booster&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('callbacks',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">callbacks&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('colsample_bylevel',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">colsample_bylevel&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('colsample_bynode',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">colsample_bynode&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('colsample_bytree',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">colsample_bytree&nbsp;</td>\n",
       "            <td class=\"value\">0.8</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('device',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">device&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('early_stopping_rounds',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">early_stopping_rounds&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('enable_categorical',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">enable_categorical&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('eval_metric',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">eval_metric&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;logloss&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('feature_types',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">feature_types&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('feature_weights',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">feature_weights&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('gamma',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">gamma&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('grow_policy',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">grow_policy&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('importance_type',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">importance_type&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('interaction_constraints',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">interaction_constraints&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('learning_rate',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">learning_rate&nbsp;</td>\n",
       "            <td class=\"value\">0.05</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_bin',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_bin&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_cat_threshold',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_cat_threshold&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_cat_to_onehot',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_cat_to_onehot&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_delta_step',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_delta_step&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_depth',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_depth&nbsp;</td>\n",
       "            <td class=\"value\">6</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_leaves',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_leaves&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('min_child_weight',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">min_child_weight&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('missing',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">missing&nbsp;</td>\n",
       "            <td class=\"value\">nan</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('monotone_constraints',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">monotone_constraints&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('multi_strategy',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">multi_strategy&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_estimators',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_estimators&nbsp;</td>\n",
       "            <td class=\"value\">300</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_jobs&nbsp;</td>\n",
       "            <td class=\"value\">4</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('num_parallel_tree',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">num_parallel_tree&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('random_state',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">random_state&nbsp;</td>\n",
       "            <td class=\"value\">42</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('reg_alpha',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">reg_alpha&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('reg_lambda',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">reg_lambda&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('sampling_method',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">sampling_method&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('scale_pos_weight',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">scale_pos_weight&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('subsample',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">subsample&nbsp;</td>\n",
       "            <td class=\"value\">0.8</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tree_method',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">tree_method&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('validate_parameters',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">validate_parameters&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbosity',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbosity&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='logloss',\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=300, n_jobs=4,\n",
       "              num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf.fit(X_train_scaledd, y_train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "673bd164",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_clf.predict(X_test_scaledd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "768b4de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "        Benign       0.97      0.96      0.97      3210\n",
      "          DDoS       1.00      1.00      1.00      2773\n",
      "           DoS       1.00      1.00      1.00      2962\n",
      "          MQTT       1.00      0.99      1.00      3209\n",
      "Reconnaissance       1.00      0.97      0.98      3059\n",
      "      Spoofing       0.93      0.98      0.96      3202\n",
      "\n",
      "      accuracy                           0.98     18415\n",
      "     macro avg       0.98      0.98      0.98     18415\n",
      "  weighted avg       0.98      0.98      0.98     18415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_enc, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9cc662a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "decc5768",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_enc, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_enc, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8b74de08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b0cc68ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bd406aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneDCNN(nn.Module):\n",
    "    def __init__(self, input_len, num_classes):\n",
    "        super(OneDCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "        self.global_pool = nn.AdaptiveMaxPool1d(1)   # replaces GlobalMaxPooling1D\n",
    "        self.fc1 = nn.Linear(128, 128)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, channels) → need to permute for Conv1d\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6db377a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneDCNN(\n",
      "  (conv1): Conv1d(1, 64, kernel_size=(3,), stride=(1,))\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv1d(64, 128, kernel_size=(3,), stride=(1,))\n",
      "  (global_pool): AdaptiveMaxPool1d(output_size=1)\n",
      "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_len = X_train_scaled.shape[1]\n",
    "model = OneDCNN(input_len, num_classes).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c3ef6d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9b20051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n",
    "    \"epoch\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "    \"precision\": [],\n",
    "    \"recall\": [],\n",
    "    \"f1\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9faaba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "92f414b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300] - Train Loss: 0.3416, Train Acc: 0.8328 | Val Loss: 0.3413, Val Acc: 0.8352 | F1: 0.8243\n",
      "Epoch [2/300] - Train Loss: 0.3400, Train Acc: 0.8338 | Val Loss: 0.3437, Val Acc: 0.8260 | F1: 0.8184\n",
      "Epoch [3/300] - Train Loss: 0.3404, Train Acc: 0.8354 | Val Loss: 0.3379, Val Acc: 0.8338 | F1: 0.8279\n",
      "Epoch [4/300] - Train Loss: 0.3383, Train Acc: 0.8358 | Val Loss: 0.3466, Val Acc: 0.8293 | F1: 0.8247\n",
      "Epoch [5/300] - Train Loss: 0.3360, Train Acc: 0.8373 | Val Loss: 0.3386, Val Acc: 0.8342 | F1: 0.8253\n",
      "Epoch [6/300] - Train Loss: 0.3387, Train Acc: 0.8373 | Val Loss: 0.3349, Val Acc: 0.8377 | F1: 0.8318\n",
      "Epoch [7/300] - Train Loss: 0.3357, Train Acc: 0.8369 | Val Loss: 0.3372, Val Acc: 0.8330 | F1: 0.8282\n",
      "Epoch [8/300] - Train Loss: 0.3334, Train Acc: 0.8372 | Val Loss: 0.3357, Val Acc: 0.8319 | F1: 0.8271\n",
      "Epoch [9/300] - Train Loss: 0.3329, Train Acc: 0.8375 | Val Loss: 0.3378, Val Acc: 0.8336 | F1: 0.8227\n",
      "Epoch [10/300] - Train Loss: 0.3337, Train Acc: 0.8379 | Val Loss: 0.3327, Val Acc: 0.8367 | F1: 0.8258\n",
      "Epoch [11/300] - Train Loss: 0.3300, Train Acc: 0.8394 | Val Loss: 0.3310, Val Acc: 0.8392 | F1: 0.8343\n",
      "Epoch [12/300] - Train Loss: 0.3372, Train Acc: 0.8377 | Val Loss: 0.3286, Val Acc: 0.8401 | F1: 0.8354\n",
      "Epoch [13/300] - Train Loss: 0.3264, Train Acc: 0.8419 | Val Loss: 0.3304, Val Acc: 0.8387 | F1: 0.8335\n",
      "Epoch [14/300] - Train Loss: 0.3282, Train Acc: 0.8414 | Val Loss: 0.3316, Val Acc: 0.8385 | F1: 0.8264\n",
      "Epoch [15/300] - Train Loss: 0.3273, Train Acc: 0.8407 | Val Loss: 0.3306, Val Acc: 0.8396 | F1: 0.8286\n",
      "Epoch [16/300] - Train Loss: 0.3262, Train Acc: 0.8426 | Val Loss: 0.3293, Val Acc: 0.8389 | F1: 0.8292\n",
      "Epoch [17/300] - Train Loss: 0.3257, Train Acc: 0.8424 | Val Loss: 0.3250, Val Acc: 0.8423 | F1: 0.8375\n",
      "Epoch [18/300] - Train Loss: 0.3253, Train Acc: 0.8430 | Val Loss: 0.3305, Val Acc: 0.8405 | F1: 0.8355\n",
      "Epoch [19/300] - Train Loss: 0.3245, Train Acc: 0.8425 | Val Loss: 0.3279, Val Acc: 0.8375 | F1: 0.8322\n",
      "Epoch [20/300] - Train Loss: 0.3233, Train Acc: 0.8438 | Val Loss: 0.3243, Val Acc: 0.8436 | F1: 0.8390\n",
      "Epoch [21/300] - Train Loss: 0.3230, Train Acc: 0.8432 | Val Loss: 0.3265, Val Acc: 0.8428 | F1: 0.8377\n",
      "Epoch [22/300] - Train Loss: 0.3226, Train Acc: 0.8446 | Val Loss: 0.3301, Val Acc: 0.8395 | F1: 0.8348\n",
      "Epoch [23/300] - Train Loss: 0.3218, Train Acc: 0.8443 | Val Loss: 0.3235, Val Acc: 0.8424 | F1: 0.8317\n",
      "Epoch [24/300] - Train Loss: 0.3202, Train Acc: 0.8446 | Val Loss: 0.3286, Val Acc: 0.8409 | F1: 0.8319\n",
      "Epoch [25/300] - Train Loss: 0.3191, Train Acc: 0.8455 | Val Loss: 0.3234, Val Acc: 0.8406 | F1: 0.8310\n",
      "Epoch [26/300] - Train Loss: 0.3207, Train Acc: 0.8440 | Val Loss: 0.3232, Val Acc: 0.8417 | F1: 0.8306\n",
      "Epoch [27/300] - Train Loss: 0.3184, Train Acc: 0.8460 | Val Loss: 0.3254, Val Acc: 0.8421 | F1: 0.8374\n",
      "Epoch [28/300] - Train Loss: 0.3174, Train Acc: 0.8460 | Val Loss: 0.3263, Val Acc: 0.8394 | F1: 0.8272\n",
      "Epoch [29/300] - Train Loss: 0.3185, Train Acc: 0.8449 | Val Loss: 0.3303, Val Acc: 0.8376 | F1: 0.8227\n",
      "Epoch [30/300] - Train Loss: 0.3180, Train Acc: 0.8464 | Val Loss: 0.3217, Val Acc: 0.8440 | F1: 0.8393\n",
      "Epoch [31/300] - Train Loss: 0.3166, Train Acc: 0.8458 | Val Loss: 0.3243, Val Acc: 0.8451 | F1: 0.8389\n",
      "Epoch [32/300] - Train Loss: 0.3164, Train Acc: 0.8477 | Val Loss: 0.3227, Val Acc: 0.8425 | F1: 0.8331\n",
      "Epoch [33/300] - Train Loss: 0.3155, Train Acc: 0.8465 | Val Loss: 0.3285, Val Acc: 0.8445 | F1: 0.8338\n",
      "Epoch [34/300] - Train Loss: 0.3156, Train Acc: 0.8460 | Val Loss: 0.3249, Val Acc: 0.8421 | F1: 0.8310\n",
      "Epoch [35/300] - Train Loss: 0.3152, Train Acc: 0.8480 | Val Loss: 0.3318, Val Acc: 0.8388 | F1: 0.8272\n",
      "Epoch [36/300] - Train Loss: 0.3133, Train Acc: 0.8486 | Val Loss: 0.3225, Val Acc: 0.8441 | F1: 0.8391\n",
      "Epoch [37/300] - Train Loss: 0.3137, Train Acc: 0.8477 | Val Loss: 0.3238, Val Acc: 0.8419 | F1: 0.8322\n",
      "Epoch [38/300] - Train Loss: 0.3138, Train Acc: 0.8464 | Val Loss: 0.3240, Val Acc: 0.8476 | F1: 0.8405\n",
      "Epoch [39/300] - Train Loss: 0.3124, Train Acc: 0.8488 | Val Loss: 0.3200, Val Acc: 0.8453 | F1: 0.8359\n",
      "Epoch [40/300] - Train Loss: 0.3134, Train Acc: 0.8479 | Val Loss: 0.3206, Val Acc: 0.8466 | F1: 0.8390\n",
      "Epoch [41/300] - Train Loss: 0.3117, Train Acc: 0.8482 | Val Loss: 0.3208, Val Acc: 0.8421 | F1: 0.8313\n",
      "Epoch [42/300] - Train Loss: 0.3118, Train Acc: 0.8485 | Val Loss: 0.3241, Val Acc: 0.8462 | F1: 0.8412\n",
      "Epoch [43/300] - Train Loss: 0.3114, Train Acc: 0.8497 | Val Loss: 0.3190, Val Acc: 0.8451 | F1: 0.8403\n",
      "Epoch [44/300] - Train Loss: 0.3099, Train Acc: 0.8498 | Val Loss: 0.3197, Val Acc: 0.8466 | F1: 0.8420\n",
      "Epoch [45/300] - Train Loss: 0.3093, Train Acc: 0.8499 | Val Loss: 0.3216, Val Acc: 0.8422 | F1: 0.8297\n",
      "Epoch [46/300] - Train Loss: 0.3093, Train Acc: 0.8507 | Val Loss: 0.3262, Val Acc: 0.8430 | F1: 0.8378\n",
      "Epoch [47/300] - Train Loss: 0.3094, Train Acc: 0.8506 | Val Loss: 0.3197, Val Acc: 0.8464 | F1: 0.8412\n",
      "Epoch [48/300] - Train Loss: 0.3086, Train Acc: 0.8502 | Val Loss: 0.3252, Val Acc: 0.8420 | F1: 0.8372\n",
      "Epoch [49/300] - Train Loss: 0.3072, Train Acc: 0.8501 | Val Loss: 0.3233, Val Acc: 0.8460 | F1: 0.8413\n",
      "Epoch [50/300] - Train Loss: 0.3087, Train Acc: 0.8510 | Val Loss: 0.3187, Val Acc: 0.8473 | F1: 0.8425\n",
      "Epoch [51/300] - Train Loss: 0.3077, Train Acc: 0.8510 | Val Loss: 0.3223, Val Acc: 0.8464 | F1: 0.8409\n",
      "Epoch [52/300] - Train Loss: 0.3079, Train Acc: 0.8505 | Val Loss: 0.3200, Val Acc: 0.8466 | F1: 0.8395\n",
      "Epoch [53/300] - Train Loss: 0.3068, Train Acc: 0.8502 | Val Loss: 0.3222, Val Acc: 0.8427 | F1: 0.8378\n",
      "Epoch [54/300] - Train Loss: 0.3074, Train Acc: 0.8500 | Val Loss: 0.3161, Val Acc: 0.8474 | F1: 0.8389\n",
      "Epoch [55/300] - Train Loss: 0.3073, Train Acc: 0.8499 | Val Loss: 0.3218, Val Acc: 0.8448 | F1: 0.8376\n",
      "Epoch [56/300] - Train Loss: 0.3059, Train Acc: 0.8504 | Val Loss: 0.3198, Val Acc: 0.8484 | F1: 0.8432\n",
      "Epoch [57/300] - Train Loss: 0.3054, Train Acc: 0.8511 | Val Loss: 0.3198, Val Acc: 0.8445 | F1: 0.8347\n",
      "Epoch [58/300] - Train Loss: 0.3051, Train Acc: 0.8512 | Val Loss: 0.3174, Val Acc: 0.8475 | F1: 0.8424\n",
      "Epoch [59/300] - Train Loss: 0.3040, Train Acc: 0.8516 | Val Loss: 0.3169, Val Acc: 0.8522 | F1: 0.8466\n",
      "Epoch [60/300] - Train Loss: 0.3067, Train Acc: 0.8516 | Val Loss: 0.3202, Val Acc: 0.8459 | F1: 0.8370\n",
      "Epoch [61/300] - Train Loss: 0.3030, Train Acc: 0.8528 | Val Loss: 0.3172, Val Acc: 0.8495 | F1: 0.8447\n",
      "Epoch [62/300] - Train Loss: 0.3045, Train Acc: 0.8527 | Val Loss: 0.3153, Val Acc: 0.8494 | F1: 0.8443\n",
      "Epoch [63/300] - Train Loss: 0.3032, Train Acc: 0.8521 | Val Loss: 0.3162, Val Acc: 0.8485 | F1: 0.8434\n",
      "Epoch [64/300] - Train Loss: 0.3044, Train Acc: 0.8515 | Val Loss: 0.3192, Val Acc: 0.8460 | F1: 0.8352\n",
      "Epoch [65/300] - Train Loss: 0.3039, Train Acc: 0.8516 | Val Loss: 0.3192, Val Acc: 0.8473 | F1: 0.8426\n",
      "Epoch [66/300] - Train Loss: 0.3017, Train Acc: 0.8530 | Val Loss: 0.3186, Val Acc: 0.8479 | F1: 0.8383\n",
      "Epoch [67/300] - Train Loss: 0.3024, Train Acc: 0.8524 | Val Loss: 0.3201, Val Acc: 0.8493 | F1: 0.8411\n",
      "Epoch [68/300] - Train Loss: 0.3029, Train Acc: 0.8536 | Val Loss: 0.3142, Val Acc: 0.8490 | F1: 0.8403\n",
      "Epoch [69/300] - Train Loss: 0.3024, Train Acc: 0.8532 | Val Loss: 0.3161, Val Acc: 0.8483 | F1: 0.8430\n",
      "Epoch [70/300] - Train Loss: 0.3022, Train Acc: 0.8523 | Val Loss: 0.3159, Val Acc: 0.8500 | F1: 0.8450\n",
      "Epoch [71/300] - Train Loss: 0.3012, Train Acc: 0.8528 | Val Loss: 0.3202, Val Acc: 0.8481 | F1: 0.8384\n",
      "Epoch [72/300] - Train Loss: 0.3010, Train Acc: 0.8536 | Val Loss: 0.3193, Val Acc: 0.8478 | F1: 0.8431\n",
      "Epoch [73/300] - Train Loss: 0.3012, Train Acc: 0.8534 | Val Loss: 0.3192, Val Acc: 0.8508 | F1: 0.8450\n",
      "Epoch [74/300] - Train Loss: 0.3007, Train Acc: 0.8532 | Val Loss: 0.3186, Val Acc: 0.8513 | F1: 0.8427\n",
      "Epoch [75/300] - Train Loss: 0.3008, Train Acc: 0.8534 | Val Loss: 0.3163, Val Acc: 0.8504 | F1: 0.8420\n",
      "Epoch [76/300] - Train Loss: 0.2998, Train Acc: 0.8545 | Val Loss: 0.3210, Val Acc: 0.8478 | F1: 0.8428\n",
      "Epoch [77/300] - Train Loss: 0.2990, Train Acc: 0.8544 | Val Loss: 0.3181, Val Acc: 0.8478 | F1: 0.8382\n",
      "Epoch [78/300] - Train Loss: 0.3014, Train Acc: 0.8538 | Val Loss: 0.3145, Val Acc: 0.8502 | F1: 0.8452\n",
      "Epoch [79/300] - Train Loss: 0.3008, Train Acc: 0.8539 | Val Loss: 0.3158, Val Acc: 0.8497 | F1: 0.8393\n",
      "Epoch [80/300] - Train Loss: 0.2989, Train Acc: 0.8548 | Val Loss: 0.3152, Val Acc: 0.8520 | F1: 0.8463\n",
      "Epoch [81/300] - Train Loss: 0.2999, Train Acc: 0.8525 | Val Loss: 0.3158, Val Acc: 0.8499 | F1: 0.8413\n",
      "Epoch [82/300] - Train Loss: 0.2986, Train Acc: 0.8548 | Val Loss: 0.3163, Val Acc: 0.8503 | F1: 0.8452\n",
      "Epoch [83/300] - Train Loss: 0.2993, Train Acc: 0.8549 | Val Loss: 0.3145, Val Acc: 0.8491 | F1: 0.8441\n",
      "Epoch [84/300] - Train Loss: 0.2984, Train Acc: 0.8543 | Val Loss: 0.3181, Val Acc: 0.8482 | F1: 0.8436\n",
      "Epoch [85/300] - Train Loss: 0.2991, Train Acc: 0.8541 | Val Loss: 0.3195, Val Acc: 0.8484 | F1: 0.8382\n",
      "Epoch [86/300] - Train Loss: 0.2973, Train Acc: 0.8547 | Val Loss: 0.3191, Val Acc: 0.8490 | F1: 0.8442\n",
      "Epoch [87/300] - Train Loss: 0.2975, Train Acc: 0.8553 | Val Loss: 0.3151, Val Acc: 0.8493 | F1: 0.8442\n",
      "Epoch [88/300] - Train Loss: 0.2971, Train Acc: 0.8561 | Val Loss: 0.3181, Val Acc: 0.8499 | F1: 0.8401\n",
      "Epoch [89/300] - Train Loss: 0.2979, Train Acc: 0.8544 | Val Loss: 0.3155, Val Acc: 0.8495 | F1: 0.8391\n",
      "Epoch [90/300] - Train Loss: 0.2966, Train Acc: 0.8555 | Val Loss: 0.3218, Val Acc: 0.8463 | F1: 0.8413\n",
      "Epoch [91/300] - Train Loss: 0.2972, Train Acc: 0.8550 | Val Loss: 0.3196, Val Acc: 0.8480 | F1: 0.8384\n",
      "Epoch [92/300] - Train Loss: 0.2968, Train Acc: 0.8545 | Val Loss: 0.3147, Val Acc: 0.8508 | F1: 0.8421\n",
      "Epoch [93/300] - Train Loss: 0.2973, Train Acc: 0.8555 | Val Loss: 0.3160, Val Acc: 0.8497 | F1: 0.8449\n",
      "Epoch [94/300] - Train Loss: 0.2987, Train Acc: 0.8542 | Val Loss: 0.3151, Val Acc: 0.8507 | F1: 0.8418\n",
      "Epoch [95/300] - Train Loss: 0.2956, Train Acc: 0.8558 | Val Loss: 0.3198, Val Acc: 0.8545 | F1: 0.8491\n",
      "Epoch [96/300] - Train Loss: 0.2964, Train Acc: 0.8555 | Val Loss: 0.3184, Val Acc: 0.8487 | F1: 0.8438\n",
      "Epoch [97/300] - Train Loss: 0.2958, Train Acc: 0.8559 | Val Loss: 0.3148, Val Acc: 0.8496 | F1: 0.8448\n",
      "Epoch [98/300] - Train Loss: 0.2950, Train Acc: 0.8561 | Val Loss: 0.3230, Val Acc: 0.8480 | F1: 0.8392\n",
      "Epoch [99/300] - Train Loss: 0.2962, Train Acc: 0.8551 | Val Loss: 0.3246, Val Acc: 0.8480 | F1: 0.8382\n",
      "Epoch [100/300] - Train Loss: 0.2958, Train Acc: 0.8558 | Val Loss: 0.3174, Val Acc: 0.8514 | F1: 0.8421\n",
      "Epoch [101/300] - Train Loss: 0.2960, Train Acc: 0.8557 | Val Loss: 0.3194, Val Acc: 0.8483 | F1: 0.8376\n",
      "Epoch [102/300] - Train Loss: 0.2962, Train Acc: 0.8552 | Val Loss: 0.3178, Val Acc: 0.8490 | F1: 0.8427\n",
      "Epoch [103/300] - Train Loss: 0.2945, Train Acc: 0.8566 | Val Loss: 0.3250, Val Acc: 0.8493 | F1: 0.8402\n",
      "Epoch [104/300] - Train Loss: 0.2951, Train Acc: 0.8563 | Val Loss: 0.3259, Val Acc: 0.8437 | F1: 0.8383\n",
      "Epoch [105/300] - Train Loss: 0.2936, Train Acc: 0.8560 | Val Loss: 0.3258, Val Acc: 0.8496 | F1: 0.8392\n",
      "Epoch [106/300] - Train Loss: 0.2945, Train Acc: 0.8560 | Val Loss: 0.3198, Val Acc: 0.8488 | F1: 0.8439\n",
      "Epoch [107/300] - Train Loss: 0.2941, Train Acc: 0.8550 | Val Loss: 0.3165, Val Acc: 0.8513 | F1: 0.8440\n",
      "Epoch [108/300] - Train Loss: 0.2945, Train Acc: 0.8567 | Val Loss: 0.3293, Val Acc: 0.8467 | F1: 0.8371\n",
      "Epoch [109/300] - Train Loss: 0.2925, Train Acc: 0.8567 | Val Loss: 0.3171, Val Acc: 0.8497 | F1: 0.8445\n",
      "Epoch [110/300] - Train Loss: 0.2932, Train Acc: 0.8563 | Val Loss: 0.3181, Val Acc: 0.8493 | F1: 0.8447\n",
      "Epoch [111/300] - Train Loss: 0.2929, Train Acc: 0.8568 | Val Loss: 0.3186, Val Acc: 0.8476 | F1: 0.8428\n",
      "Epoch [112/300] - Train Loss: 0.2930, Train Acc: 0.8566 | Val Loss: 0.3275, Val Acc: 0.8448 | F1: 0.8316\n",
      "Epoch [113/300] - Train Loss: 0.2933, Train Acc: 0.8564 | Val Loss: 0.3233, Val Acc: 0.8485 | F1: 0.8408\n",
      "Epoch [114/300] - Train Loss: 0.2922, Train Acc: 0.8567 | Val Loss: 0.3193, Val Acc: 0.8476 | F1: 0.8411\n",
      "Epoch [115/300] - Train Loss: 0.2941, Train Acc: 0.8559 | Val Loss: 0.3274, Val Acc: 0.8462 | F1: 0.8413\n",
      "Epoch [116/300] - Train Loss: 0.2914, Train Acc: 0.8576 | Val Loss: 0.3214, Val Acc: 0.8471 | F1: 0.8414\n",
      "Epoch [117/300] - Train Loss: 0.2916, Train Acc: 0.8580 | Val Loss: 0.3196, Val Acc: 0.8503 | F1: 0.8439\n",
      "Epoch [118/300] - Train Loss: 0.2937, Train Acc: 0.8561 | Val Loss: 0.3221, Val Acc: 0.8503 | F1: 0.8451\n",
      "Epoch [119/300] - Train Loss: 0.2921, Train Acc: 0.8572 | Val Loss: 0.3187, Val Acc: 0.8508 | F1: 0.8404\n",
      "Epoch [120/300] - Train Loss: 0.2917, Train Acc: 0.8573 | Val Loss: 0.3185, Val Acc: 0.8493 | F1: 0.8426\n",
      "Epoch [121/300] - Train Loss: 0.2922, Train Acc: 0.8579 | Val Loss: 0.3171, Val Acc: 0.8535 | F1: 0.8481\n",
      "Epoch [122/300] - Train Loss: 0.2916, Train Acc: 0.8577 | Val Loss: 0.3218, Val Acc: 0.8499 | F1: 0.8399\n",
      "Epoch [123/300] - Train Loss: 0.2913, Train Acc: 0.8576 | Val Loss: 0.3181, Val Acc: 0.8501 | F1: 0.8453\n",
      "Epoch [124/300] - Train Loss: 0.2911, Train Acc: 0.8580 | Val Loss: 0.3256, Val Acc: 0.8464 | F1: 0.8417\n",
      "Epoch [125/300] - Train Loss: 0.2920, Train Acc: 0.8570 | Val Loss: 0.3205, Val Acc: 0.8521 | F1: 0.8434\n",
      "Epoch [126/300] - Train Loss: 0.2922, Train Acc: 0.8578 | Val Loss: 0.3180, Val Acc: 0.8499 | F1: 0.8452\n",
      "Epoch [127/300] - Train Loss: 0.2907, Train Acc: 0.8565 | Val Loss: 0.3223, Val Acc: 0.8500 | F1: 0.8452\n",
      "Epoch [128/300] - Train Loss: 0.2899, Train Acc: 0.8576 | Val Loss: 0.3191, Val Acc: 0.8510 | F1: 0.8431\n",
      "Epoch [129/300] - Train Loss: 0.2907, Train Acc: 0.8577 | Val Loss: 0.3262, Val Acc: 0.8457 | F1: 0.8329\n",
      "Epoch [130/300] - Train Loss: 0.2888, Train Acc: 0.8578 | Val Loss: 0.3181, Val Acc: 0.8485 | F1: 0.8420\n",
      "Epoch [131/300] - Train Loss: 0.2900, Train Acc: 0.8578 | Val Loss: 0.3167, Val Acc: 0.8508 | F1: 0.8460\n",
      "Epoch [132/300] - Train Loss: 0.2905, Train Acc: 0.8580 | Val Loss: 0.3205, Val Acc: 0.8524 | F1: 0.8434\n",
      "Epoch [133/300] - Train Loss: 0.2897, Train Acc: 0.8577 | Val Loss: 0.3158, Val Acc: 0.8523 | F1: 0.8476\n",
      "Epoch [134/300] - Train Loss: 0.2920, Train Acc: 0.8573 | Val Loss: 0.3238, Val Acc: 0.8499 | F1: 0.8447\n",
      "Epoch [135/300] - Train Loss: 0.2893, Train Acc: 0.8580 | Val Loss: 0.3211, Val Acc: 0.8509 | F1: 0.8459\n",
      "Epoch [136/300] - Train Loss: 0.2887, Train Acc: 0.8580 | Val Loss: 0.3176, Val Acc: 0.8503 | F1: 0.8427\n",
      "Epoch [137/300] - Train Loss: 0.2907, Train Acc: 0.8575 | Val Loss: 0.3178, Val Acc: 0.8509 | F1: 0.8462\n",
      "Epoch [138/300] - Train Loss: 0.2885, Train Acc: 0.8578 | Val Loss: 0.3207, Val Acc: 0.8454 | F1: 0.8403\n",
      "Epoch [139/300] - Train Loss: 0.2894, Train Acc: 0.8571 | Val Loss: 0.3212, Val Acc: 0.8503 | F1: 0.8399\n",
      "Epoch [140/300] - Train Loss: 0.2890, Train Acc: 0.8585 | Val Loss: 0.3211, Val Acc: 0.8526 | F1: 0.8479\n",
      "Epoch [141/300] - Train Loss: 0.2895, Train Acc: 0.8578 | Val Loss: 0.3238, Val Acc: 0.8487 | F1: 0.8440\n",
      "Epoch [142/300] - Train Loss: 0.2891, Train Acc: 0.8574 | Val Loss: 0.3210, Val Acc: 0.8514 | F1: 0.8411\n",
      "Epoch [143/300] - Train Loss: 0.2874, Train Acc: 0.8596 | Val Loss: 0.3240, Val Acc: 0.8485 | F1: 0.8437\n",
      "Epoch [144/300] - Train Loss: 0.2891, Train Acc: 0.8583 | Val Loss: 0.3188, Val Acc: 0.8481 | F1: 0.8432\n",
      "Epoch [145/300] - Train Loss: 0.2878, Train Acc: 0.8592 | Val Loss: 0.3182, Val Acc: 0.8506 | F1: 0.8396\n",
      "Epoch [146/300] - Train Loss: 0.2889, Train Acc: 0.8588 | Val Loss: 0.3164, Val Acc: 0.8497 | F1: 0.8432\n",
      "Epoch [147/300] - Train Loss: 0.2895, Train Acc: 0.8592 | Val Loss: 0.3194, Val Acc: 0.8538 | F1: 0.8456\n",
      "Epoch [148/300] - Train Loss: 0.2875, Train Acc: 0.8603 | Val Loss: 0.3234, Val Acc: 0.8529 | F1: 0.8437\n",
      "Epoch [149/300] - Train Loss: 0.2867, Train Acc: 0.8579 | Val Loss: 0.3196, Val Acc: 0.8503 | F1: 0.8405\n",
      "Epoch [150/300] - Train Loss: 0.2875, Train Acc: 0.8585 | Val Loss: 0.3156, Val Acc: 0.8511 | F1: 0.8463\n",
      "Epoch [151/300] - Train Loss: 0.2885, Train Acc: 0.8597 | Val Loss: 0.3205, Val Acc: 0.8519 | F1: 0.8467\n",
      "Epoch [152/300] - Train Loss: 0.2871, Train Acc: 0.8591 | Val Loss: 0.3214, Val Acc: 0.8503 | F1: 0.8419\n",
      "Epoch [153/300] - Train Loss: 0.2891, Train Acc: 0.8592 | Val Loss: 0.3267, Val Acc: 0.8494 | F1: 0.8384\n",
      "Epoch [154/300] - Train Loss: 0.2888, Train Acc: 0.8582 | Val Loss: 0.3240, Val Acc: 0.8492 | F1: 0.8444\n",
      "Epoch [155/300] - Train Loss: 0.2874, Train Acc: 0.8591 | Val Loss: 0.3237, Val Acc: 0.8516 | F1: 0.8446\n",
      "Epoch [156/300] - Train Loss: 0.2879, Train Acc: 0.8592 | Val Loss: 0.3222, Val Acc: 0.8509 | F1: 0.8425\n",
      "Epoch [157/300] - Train Loss: 0.2871, Train Acc: 0.8595 | Val Loss: 0.3211, Val Acc: 0.8518 | F1: 0.8420\n",
      "Epoch [158/300] - Train Loss: 0.2864, Train Acc: 0.8588 | Val Loss: 0.3189, Val Acc: 0.8524 | F1: 0.8427\n",
      "Epoch [159/300] - Train Loss: 0.2863, Train Acc: 0.8594 | Val Loss: 0.3167, Val Acc: 0.8506 | F1: 0.8457\n",
      "Epoch [160/300] - Train Loss: 0.2855, Train Acc: 0.8592 | Val Loss: 0.3260, Val Acc: 0.8485 | F1: 0.8421\n",
      "Epoch [161/300] - Train Loss: 0.2860, Train Acc: 0.8601 | Val Loss: 0.3310, Val Acc: 0.8480 | F1: 0.8370\n",
      "Epoch [162/300] - Train Loss: 0.2859, Train Acc: 0.8593 | Val Loss: 0.3251, Val Acc: 0.8510 | F1: 0.8424\n",
      "Epoch [163/300] - Train Loss: 0.2862, Train Acc: 0.8591 | Val Loss: 0.3208, Val Acc: 0.8517 | F1: 0.8418\n",
      "Epoch [164/300] - Train Loss: 0.2868, Train Acc: 0.8597 | Val Loss: 0.3185, Val Acc: 0.8515 | F1: 0.8466\n",
      "Epoch [165/300] - Train Loss: 0.2862, Train Acc: 0.8592 | Val Loss: 0.3225, Val Acc: 0.8516 | F1: 0.8467\n",
      "Epoch [166/300] - Train Loss: 0.2860, Train Acc: 0.8578 | Val Loss: 0.3189, Val Acc: 0.8506 | F1: 0.8441\n",
      "Epoch [167/300] - Train Loss: 0.2846, Train Acc: 0.8595 | Val Loss: 0.3202, Val Acc: 0.8525 | F1: 0.8447\n",
      "Epoch [168/300] - Train Loss: 0.2863, Train Acc: 0.8580 | Val Loss: 0.3210, Val Acc: 0.8515 | F1: 0.8432\n",
      "Epoch [169/300] - Train Loss: 0.2855, Train Acc: 0.8599 | Val Loss: 0.3196, Val Acc: 0.8495 | F1: 0.8443\n",
      "Epoch [170/300] - Train Loss: 0.2858, Train Acc: 0.8589 | Val Loss: 0.3176, Val Acc: 0.8495 | F1: 0.8426\n",
      "Epoch [171/300] - Train Loss: 0.2864, Train Acc: 0.8596 | Val Loss: 0.3240, Val Acc: 0.8500 | F1: 0.8449\n",
      "Epoch [172/300] - Train Loss: 0.2853, Train Acc: 0.8585 | Val Loss: 0.3164, Val Acc: 0.8531 | F1: 0.8455\n",
      "Epoch [173/300] - Train Loss: 0.2859, Train Acc: 0.8597 | Val Loss: 0.3257, Val Acc: 0.8501 | F1: 0.8453\n",
      "Epoch [174/300] - Train Loss: 0.2849, Train Acc: 0.8605 | Val Loss: 0.3259, Val Acc: 0.8485 | F1: 0.8375\n",
      "Epoch [175/300] - Train Loss: 0.2839, Train Acc: 0.8601 | Val Loss: 0.3222, Val Acc: 0.8527 | F1: 0.8438\n",
      "Epoch [176/300] - Train Loss: 0.2855, Train Acc: 0.8594 | Val Loss: 0.3184, Val Acc: 0.8544 | F1: 0.8488\n",
      "Epoch [177/300] - Train Loss: 0.2853, Train Acc: 0.8599 | Val Loss: 0.3251, Val Acc: 0.8521 | F1: 0.8415\n",
      "Epoch [178/300] - Train Loss: 0.2837, Train Acc: 0.8606 | Val Loss: 0.3225, Val Acc: 0.8506 | F1: 0.8420\n",
      "Epoch [179/300] - Train Loss: 0.2850, Train Acc: 0.8613 | Val Loss: 0.3199, Val Acc: 0.8506 | F1: 0.8436\n",
      "Epoch [180/300] - Train Loss: 0.2841, Train Acc: 0.8606 | Val Loss: 0.3229, Val Acc: 0.8463 | F1: 0.8332\n",
      "Epoch [181/300] - Train Loss: 0.2850, Train Acc: 0.8597 | Val Loss: 0.3252, Val Acc: 0.8525 | F1: 0.8438\n",
      "Epoch [182/300] - Train Loss: 0.2835, Train Acc: 0.8600 | Val Loss: 0.3187, Val Acc: 0.8486 | F1: 0.8437\n",
      "Epoch [183/300] - Train Loss: 0.2849, Train Acc: 0.8598 | Val Loss: 0.3280, Val Acc: 0.8507 | F1: 0.8416\n",
      "Epoch [184/300] - Train Loss: 0.2845, Train Acc: 0.8599 | Val Loss: 0.3188, Val Acc: 0.8532 | F1: 0.8446\n",
      "Epoch [185/300] - Train Loss: 0.2848, Train Acc: 0.8595 | Val Loss: 0.3235, Val Acc: 0.8496 | F1: 0.8400\n",
      "Epoch [186/300] - Train Loss: 0.2830, Train Acc: 0.8591 | Val Loss: 0.3220, Val Acc: 0.8527 | F1: 0.8437\n",
      "Epoch [187/300] - Train Loss: 0.2852, Train Acc: 0.8602 | Val Loss: 0.3271, Val Acc: 0.8505 | F1: 0.8440\n",
      "Epoch [188/300] - Train Loss: 0.2830, Train Acc: 0.8610 | Val Loss: 0.3211, Val Acc: 0.8531 | F1: 0.8448\n",
      "Epoch [189/300] - Train Loss: 0.2826, Train Acc: 0.8605 | Val Loss: 0.3212, Val Acc: 0.8535 | F1: 0.8487\n",
      "Epoch [190/300] - Train Loss: 0.2848, Train Acc: 0.8598 | Val Loss: 0.3212, Val Acc: 0.8510 | F1: 0.8449\n",
      "Epoch [191/300] - Train Loss: 0.2817, Train Acc: 0.8603 | Val Loss: 0.3193, Val Acc: 0.8533 | F1: 0.8452\n",
      "Epoch [192/300] - Train Loss: 0.2833, Train Acc: 0.8609 | Val Loss: 0.3199, Val Acc: 0.8518 | F1: 0.8415\n",
      "Epoch [193/300] - Train Loss: 0.2849, Train Acc: 0.8604 | Val Loss: 0.3213, Val Acc: 0.8521 | F1: 0.8471\n",
      "Epoch [194/300] - Train Loss: 0.2822, Train Acc: 0.8609 | Val Loss: 0.3254, Val Acc: 0.8524 | F1: 0.8435\n",
      "Epoch [195/300] - Train Loss: 0.2835, Train Acc: 0.8606 | Val Loss: 0.3259, Val Acc: 0.8509 | F1: 0.8402\n",
      "Epoch [196/300] - Train Loss: 0.2838, Train Acc: 0.8607 | Val Loss: 0.3265, Val Acc: 0.8493 | F1: 0.8379\n",
      "Epoch [197/300] - Train Loss: 0.2826, Train Acc: 0.8611 | Val Loss: 0.3265, Val Acc: 0.8516 | F1: 0.8420\n",
      "Epoch [198/300] - Train Loss: 0.2823, Train Acc: 0.8615 | Val Loss: 0.3335, Val Acc: 0.8482 | F1: 0.8376\n",
      "Epoch [199/300] - Train Loss: 0.2827, Train Acc: 0.8615 | Val Loss: 0.3259, Val Acc: 0.8499 | F1: 0.8380\n",
      "Epoch [200/300] - Train Loss: 0.2838, Train Acc: 0.8603 | Val Loss: 0.3205, Val Acc: 0.8520 | F1: 0.8417\n",
      "Epoch [201/300] - Train Loss: 0.2825, Train Acc: 0.8617 | Val Loss: 0.3272, Val Acc: 0.8515 | F1: 0.8450\n",
      "Epoch [202/300] - Train Loss: 0.2829, Train Acc: 0.8599 | Val Loss: 0.3260, Val Acc: 0.8519 | F1: 0.8471\n",
      "Epoch [203/300] - Train Loss: 0.2825, Train Acc: 0.8609 | Val Loss: 0.3252, Val Acc: 0.8533 | F1: 0.8445\n",
      "Epoch [204/300] - Train Loss: 0.2828, Train Acc: 0.8608 | Val Loss: 0.3241, Val Acc: 0.8506 | F1: 0.8458\n",
      "Epoch [205/300] - Train Loss: 0.2822, Train Acc: 0.8602 | Val Loss: 0.3197, Val Acc: 0.8513 | F1: 0.8434\n",
      "Epoch [206/300] - Train Loss: 0.2828, Train Acc: 0.8620 | Val Loss: 0.3215, Val Acc: 0.8531 | F1: 0.8444\n",
      "Epoch [207/300] - Train Loss: 0.2811, Train Acc: 0.8618 | Val Loss: 0.3268, Val Acc: 0.8524 | F1: 0.8436\n",
      "Epoch [208/300] - Train Loss: 0.2827, Train Acc: 0.8610 | Val Loss: 0.3221, Val Acc: 0.8518 | F1: 0.8419\n",
      "Epoch [209/300] - Train Loss: 0.2815, Train Acc: 0.8627 | Val Loss: 0.3201, Val Acc: 0.8598 | F1: 0.8549\n",
      "Epoch [210/300] - Train Loss: 0.2805, Train Acc: 0.8626 | Val Loss: 0.3265, Val Acc: 0.8537 | F1: 0.8458\n",
      "Epoch [211/300] - Train Loss: 0.2811, Train Acc: 0.8612 | Val Loss: 0.3240, Val Acc: 0.8509 | F1: 0.8414\n",
      "Epoch [212/300] - Train Loss: 0.2822, Train Acc: 0.8609 | Val Loss: 0.3227, Val Acc: 0.8516 | F1: 0.8421\n",
      "Epoch [213/300] - Train Loss: 0.2809, Train Acc: 0.8616 | Val Loss: 0.3227, Val Acc: 0.8516 | F1: 0.8453\n",
      "Epoch [214/300] - Train Loss: 0.2815, Train Acc: 0.8616 | Val Loss: 0.3326, Val Acc: 0.8509 | F1: 0.8460\n",
      "Epoch [215/300] - Train Loss: 0.2811, Train Acc: 0.8616 | Val Loss: 0.3170, Val Acc: 0.8534 | F1: 0.8444\n",
      "Epoch [216/300] - Train Loss: 0.2804, Train Acc: 0.8611 | Val Loss: 0.3312, Val Acc: 0.8501 | F1: 0.8455\n",
      "Epoch [217/300] - Train Loss: 0.2830, Train Acc: 0.8609 | Val Loss: 0.3205, Val Acc: 0.8531 | F1: 0.8466\n",
      "Epoch [218/300] - Train Loss: 0.2801, Train Acc: 0.8609 | Val Loss: 0.3262, Val Acc: 0.8531 | F1: 0.8433\n",
      "Epoch [219/300] - Train Loss: 0.2806, Train Acc: 0.8612 | Val Loss: 0.3260, Val Acc: 0.8531 | F1: 0.8440\n",
      "Epoch [220/300] - Train Loss: 0.2803, Train Acc: 0.8620 | Val Loss: 0.3204, Val Acc: 0.8535 | F1: 0.8437\n",
      "Epoch [221/300] - Train Loss: 0.2829, Train Acc: 0.8610 | Val Loss: 0.3295, Val Acc: 0.8504 | F1: 0.8426\n",
      "Epoch [222/300] - Train Loss: 0.2807, Train Acc: 0.8622 | Val Loss: 0.3196, Val Acc: 0.8528 | F1: 0.8482\n",
      "Epoch [223/300] - Train Loss: 0.2800, Train Acc: 0.8621 | Val Loss: 0.3287, Val Acc: 0.8518 | F1: 0.8424\n",
      "Epoch [224/300] - Train Loss: 0.2805, Train Acc: 0.8620 | Val Loss: 0.3203, Val Acc: 0.8529 | F1: 0.8441\n",
      "Epoch [225/300] - Train Loss: 0.2820, Train Acc: 0.8623 | Val Loss: 0.3180, Val Acc: 0.8531 | F1: 0.8483\n",
      "Epoch [226/300] - Train Loss: 0.2809, Train Acc: 0.8621 | Val Loss: 0.3260, Val Acc: 0.8504 | F1: 0.8453\n",
      "Epoch [227/300] - Train Loss: 0.2794, Train Acc: 0.8617 | Val Loss: 0.3271, Val Acc: 0.8520 | F1: 0.8446\n",
      "Epoch [228/300] - Train Loss: 0.2805, Train Acc: 0.8627 | Val Loss: 0.3261, Val Acc: 0.8514 | F1: 0.8449\n",
      "Epoch [229/300] - Train Loss: 0.2800, Train Acc: 0.8617 | Val Loss: 0.3248, Val Acc: 0.8547 | F1: 0.8468\n",
      "Epoch [230/300] - Train Loss: 0.2825, Train Acc: 0.8604 | Val Loss: 0.3240, Val Acc: 0.8533 | F1: 0.8443\n",
      "Epoch [231/300] - Train Loss: 0.2798, Train Acc: 0.8615 | Val Loss: 0.3291, Val Acc: 0.8525 | F1: 0.8477\n",
      "Epoch [232/300] - Train Loss: 0.2804, Train Acc: 0.8618 | Val Loss: 0.3246, Val Acc: 0.8538 | F1: 0.8446\n",
      "Epoch [233/300] - Train Loss: 0.2796, Train Acc: 0.8625 | Val Loss: 0.3306, Val Acc: 0.8497 | F1: 0.8422\n",
      "Epoch [234/300] - Train Loss: 0.2808, Train Acc: 0.8612 | Val Loss: 0.3227, Val Acc: 0.8525 | F1: 0.8435\n",
      "Epoch [235/300] - Train Loss: 0.2798, Train Acc: 0.8625 | Val Loss: 0.3207, Val Acc: 0.8531 | F1: 0.8449\n",
      "Epoch [236/300] - Train Loss: 0.2788, Train Acc: 0.8618 | Val Loss: 0.3275, Val Acc: 0.8506 | F1: 0.8395\n",
      "Epoch [237/300] - Train Loss: 0.2788, Train Acc: 0.8615 | Val Loss: 0.3234, Val Acc: 0.8527 | F1: 0.8462\n",
      "Epoch [238/300] - Train Loss: 0.2813, Train Acc: 0.8632 | Val Loss: 0.3262, Val Acc: 0.8510 | F1: 0.8439\n",
      "Epoch [239/300] - Train Loss: 0.2782, Train Acc: 0.8626 | Val Loss: 0.3277, Val Acc: 0.8515 | F1: 0.8466\n",
      "Epoch [240/300] - Train Loss: 0.2809, Train Acc: 0.8611 | Val Loss: 0.3259, Val Acc: 0.8524 | F1: 0.8452\n",
      "Epoch [241/300] - Train Loss: 0.2806, Train Acc: 0.8624 | Val Loss: 0.3242, Val Acc: 0.8537 | F1: 0.8437\n",
      "Epoch [242/300] - Train Loss: 0.2806, Train Acc: 0.8607 | Val Loss: 0.3291, Val Acc: 0.8500 | F1: 0.8433\n",
      "Epoch [243/300] - Train Loss: 0.2780, Train Acc: 0.8624 | Val Loss: 0.3237, Val Acc: 0.8538 | F1: 0.8438\n",
      "Epoch [244/300] - Train Loss: 0.2792, Train Acc: 0.8631 | Val Loss: 0.3337, Val Acc: 0.8519 | F1: 0.8469\n",
      "Epoch [245/300] - Train Loss: 0.2793, Train Acc: 0.8627 | Val Loss: 0.3289, Val Acc: 0.8493 | F1: 0.8429\n",
      "Epoch [246/300] - Train Loss: 0.2771, Train Acc: 0.8633 | Val Loss: 0.3257, Val Acc: 0.8507 | F1: 0.8428\n",
      "Epoch [247/300] - Train Loss: 0.2787, Train Acc: 0.8629 | Val Loss: 0.3293, Val Acc: 0.8518 | F1: 0.8471\n",
      "Epoch [248/300] - Train Loss: 0.2795, Train Acc: 0.8614 | Val Loss: 0.3263, Val Acc: 0.8522 | F1: 0.8475\n",
      "Epoch [249/300] - Train Loss: 0.2782, Train Acc: 0.8624 | Val Loss: 0.3215, Val Acc: 0.8552 | F1: 0.8465\n",
      "Epoch [250/300] - Train Loss: 0.2796, Train Acc: 0.8618 | Val Loss: 0.3228, Val Acc: 0.8517 | F1: 0.8455\n",
      "Epoch [251/300] - Train Loss: 0.2792, Train Acc: 0.8629 | Val Loss: 0.3290, Val Acc: 0.8520 | F1: 0.8427\n",
      "Epoch [252/300] - Train Loss: 0.2785, Train Acc: 0.8631 | Val Loss: 0.3190, Val Acc: 0.8515 | F1: 0.8449\n",
      "Epoch [253/300] - Train Loss: 0.2780, Train Acc: 0.8628 | Val Loss: 0.3274, Val Acc: 0.8527 | F1: 0.8443\n",
      "Epoch [254/300] - Train Loss: 0.2799, Train Acc: 0.8622 | Val Loss: 0.3266, Val Acc: 0.8514 | F1: 0.8467\n",
      "Epoch [255/300] - Train Loss: 0.2783, Train Acc: 0.8620 | Val Loss: 0.3226, Val Acc: 0.8533 | F1: 0.8451\n",
      "Epoch [256/300] - Train Loss: 0.2790, Train Acc: 0.8621 | Val Loss: 0.3278, Val Acc: 0.8535 | F1: 0.8456\n",
      "Epoch [257/300] - Train Loss: 0.2788, Train Acc: 0.8625 | Val Loss: 0.3278, Val Acc: 0.8504 | F1: 0.8434\n",
      "Epoch [258/300] - Train Loss: 0.2800, Train Acc: 0.8624 | Val Loss: 0.3263, Val Acc: 0.8539 | F1: 0.8478\n",
      "Epoch [259/300] - Train Loss: 0.2787, Train Acc: 0.8618 | Val Loss: 0.3260, Val Acc: 0.8510 | F1: 0.8446\n",
      "Epoch [260/300] - Train Loss: 0.2790, Train Acc: 0.8628 | Val Loss: 0.3234, Val Acc: 0.8541 | F1: 0.8483\n",
      "Epoch [261/300] - Train Loss: 0.2781, Train Acc: 0.8627 | Val Loss: 0.3330, Val Acc: 0.8521 | F1: 0.8437\n",
      "Epoch [262/300] - Train Loss: 0.2790, Train Acc: 0.8625 | Val Loss: 0.3262, Val Acc: 0.8520 | F1: 0.8426\n",
      "Epoch [263/300] - Train Loss: 0.2787, Train Acc: 0.8628 | Val Loss: 0.3230, Val Acc: 0.8526 | F1: 0.8476\n",
      "Epoch [264/300] - Train Loss: 0.2776, Train Acc: 0.8626 | Val Loss: 0.3325, Val Acc: 0.8519 | F1: 0.8471\n",
      "Epoch [265/300] - Train Loss: 0.2789, Train Acc: 0.8621 | Val Loss: 0.3316, Val Acc: 0.8542 | F1: 0.8458\n",
      "Epoch [266/300] - Train Loss: 0.2777, Train Acc: 0.8632 | Val Loss: 0.3282, Val Acc: 0.8527 | F1: 0.8436\n",
      "Epoch [267/300] - Train Loss: 0.2778, Train Acc: 0.8618 | Val Loss: 0.3249, Val Acc: 0.8520 | F1: 0.8436\n",
      "Epoch [268/300] - Train Loss: 0.2773, Train Acc: 0.8635 | Val Loss: 0.3259, Val Acc: 0.8526 | F1: 0.8477\n",
      "Epoch [269/300] - Train Loss: 0.2778, Train Acc: 0.8632 | Val Loss: 0.3338, Val Acc: 0.8519 | F1: 0.8471\n",
      "Epoch [270/300] - Train Loss: 0.2772, Train Acc: 0.8630 | Val Loss: 0.3280, Val Acc: 0.8517 | F1: 0.8453\n",
      "Epoch [271/300] - Train Loss: 0.2782, Train Acc: 0.8637 | Val Loss: 0.3291, Val Acc: 0.8521 | F1: 0.8433\n",
      "Epoch [272/300] - Train Loss: 0.2766, Train Acc: 0.8633 | Val Loss: 0.3350, Val Acc: 0.8537 | F1: 0.8490\n",
      "Epoch [273/300] - Train Loss: 0.2768, Train Acc: 0.8631 | Val Loss: 0.3313, Val Acc: 0.8518 | F1: 0.8438\n",
      "Epoch [274/300] - Train Loss: 0.2792, Train Acc: 0.8620 | Val Loss: 0.3239, Val Acc: 0.8516 | F1: 0.8468\n",
      "Epoch [275/300] - Train Loss: 0.2766, Train Acc: 0.8633 | Val Loss: 0.3291, Val Acc: 0.8508 | F1: 0.8459\n",
      "Epoch [276/300] - Train Loss: 0.2781, Train Acc: 0.8619 | Val Loss: 0.3280, Val Acc: 0.8526 | F1: 0.8448\n",
      "Epoch [277/300] - Train Loss: 0.2778, Train Acc: 0.8628 | Val Loss: 0.3228, Val Acc: 0.8558 | F1: 0.8501\n",
      "Epoch [278/300] - Train Loss: 0.2755, Train Acc: 0.8632 | Val Loss: 0.3260, Val Acc: 0.8562 | F1: 0.8508\n",
      "Epoch [279/300] - Train Loss: 0.2784, Train Acc: 0.8629 | Val Loss: 0.3260, Val Acc: 0.8552 | F1: 0.8499\n",
      "Epoch [280/300] - Train Loss: 0.2760, Train Acc: 0.8639 | Val Loss: 0.3289, Val Acc: 0.8512 | F1: 0.8449\n",
      "Epoch [281/300] - Train Loss: 0.2759, Train Acc: 0.8639 | Val Loss: 0.3364, Val Acc: 0.8521 | F1: 0.8426\n",
      "Epoch [282/300] - Train Loss: 0.2783, Train Acc: 0.8617 | Val Loss: 0.3269, Val Acc: 0.8529 | F1: 0.8471\n",
      "Epoch [283/300] - Train Loss: 0.2777, Train Acc: 0.8624 | Val Loss: 0.3316, Val Acc: 0.8541 | F1: 0.8446\n",
      "Epoch [284/300] - Train Loss: 0.2774, Train Acc: 0.8616 | Val Loss: 0.3224, Val Acc: 0.8536 | F1: 0.8450\n",
      "Epoch [285/300] - Train Loss: 0.2773, Train Acc: 0.8623 | Val Loss: 0.3281, Val Acc: 0.8539 | F1: 0.8444\n",
      "Epoch [286/300] - Train Loss: 0.2774, Train Acc: 0.8630 | Val Loss: 0.3306, Val Acc: 0.8528 | F1: 0.8454\n",
      "Epoch [287/300] - Train Loss: 0.2765, Train Acc: 0.8634 | Val Loss: 0.3372, Val Acc: 0.8517 | F1: 0.8469\n",
      "Epoch [288/300] - Train Loss: 0.2775, Train Acc: 0.8626 | Val Loss: 0.3340, Val Acc: 0.8507 | F1: 0.8408\n",
      "Epoch [289/300] - Train Loss: 0.2758, Train Acc: 0.8633 | Val Loss: 0.3300, Val Acc: 0.8548 | F1: 0.8468\n",
      "Epoch [290/300] - Train Loss: 0.2774, Train Acc: 0.8637 | Val Loss: 0.3260, Val Acc: 0.8521 | F1: 0.8452\n",
      "Epoch [291/300] - Train Loss: 0.2764, Train Acc: 0.8628 | Val Loss: 0.3349, Val Acc: 0.8512 | F1: 0.8462\n",
      "Epoch [292/300] - Train Loss: 0.2760, Train Acc: 0.8632 | Val Loss: 0.3282, Val Acc: 0.8509 | F1: 0.8461\n",
      "Epoch [293/300] - Train Loss: 0.2760, Train Acc: 0.8632 | Val Loss: 0.3310, Val Acc: 0.8499 | F1: 0.8425\n",
      "Epoch [294/300] - Train Loss: 0.2766, Train Acc: 0.8636 | Val Loss: 0.3306, Val Acc: 0.8510 | F1: 0.8442\n",
      "Epoch [295/300] - Train Loss: 0.2758, Train Acc: 0.8638 | Val Loss: 0.3288, Val Acc: 0.8578 | F1: 0.8524\n",
      "Epoch [296/300] - Train Loss: 0.2755, Train Acc: 0.8626 | Val Loss: 0.3315, Val Acc: 0.8521 | F1: 0.8473\n",
      "Epoch [297/300] - Train Loss: 0.2771, Train Acc: 0.8627 | Val Loss: 0.3314, Val Acc: 0.8511 | F1: 0.8449\n",
      "Epoch [298/300] - Train Loss: 0.2763, Train Acc: 0.8644 | Val Loss: 0.3274, Val Acc: 0.8545 | F1: 0.8481\n",
      "Epoch [299/300] - Train Loss: 0.2761, Train Acc: 0.8645 | Val Loss: 0.3397, Val Acc: 0.8509 | F1: 0.8462\n",
      "Epoch [300/300] - Train Loss: 0.2766, Train Acc: 0.8637 | Val Loss: 0.3369, Val Acc: 0.8514 | F1: 0.8456\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "    all_preds, all_labels = [], []  # <-- added\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in test_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            outputs = model(X_val)\n",
    "\n",
    "            # record loss\n",
    "            loss = criterion(outputs, y_val)\n",
    "            val_loss += loss.item() * X_val.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += y_val.size(0)\n",
    "            val_correct += (predicted == y_val).sum().item()\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(y_val.cpu().numpy())\n",
    "\n",
    "    val_loss /= val_total\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    # extra metrics\n",
    "    precision = precision_score(all_labels, all_preds, average=\"macro\")\n",
    "    recall = recall_score(all_labels, all_preds, average=\"macro\")\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "    # Store metrics\n",
    "    history[\"epoch\"].append(epoch + 1)\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    history[\"precision\"].append(precision)\n",
    "    history[\"recall\"].append(recall)\n",
    "    history[\"f1\"].append(f1)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} | \"\n",
    "          f\"F1: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2145b657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9380    0.8533    0.8936      3210\n",
      "           1     0.6955    0.6055    0.6474      2773\n",
      "           2     0.6699    0.7495    0.7075      2962\n",
      "           3     0.9962    0.9816    0.9889      3209\n",
      "           4     0.9777    0.9300    0.9533      3059\n",
      "           5     0.8244    0.9513    0.8833      3202\n",
      "\n",
      "    accuracy                         0.8514     18415\n",
      "   macro avg     0.8503    0.8452    0.8456     18415\n",
      "weighted avg     0.8553    0.8514    0.8513     18415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_val, y_val in test_loader:\n",
    "        X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "        outputs = model(X_val)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_val.cpu().numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2f8e026c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ff36eb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch': [1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  84,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  93,\n",
       "  94,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  100,\n",
       "  101,\n",
       "  102,\n",
       "  103,\n",
       "  104,\n",
       "  105,\n",
       "  106,\n",
       "  107,\n",
       "  108,\n",
       "  109,\n",
       "  110,\n",
       "  111,\n",
       "  112,\n",
       "  113,\n",
       "  114,\n",
       "  115,\n",
       "  116,\n",
       "  117,\n",
       "  118,\n",
       "  119,\n",
       "  120,\n",
       "  121,\n",
       "  122,\n",
       "  123,\n",
       "  124,\n",
       "  125,\n",
       "  126,\n",
       "  127,\n",
       "  128,\n",
       "  129,\n",
       "  130,\n",
       "  131,\n",
       "  132,\n",
       "  133,\n",
       "  134,\n",
       "  135,\n",
       "  136,\n",
       "  137,\n",
       "  138,\n",
       "  139,\n",
       "  140,\n",
       "  141,\n",
       "  142,\n",
       "  143,\n",
       "  144,\n",
       "  145,\n",
       "  146,\n",
       "  147,\n",
       "  148,\n",
       "  149,\n",
       "  150,\n",
       "  151,\n",
       "  152,\n",
       "  153,\n",
       "  154,\n",
       "  155,\n",
       "  156,\n",
       "  157,\n",
       "  158,\n",
       "  159,\n",
       "  160,\n",
       "  161,\n",
       "  162,\n",
       "  163,\n",
       "  164,\n",
       "  165,\n",
       "  166,\n",
       "  167,\n",
       "  168,\n",
       "  169,\n",
       "  170,\n",
       "  171,\n",
       "  172,\n",
       "  173,\n",
       "  174,\n",
       "  175,\n",
       "  176,\n",
       "  177,\n",
       "  178,\n",
       "  179,\n",
       "  180,\n",
       "  181,\n",
       "  182,\n",
       "  183,\n",
       "  184,\n",
       "  185,\n",
       "  186,\n",
       "  187,\n",
       "  188,\n",
       "  189,\n",
       "  190,\n",
       "  191,\n",
       "  192,\n",
       "  193,\n",
       "  194,\n",
       "  195,\n",
       "  196,\n",
       "  197,\n",
       "  198,\n",
       "  199,\n",
       "  200,\n",
       "  201,\n",
       "  202,\n",
       "  203,\n",
       "  204,\n",
       "  205,\n",
       "  206,\n",
       "  207,\n",
       "  208,\n",
       "  209,\n",
       "  210,\n",
       "  211,\n",
       "  212,\n",
       "  213,\n",
       "  214,\n",
       "  215,\n",
       "  216,\n",
       "  217,\n",
       "  218,\n",
       "  219,\n",
       "  220,\n",
       "  221,\n",
       "  222,\n",
       "  223,\n",
       "  224,\n",
       "  225,\n",
       "  226,\n",
       "  227,\n",
       "  228,\n",
       "  229,\n",
       "  230,\n",
       "  231,\n",
       "  232,\n",
       "  233,\n",
       "  234,\n",
       "  235,\n",
       "  236,\n",
       "  237,\n",
       "  238,\n",
       "  239,\n",
       "  240,\n",
       "  241,\n",
       "  242,\n",
       "  243,\n",
       "  244,\n",
       "  245,\n",
       "  246,\n",
       "  247,\n",
       "  248,\n",
       "  249,\n",
       "  250,\n",
       "  251,\n",
       "  252,\n",
       "  253,\n",
       "  254,\n",
       "  255,\n",
       "  256,\n",
       "  257,\n",
       "  258,\n",
       "  259,\n",
       "  260,\n",
       "  261,\n",
       "  262,\n",
       "  263,\n",
       "  264,\n",
       "  265,\n",
       "  266,\n",
       "  267,\n",
       "  268,\n",
       "  269,\n",
       "  270,\n",
       "  271,\n",
       "  272,\n",
       "  273,\n",
       "  274,\n",
       "  275,\n",
       "  276,\n",
       "  277,\n",
       "  278,\n",
       "  279,\n",
       "  280,\n",
       "  281,\n",
       "  282,\n",
       "  283,\n",
       "  284,\n",
       "  285,\n",
       "  286,\n",
       "  287,\n",
       "  288,\n",
       "  289,\n",
       "  290,\n",
       "  291,\n",
       "  292,\n",
       "  293,\n",
       "  294,\n",
       "  295,\n",
       "  296,\n",
       "  297,\n",
       "  298,\n",
       "  299,\n",
       "  300],\n",
       " 'train_loss': [0.9217542087977714,\n",
       "  0.5516450415009372,\n",
       "  0.48057236863861236,\n",
       "  0.44356070667219616,\n",
       "  0.423227750556527,\n",
       "  0.4082124542824564,\n",
       "  0.3983358626319828,\n",
       "  0.39058607201455303,\n",
       "  0.3831234902573168,\n",
       "  0.3801766879128906,\n",
       "  0.3755915301064392,\n",
       "  0.37246208996775876,\n",
       "  0.36678066202584897,\n",
       "  0.36325247487495155,\n",
       "  0.3617370507850749,\n",
       "  0.359713191473564,\n",
       "  0.35779428804929664,\n",
       "  0.3558407594721687,\n",
       "  0.3543891208310834,\n",
       "  0.3523365510089757,\n",
       "  0.35113652561145875,\n",
       "  0.34975667291325996,\n",
       "  0.34579469821543013,\n",
       "  0.3464459486016624,\n",
       "  0.3436126688106808,\n",
       "  0.34160857864788136,\n",
       "  0.33997984370309026,\n",
       "  0.34040202238405065,\n",
       "  0.3382970472321988,\n",
       "  0.3359605609486981,\n",
       "  0.3387369863283735,\n",
       "  0.335680901568888,\n",
       "  0.33337798048983236,\n",
       "  0.3328682459447194,\n",
       "  0.33365544503852945,\n",
       "  0.3300150532969638,\n",
       "  0.33715826113226816,\n",
       "  0.32635482522380865,\n",
       "  0.32819359619536087,\n",
       "  0.32732505970167075,\n",
       "  0.32621801562085145,\n",
       "  0.32568467099523724,\n",
       "  0.32534932322676147,\n",
       "  0.324475257488268,\n",
       "  0.32334530135545503,\n",
       "  0.3230276148839209,\n",
       "  0.3226340232826295,\n",
       "  0.3218469894711772,\n",
       "  0.3201986296206236,\n",
       "  0.3191195950788039,\n",
       "  0.32066676917953035,\n",
       "  0.31839800016847475,\n",
       "  0.31741527671461667,\n",
       "  0.31846500386267074,\n",
       "  0.3179507970214359,\n",
       "  0.3166271700972424,\n",
       "  0.3163552985492306,\n",
       "  0.31546280902215806,\n",
       "  0.31564830249777936,\n",
       "  0.3151534979815474,\n",
       "  0.3133309760778991,\n",
       "  0.31365074264536774,\n",
       "  0.313810494884336,\n",
       "  0.31237274019111394,\n",
       "  0.31340684740460956,\n",
       "  0.31172603361638124,\n",
       "  0.3118244637774005,\n",
       "  0.3113737821873531,\n",
       "  0.30993240641040537,\n",
       "  0.3093138656157487,\n",
       "  0.3093159305641182,\n",
       "  0.3093590290887774,\n",
       "  0.30861916546973206,\n",
       "  0.3071554686118857,\n",
       "  0.30865475909813317,\n",
       "  0.3076696103029731,\n",
       "  0.30786489897956515,\n",
       "  0.30679631260647483,\n",
       "  0.30737294309892593,\n",
       "  0.307319024245911,\n",
       "  0.3059153484755664,\n",
       "  0.30544586860598527,\n",
       "  0.3050507293287984,\n",
       "  0.30402587092606703,\n",
       "  0.306678842508083,\n",
       "  0.30299522423728664,\n",
       "  0.30453349998553436,\n",
       "  0.3032327293516186,\n",
       "  0.3044308790219248,\n",
       "  0.3039396344011537,\n",
       "  0.3016999045880477,\n",
       "  0.302402501482026,\n",
       "  0.30288387821714313,\n",
       "  0.3023976862071284,\n",
       "  0.3022271407515101,\n",
       "  0.3012278060015318,\n",
       "  0.3010167141986316,\n",
       "  0.3011778094235159,\n",
       "  0.3006756313236399,\n",
       "  0.3007792526172638,\n",
       "  0.2997927049984062,\n",
       "  0.29896275614616863,\n",
       "  0.301404658928641,\n",
       "  0.3008324024353852,\n",
       "  0.29888753544213204,\n",
       "  0.29987095556988347,\n",
       "  0.29862331477878906,\n",
       "  0.29934666212620337,\n",
       "  0.298388374980945,\n",
       "  0.2991353253048234,\n",
       "  0.2972974675633704,\n",
       "  0.29749156820119915,\n",
       "  0.29710081473845396,\n",
       "  0.2978847219974815,\n",
       "  0.29658146153081166,\n",
       "  0.29719192519856397,\n",
       "  0.2967966034935197,\n",
       "  0.29727620850003234,\n",
       "  0.29874877152653584,\n",
       "  0.2956371214192743,\n",
       "  0.2964022011483255,\n",
       "  0.2958025452912901,\n",
       "  0.29503186008518417,\n",
       "  0.29617543570453936,\n",
       "  0.2958412583130526,\n",
       "  0.2960311413675513,\n",
       "  0.2961683649111637,\n",
       "  0.29449725618946093,\n",
       "  0.2950787887637914,\n",
       "  0.2936281522580636,\n",
       "  0.2945494012892329,\n",
       "  0.2940759508446441,\n",
       "  0.2944990440310156,\n",
       "  0.2925199161838814,\n",
       "  0.29318253469427025,\n",
       "  0.2929228691761945,\n",
       "  0.29304048704001784,\n",
       "  0.29325902778648716,\n",
       "  0.2922242435814649,\n",
       "  0.294084851434583,\n",
       "  0.29140745183430355,\n",
       "  0.29159849017972606,\n",
       "  0.29365433686464937,\n",
       "  0.29207410513683063,\n",
       "  0.29168642769227826,\n",
       "  0.2921781283025812,\n",
       "  0.29159735579631013,\n",
       "  0.29127048711992437,\n",
       "  0.291102539531996,\n",
       "  0.2920122235101435,\n",
       "  0.29216468313725036,\n",
       "  0.29073822694959084,\n",
       "  0.28992047212106975,\n",
       "  0.29067295905435375,\n",
       "  0.28884312608242746,\n",
       "  0.28999321531497213,\n",
       "  0.29052158102845094,\n",
       "  0.2897044429248795,\n",
       "  0.2920072563153403,\n",
       "  0.28931527822152125,\n",
       "  0.288689627559261,\n",
       "  0.29073295535338606,\n",
       "  0.28845017397881795,\n",
       "  0.28939541352022097,\n",
       "  0.2890124963430323,\n",
       "  0.2895222408575881,\n",
       "  0.2890698259504228,\n",
       "  0.2874311514192913,\n",
       "  0.28906751930213564,\n",
       "  0.28782226253984583,\n",
       "  0.2888843591454888,\n",
       "  0.28952021134848405,\n",
       "  0.28749983375160976,\n",
       "  0.28668125466783945,\n",
       "  0.28753082749120423,\n",
       "  0.28850984422689724,\n",
       "  0.2870990469865243,\n",
       "  0.2891428404944341,\n",
       "  0.2888094361628415,\n",
       "  0.2874157674789247,\n",
       "  0.28791622297964314,\n",
       "  0.2871396823288865,\n",
       "  0.2864055736799866,\n",
       "  0.28627611564940597,\n",
       "  0.2854605982284781,\n",
       "  0.28599316640928096,\n",
       "  0.2859014708634459,\n",
       "  0.28619500346587606,\n",
       "  0.2867877508477645,\n",
       "  0.2862274901246865,\n",
       "  0.2860123163066953,\n",
       "  0.2846249031957127,\n",
       "  0.2863016010897376,\n",
       "  0.2854745660507532,\n",
       "  0.2857736145039755,\n",
       "  0.2864198313484385,\n",
       "  0.285324070498571,\n",
       "  0.2858953907324763,\n",
       "  0.2848696353946677,\n",
       "  0.2838877388737282,\n",
       "  0.28551208777911774,\n",
       "  0.28525217729030544,\n",
       "  0.2837125582340768,\n",
       "  0.2849988902015539,\n",
       "  0.28405947386433544,\n",
       "  0.28501742333436747,\n",
       "  0.28346778279185414,\n",
       "  0.2848620130531936,\n",
       "  0.2845042824054115,\n",
       "  0.2848009940681568,\n",
       "  0.28302713397816187,\n",
       "  0.28520693671027164,\n",
       "  0.2830410038021082,\n",
       "  0.28259124775345107,\n",
       "  0.2847970386164379,\n",
       "  0.2816824692297229,\n",
       "  0.2833333036076699,\n",
       "  0.28493919304649856,\n",
       "  0.2822039698246116,\n",
       "  0.28354818269267706,\n",
       "  0.2837616301495025,\n",
       "  0.28263439450933614,\n",
       "  0.28231401196381684,\n",
       "  0.2826986003586903,\n",
       "  0.2837609592869175,\n",
       "  0.282512430189331,\n",
       "  0.282910143695623,\n",
       "  0.28247810701105697,\n",
       "  0.28284141058469175,\n",
       "  0.28223577756436197,\n",
       "  0.2828074039710251,\n",
       "  0.2810756961259159,\n",
       "  0.2827006282178192,\n",
       "  0.281522445116279,\n",
       "  0.2805384518806997,\n",
       "  0.28112646669765207,\n",
       "  0.2822242868605644,\n",
       "  0.2809221940859313,\n",
       "  0.28151904106538267,\n",
       "  0.2810627152962884,\n",
       "  0.2803595091916661,\n",
       "  0.2830477989832763,\n",
       "  0.2800927180005355,\n",
       "  0.28059476225012114,\n",
       "  0.2802994429317447,\n",
       "  0.28290574822732956,\n",
       "  0.2806941421656531,\n",
       "  0.2800219536277556,\n",
       "  0.28054297151306345,\n",
       "  0.28200570486471593,\n",
       "  0.28093949361752363,\n",
       "  0.27942343987559914,\n",
       "  0.28048815031460644,\n",
       "  0.2800243799392981,\n",
       "  0.28251733775925925,\n",
       "  0.27977343485313233,\n",
       "  0.2803615140579581,\n",
       "  0.2795730926076816,\n",
       "  0.28076693449709444,\n",
       "  0.27977461263842074,\n",
       "  0.27876639369467654,\n",
       "  0.27878294013843796,\n",
       "  0.2812923185986348,\n",
       "  0.27815484757767023,\n",
       "  0.28086464755009916,\n",
       "  0.2805957109971699,\n",
       "  0.28058446129089737,\n",
       "  0.27796701577407673,\n",
       "  0.2791861580986363,\n",
       "  0.27934038422443613,\n",
       "  0.27714118688312206,\n",
       "  0.27872328621855824,\n",
       "  0.2794512690623599,\n",
       "  0.278240043985429,\n",
       "  0.2795611665863804,\n",
       "  0.2792402808065279,\n",
       "  0.2784912938460412,\n",
       "  0.277986460792895,\n",
       "  0.2798874349501737,\n",
       "  0.2782926948523553,\n",
       "  0.2790258210984355,\n",
       "  0.2787699312171539,\n",
       "  0.2800300234313274,\n",
       "  0.2786512714854544,\n",
       "  0.2789875201504852,\n",
       "  0.27805454089317955,\n",
       "  0.2790011463332239,\n",
       "  0.2787299134801597,\n",
       "  0.27759481646898804,\n",
       "  0.2788958594750425,\n",
       "  0.27768981983631935,\n",
       "  0.2778329949657788,\n",
       "  0.27727324013690086,\n",
       "  0.2777691803422059,\n",
       "  0.27721797008693594,\n",
       "  0.2782054507857657,\n",
       "  0.27659229748219083,\n",
       "  0.2768326134542388,\n",
       "  0.2792059620853717,\n",
       "  0.2765700275383841,\n",
       "  0.278066027290372,\n",
       "  0.27775190102098857,\n",
       "  0.275454830988538,\n",
       "  0.2783982637304581,\n",
       "  0.27604500835648593,\n",
       "  0.2758519877451424,\n",
       "  0.27833694288776517,\n",
       "  0.2777189763945182,\n",
       "  0.2773722775996422,\n",
       "  0.27728062457930736,\n",
       "  0.27736615603641246,\n",
       "  0.2764503232975535,\n",
       "  0.2774932189819792,\n",
       "  0.2758143359527719,\n",
       "  0.27740779414288674,\n",
       "  0.2764064063407061,\n",
       "  0.2760403455460909,\n",
       "  0.27601960545375454,\n",
       "  0.276601009617291,\n",
       "  0.2757607470462375,\n",
       "  0.2755348348937029,\n",
       "  0.27713765531398815,\n",
       "  0.27625424562614276,\n",
       "  0.2761386395651426,\n",
       "  0.2766399226874156],\n",
       " 'train_acc': [0.6461891444242309,\n",
       "  0.7519074642265606,\n",
       "  0.7731000027152516,\n",
       "  0.7874501072524369,\n",
       "  0.799057807705884,\n",
       "  0.8034022102147764,\n",
       "  0.8088462896087323,\n",
       "  0.8130549295392218,\n",
       "  0.8151592495044666,\n",
       "  0.8173586032745934,\n",
       "  0.8197072958809634,\n",
       "  0.8199923972956095,\n",
       "  0.8225583100274241,\n",
       "  0.8253821716582042,\n",
       "  0.8255722392679682,\n",
       "  0.8253142903690027,\n",
       "  0.8261967471286215,\n",
       "  0.8267126449265525,\n",
       "  0.8281653045154633,\n",
       "  0.8285861685085123,\n",
       "  0.830690488473757,\n",
       "  0.8296315403622145,\n",
       "  0.8319938092264249,\n",
       "  0.8324010969616334,\n",
       "  0.8324961307665155,\n",
       "  0.8327948084390019,\n",
       "  0.8338266040348639,\n",
       "  0.8353607211708165,\n",
       "  0.8358358901952266,\n",
       "  0.8373021260419777,\n",
       "  0.8372885497841375,\n",
       "  0.83693556708029,\n",
       "  0.8371935159792555,\n",
       "  0.8375329224252627,\n",
       "  0.8378587526134297,\n",
       "  0.8394064460072226,\n",
       "  0.8376686850036655,\n",
       "  0.8418909011919954,\n",
       "  0.8413614271362242,\n",
       "  0.84069619050205,\n",
       "  0.842624019115371,\n",
       "  0.8424203752477667,\n",
       "  0.8429770018192185,\n",
       "  0.8425018327948084,\n",
       "  0.8438051535474762,\n",
       "  0.8432485269760244,\n",
       "  0.8445518477286921,\n",
       "  0.844266746314046,\n",
       "  0.8446197290178935,\n",
       "  0.8455157620353526,\n",
       "  0.8439952211572402,\n",
       "  0.8459637785440821,\n",
       "  0.8459637785440821,\n",
       "  0.8448641016590187,\n",
       "  0.8464389475684923,\n",
       "  0.8458008634499986,\n",
       "  0.8477286920633197,\n",
       "  0.8465475576312146,\n",
       "  0.8459637785440821,\n",
       "  0.8479730647044449,\n",
       "  0.8485704200494176,\n",
       "  0.8477151158054794,\n",
       "  0.8463574900214504,\n",
       "  0.8488012164327025,\n",
       "  0.8478780308995628,\n",
       "  0.8482174373455701,\n",
       "  0.8484618099866953,\n",
       "  0.8497379782236825,\n",
       "  0.8498194357707242,\n",
       "  0.8499280458334465,\n",
       "  0.8506611637568221,\n",
       "  0.85056612995194,\n",
       "  0.8502131472480925,\n",
       "  0.8501045371853703,\n",
       "  0.850986993944989,\n",
       "  0.850986993944989,\n",
       "  0.8505118249205789,\n",
       "  0.8501995709902522,\n",
       "  0.8499687746069673,\n",
       "  0.8498873170599256,\n",
       "  0.8503624860843357,\n",
       "  0.8511363327812321,\n",
       "  0.8512313665861142,\n",
       "  0.8516250780634825,\n",
       "  0.8515571967742811,\n",
       "  0.8527654837220668,\n",
       "  0.8527383312063863,\n",
       "  0.8521002470878927,\n",
       "  0.8514621629693991,\n",
       "  0.851597925547802,\n",
       "  0.8530234326210323,\n",
       "  0.8524125010182193,\n",
       "  0.8535800591924841,\n",
       "  0.853240652746477,\n",
       "  0.8522903146976567,\n",
       "  0.8528062124955877,\n",
       "  0.853620787966005,\n",
       "  0.8534171440984007,\n",
       "  0.8532135002307963,\n",
       "  0.8534442966140813,\n",
       "  0.8545439734991447,\n",
       "  0.8544489396942627,\n",
       "  0.8538108555757691,\n",
       "  0.8539330418963317,\n",
       "  0.8548019223981101,\n",
       "  0.8525346873387819,\n",
       "  0.854842651171631,\n",
       "  0.8549241087186727,\n",
       "  0.8542996008580195,\n",
       "  0.8541095332482554,\n",
       "  0.8547340411089087,\n",
       "  0.8553178201960412,\n",
       "  0.8561323956664585,\n",
       "  0.8544489396942627,\n",
       "  0.8554807352901246,\n",
       "  0.8550055662657146,\n",
       "  0.8544896684677835,\n",
       "  0.8554671590322843,\n",
       "  0.8541638382796166,\n",
       "  0.8558337179939721,\n",
       "  0.8555486165793261,\n",
       "  0.8559015992831736,\n",
       "  0.8561323956664585,\n",
       "  0.8550598712970757,\n",
       "  0.855779412962611,\n",
       "  0.8557115316734095,\n",
       "  0.855168481359798,\n",
       "  0.8565532596595075,\n",
       "  0.8563360395340628,\n",
       "  0.8560373618615765,\n",
       "  0.8560102093458959,\n",
       "  0.8549648374921937,\n",
       "  0.8567025984957506,\n",
       "  0.8566754459800701,\n",
       "  0.8563360395340628,\n",
       "  0.8567840560427924,\n",
       "  0.856580412175188,\n",
       "  0.8564310733389449,\n",
       "  0.8566618697222298,\n",
       "  0.8559423280566945,\n",
       "  0.8575579027396888,\n",
       "  0.8579516142170572,\n",
       "  0.8561459719242988,\n",
       "  0.8571506150044802,\n",
       "  0.8573406826142442,\n",
       "  0.8578565804121752,\n",
       "  0.8577343940916126,\n",
       "  0.8575850552553694,\n",
       "  0.8580059192484184,\n",
       "  0.8569605473947162,\n",
       "  0.8578294278964946,\n",
       "  0.8565396834016672,\n",
       "  0.8575714789975292,\n",
       "  0.8577208178337723,\n",
       "  0.857802275380814,\n",
       "  0.8578294278964946,\n",
       "  0.8580330717640989,\n",
       "  0.8576936653180918,\n",
       "  0.8573406826142442,\n",
       "  0.8580466480219392,\n",
       "  0.8579516142170572,\n",
       "  0.8575035977083276,\n",
       "  0.8578430041543349,\n",
       "  0.8570827337152788,\n",
       "  0.8585218170463493,\n",
       "  0.8578158516386544,\n",
       "  0.8574357164191262,\n",
       "  0.8595536126422113,\n",
       "  0.8582910206630644,\n",
       "  0.8592413587118847,\n",
       "  0.8587526134296343,\n",
       "  0.8592006299383638,\n",
       "  0.8603410355969481,\n",
       "  0.8578837329278558,\n",
       "  0.8585218170463493,\n",
       "  0.8597436802519753,\n",
       "  0.8591327486491623,\n",
       "  0.8592277824540444,\n",
       "  0.8582367156317032,\n",
       "  0.8590648673599609,\n",
       "  0.8591734774226832,\n",
       "  0.8594721550951696,\n",
       "  0.858834070976676,\n",
       "  0.8593771212902875,\n",
       "  0.8592413587118847,\n",
       "  0.8601238154715034,\n",
       "  0.8593228162589264,\n",
       "  0.8590920198756414,\n",
       "  0.8597301039941351,\n",
       "  0.8592413587118847,\n",
       "  0.8578430041543349,\n",
       "  0.8594993076108501,\n",
       "  0.8580330717640989,\n",
       "  0.8599201716038991,\n",
       "  0.8588612234923566,\n",
       "  0.8595807651578918,\n",
       "  0.8585353933041896,\n",
       "  0.8597029514784544,\n",
       "  0.8605175269488718,\n",
       "  0.8600966629558229,\n",
       "  0.8593906975481278,\n",
       "  0.8598658665725379,\n",
       "  0.8605989844959135,\n",
       "  0.8613185261614489,\n",
       "  0.8606261370115941,\n",
       "  0.8596893752206142,\n",
       "  0.8600287816666214,\n",
       "  0.8597572565098156,\n",
       "  0.8599065953460588,\n",
       "  0.859540036384371,\n",
       "  0.8590784436178012,\n",
       "  0.8602188492763855,\n",
       "  0.8609655434576013,\n",
       "  0.8604767981753509,\n",
       "  0.8597708327676559,\n",
       "  0.8602867305655869,\n",
       "  0.8608840859105596,\n",
       "  0.8604496456596704,\n",
       "  0.8608976621683999,\n",
       "  0.8605854082380733,\n",
       "  0.860666865785115,\n",
       "  0.861087729778164,\n",
       "  0.8614542887398517,\n",
       "  0.8615357462868934,\n",
       "  0.8603003068234272,\n",
       "  0.8616850851231367,\n",
       "  0.8599473241195796,\n",
       "  0.8608976621683999,\n",
       "  0.860761899589997,\n",
       "  0.8602324255342257,\n",
       "  0.861983762795623,\n",
       "  0.8618344239593798,\n",
       "  0.861047001004643,\n",
       "  0.8627304569768389,\n",
       "  0.8625539656249152,\n",
       "  0.8612099160987265,\n",
       "  0.8608705096527193,\n",
       "  0.8615764750604143,\n",
       "  0.8615900513182546,\n",
       "  0.8615764750604143,\n",
       "  0.8611148822938445,\n",
       "  0.8609383909419208,\n",
       "  0.8609383909419208,\n",
       "  0.861182763583046,\n",
       "  0.8619701865377827,\n",
       "  0.8609791197154416,\n",
       "  0.8621738304053871,\n",
       "  0.8621059491161857,\n",
       "  0.8619973390534633,\n",
       "  0.86230959298379,\n",
       "  0.8620923728583453,\n",
       "  0.8616715088652964,\n",
       "  0.862689728203318,\n",
       "  0.8617258138966575,\n",
       "  0.86043606940183,\n",
       "  0.8614950175133727,\n",
       "  0.8617529664123381,\n",
       "  0.8624860843357137,\n",
       "  0.8612370686144071,\n",
       "  0.8624589318200331,\n",
       "  0.8618072714436993,\n",
       "  0.861467864997692,\n",
       "  0.8631784734855684,\n",
       "  0.8626489994297972,\n",
       "  0.8610605772624834,\n",
       "  0.8623774742729914,\n",
       "  0.8607211708164761,\n",
       "  0.8624453555621928,\n",
       "  0.8630698634228461,\n",
       "  0.8627168807189987,\n",
       "  0.8633413885796519,\n",
       "  0.8628526432974015,\n",
       "  0.8614135599663308,\n",
       "  0.8624453555621928,\n",
       "  0.8617665426701784,\n",
       "  0.8628526432974015,\n",
       "  0.863110592196367,\n",
       "  0.8627576094925195,\n",
       "  0.8622009829210676,\n",
       "  0.8619973390534633,\n",
       "  0.8620923728583453,\n",
       "  0.862499660593554,\n",
       "  0.8624182030465123,\n",
       "  0.8617801189280186,\n",
       "  0.8627711857503598,\n",
       "  0.8626761519454778,\n",
       "  0.862499660593554,\n",
       "  0.8628254907817209,\n",
       "  0.8625675418827554,\n",
       "  0.8620923728583453,\n",
       "  0.8632192022590893,\n",
       "  0.8617529664123381,\n",
       "  0.8635043036737353,\n",
       "  0.8632327785169296,\n",
       "  0.8630427109071656,\n",
       "  0.86372152379918,\n",
       "  0.8633142360639713,\n",
       "  0.8631377447120475,\n",
       "  0.8619973390534633,\n",
       "  0.8633142360639713,\n",
       "  0.8619158815064216,\n",
       "  0.8628390670395613,\n",
       "  0.8632056260012491,\n",
       "  0.8628933720709224,\n",
       "  0.863911591408944,\n",
       "  0.863911591408944,\n",
       "  0.8617122376388172,\n",
       "  0.8624317793043526,\n",
       "  0.8616443563496158,\n",
       "  0.862268864210269,\n",
       "  0.8629748296179641,\n",
       "  0.8634092698688534,\n",
       "  0.8626489994297972,\n",
       "  0.8632735072904505,\n",
       "  0.8636807950256591,\n",
       "  0.8627983382660404,\n",
       "  0.8631784734855684,\n",
       "  0.8631648972277282,\n",
       "  0.8635586087050965,\n",
       "  0.8638165576040621,\n",
       "  0.8626218469141166,\n",
       "  0.8626625756876375,\n",
       "  0.8644139129490347,\n",
       "  0.8644546417225556,\n",
       "  0.8636807950256591],\n",
       " 'val_loss': [0.607621772649202,\n",
       "  0.4808818793769379,\n",
       "  0.4372955929025696,\n",
       "  0.4361792230554291,\n",
       "  0.4292083417838467,\n",
       "  0.38879675395845426,\n",
       "  0.39688736773883715,\n",
       "  0.38083435437001517,\n",
       "  0.36824468421832457,\n",
       "  0.36943038193842265,\n",
       "  0.36309380946747155,\n",
       "  0.36811376737408946,\n",
       "  0.36865619021429175,\n",
       "  0.36549854762943224,\n",
       "  0.354923184685755,\n",
       "  0.3546654640317384,\n",
       "  0.35281100199010884,\n",
       "  0.34712789794235427,\n",
       "  0.37075255655284295,\n",
       "  0.3504512576913821,\n",
       "  0.35119982897151014,\n",
       "  0.3549149390345451,\n",
       "  0.3687886289368173,\n",
       "  0.3534341454473553,\n",
       "  0.34090591587031177,\n",
       "  0.3413039270898683,\n",
       "  0.3437249058230947,\n",
       "  0.33793017803700176,\n",
       "  0.3465558424644615,\n",
       "  0.33858907142476546,\n",
       "  0.3348878112170737,\n",
       "  0.33718793577161327,\n",
       "  0.33574998572958314,\n",
       "  0.3377938986520485,\n",
       "  0.3327225252496397,\n",
       "  0.33097055752640536,\n",
       "  0.3286037678648006,\n",
       "  0.33041331288835907,\n",
       "  0.3316060262256558,\n",
       "  0.3306343036519423,\n",
       "  0.32929920289880904,\n",
       "  0.32502897387932345,\n",
       "  0.3304661010291115,\n",
       "  0.32785532336632517,\n",
       "  0.32430618053346166,\n",
       "  0.3264909044203472,\n",
       "  0.33010998689024595,\n",
       "  0.3235287407850591,\n",
       "  0.3285539528127925,\n",
       "  0.32343368681267554,\n",
       "  0.32320742272437347,\n",
       "  0.3253516308694244,\n",
       "  0.3262744849947785,\n",
       "  0.33033302566812894,\n",
       "  0.3217049028201496,\n",
       "  0.3242986466325951,\n",
       "  0.32268879930836464,\n",
       "  0.32848917842720504,\n",
       "  0.3249447269848174,\n",
       "  0.33179359921870466,\n",
       "  0.3224679747337949,\n",
       "  0.3237620927945706,\n",
       "  0.32402157159625855,\n",
       "  0.32000650337717956,\n",
       "  0.32058650349230666,\n",
       "  0.3207883916922701,\n",
       "  0.3240704887964594,\n",
       "  0.3190220755838746,\n",
       "  0.31965692719783984,\n",
       "  0.3216353622785315,\n",
       "  0.3261598988084925,\n",
       "  0.31966203136221405,\n",
       "  0.32522843541770147,\n",
       "  0.3232698739643107,\n",
       "  0.31870658219491793,\n",
       "  0.32229139693819764,\n",
       "  0.3199704101041643,\n",
       "  0.3221588068169613,\n",
       "  0.31610344081121783,\n",
       "  0.32177121878608944,\n",
       "  0.3197748431394257,\n",
       "  0.3197694402041398,\n",
       "  0.31741340873262786,\n",
       "  0.31694726585985816,\n",
       "  0.3202186579957398,\n",
       "  0.317241297750864,\n",
       "  0.315282194918017,\n",
       "  0.3162098024822337,\n",
       "  0.3191813678479926,\n",
       "  0.3192361231248987,\n",
       "  0.31863874845002416,\n",
       "  0.3200762879320891,\n",
       "  0.31420620456519993,\n",
       "  0.31613553902263264,\n",
       "  0.3159027254196279,\n",
       "  0.32020616299135674,\n",
       "  0.31929508251338556,\n",
       "  0.3192180051206086,\n",
       "  0.3186451435072928,\n",
       "  0.316309797105147,\n",
       "  0.32097557311209524,\n",
       "  0.3180540752255525,\n",
       "  0.31454026186042205,\n",
       "  0.3157933292005686,\n",
       "  0.3152265113105018,\n",
       "  0.315755313950455,\n",
       "  0.31633112768164934,\n",
       "  0.31448894190037346,\n",
       "  0.3181364741414065,\n",
       "  0.31951744094867274,\n",
       "  0.31914858673209895,\n",
       "  0.3151469726148844,\n",
       "  0.31813144155601236,\n",
       "  0.31551507853179916,\n",
       "  0.32184280479630795,\n",
       "  0.3195771491449875,\n",
       "  0.3147101558763114,\n",
       "  0.316027740737261,\n",
       "  0.31510727667238864,\n",
       "  0.31983011795261973,\n",
       "  0.31835766392268144,\n",
       "  0.31476515162065566,\n",
       "  0.3230497658446921,\n",
       "  0.32456549696387593,\n",
       "  0.3173562683264199,\n",
       "  0.3194050444149952,\n",
       "  0.31776554537570384,\n",
       "  0.32504811063752753,\n",
       "  0.3258599296070589,\n",
       "  0.32578187156493915,\n",
       "  0.3198275042250206,\n",
       "  0.31649228929502304,\n",
       "  0.3293333503933274,\n",
       "  0.3171328567273909,\n",
       "  0.31814896333583376,\n",
       "  0.3185584298107179,\n",
       "  0.32751337050291107,\n",
       "  0.3232516021580273,\n",
       "  0.3193236158297433,\n",
       "  0.3274448088903062,\n",
       "  0.32142069497705317,\n",
       "  0.3196485242133486,\n",
       "  0.3220539341049288,\n",
       "  0.31867288597860705,\n",
       "  0.31850400975208193,\n",
       "  0.3170539433966504,\n",
       "  0.3218170599156736,\n",
       "  0.318092778908504,\n",
       "  0.32557183279636404,\n",
       "  0.320548419441958,\n",
       "  0.318004684926633,\n",
       "  0.32229639743289124,\n",
       "  0.3191139180667369,\n",
       "  0.3261777226526129,\n",
       "  0.3181465989103662,\n",
       "  0.3167041287663641,\n",
       "  0.3204799075586913,\n",
       "  0.3158145040540698,\n",
       "  0.3238161083945042,\n",
       "  0.32111551989170317,\n",
       "  0.3175501824574337,\n",
       "  0.31777033237960395,\n",
       "  0.32070099725174317,\n",
       "  0.32124655956228954,\n",
       "  0.3211400672239096,\n",
       "  0.32378401782180577,\n",
       "  0.32096162033637776,\n",
       "  0.3239832696677096,\n",
       "  0.31880659437069514,\n",
       "  0.31820779855795733,\n",
       "  0.3164252046363172,\n",
       "  0.31935087927424194,\n",
       "  0.32340132134167227,\n",
       "  0.319623027569957,\n",
       "  0.315620670492093,\n",
       "  0.3204701915678692,\n",
       "  0.3214111250944834,\n",
       "  0.3266588969898068,\n",
       "  0.32404136119282445,\n",
       "  0.3237236824286169,\n",
       "  0.3222417327751131,\n",
       "  0.3210910323729115,\n",
       "  0.3189247138840719,\n",
       "  0.31669221506182216,\n",
       "  0.3260067254766122,\n",
       "  0.33101246231863946,\n",
       "  0.3250746776835607,\n",
       "  0.3208314521705007,\n",
       "  0.31854928220512746,\n",
       "  0.32252026700533315,\n",
       "  0.3189103500644959,\n",
       "  0.3201920643954571,\n",
       "  0.3209731766897927,\n",
       "  0.3196081748401669,\n",
       "  0.31755159300305164,\n",
       "  0.32404417923181045,\n",
       "  0.316432230609479,\n",
       "  0.32569879477774355,\n",
       "  0.3258715843380776,\n",
       "  0.3222471424573924,\n",
       "  0.3183500215876468,\n",
       "  0.32511379884339986,\n",
       "  0.32253115630784246,\n",
       "  0.3198603984699798,\n",
       "  0.32289845661854183,\n",
       "  0.32515390482798173,\n",
       "  0.31874982222506054,\n",
       "  0.3280227094006674,\n",
       "  0.3188486355949348,\n",
       "  0.32345706927138695,\n",
       "  0.3219757757124097,\n",
       "  0.32707716733775577,\n",
       "  0.3210920782086644,\n",
       "  0.3212299289779109,\n",
       "  0.3211952586310602,\n",
       "  0.3192622858465543,\n",
       "  0.3198891770295918,\n",
       "  0.32133792616121476,\n",
       "  0.3254223029476904,\n",
       "  0.32590000012825276,\n",
       "  0.3264681908833796,\n",
       "  0.32653183657579893,\n",
       "  0.3335336021680467,\n",
       "  0.325929267115804,\n",
       "  0.32053888655375634,\n",
       "  0.32717843529422613,\n",
       "  0.32598094544493567,\n",
       "  0.3252103850953514,\n",
       "  0.3240753710917277,\n",
       "  0.3197181221294947,\n",
       "  0.3214719487667213,\n",
       "  0.3267937127805229,\n",
       "  0.32206488099720953,\n",
       "  0.3200911426103992,\n",
       "  0.3265378615615357,\n",
       "  0.32404812513697645,\n",
       "  0.322650752448207,\n",
       "  0.32271732455713736,\n",
       "  0.33257692634428193,\n",
       "  0.31704920771618494,\n",
       "  0.3312321389171374,\n",
       "  0.3205117044389296,\n",
       "  0.32624865601705594,\n",
       "  0.32601740271896895,\n",
       "  0.32037626420580884,\n",
       "  0.32947268564514,\n",
       "  0.3196118390417733,\n",
       "  0.3287359268678908,\n",
       "  0.3203351560863761,\n",
       "  0.31799772877651744,\n",
       "  0.3259788053889589,\n",
       "  0.3270987953904722,\n",
       "  0.3260534292195693,\n",
       "  0.3248274775196151,\n",
       "  0.3239910674732792,\n",
       "  0.3290650893869589,\n",
       "  0.32459523087767456,\n",
       "  0.33058569787023123,\n",
       "  0.32265737274565065,\n",
       "  0.3207242311730257,\n",
       "  0.3275073203579615,\n",
       "  0.32342010600198523,\n",
       "  0.32624601383685675,\n",
       "  0.32770534339658014,\n",
       "  0.32592111833873016,\n",
       "  0.3242478378839283,\n",
       "  0.3290506970710868,\n",
       "  0.32368648361971997,\n",
       "  0.3337318368298574,\n",
       "  0.328915462571902,\n",
       "  0.325669781968479,\n",
       "  0.3292529683413468,\n",
       "  0.3263143994110355,\n",
       "  0.32151624759577746,\n",
       "  0.3227791339086185,\n",
       "  0.3290142348998596,\n",
       "  0.3189790105531632,\n",
       "  0.3273583395695382,\n",
       "  0.32657350327150997,\n",
       "  0.3226155407219,\n",
       "  0.32781573649401774,\n",
       "  0.32782073496898295,\n",
       "  0.32627279272941734,\n",
       "  0.3259856934444496,\n",
       "  0.32342354893813857,\n",
       "  0.3329910132246046,\n",
       "  0.32617334538978177,\n",
       "  0.3229748798183795,\n",
       "  0.33249935715249657,\n",
       "  0.33159536388041433,\n",
       "  0.32824072392939097,\n",
       "  0.3248516616001098,\n",
       "  0.32587271692019393,\n",
       "  0.3337968624547678,\n",
       "  0.3279513035006993,\n",
       "  0.3290507406295721,\n",
       "  0.33498828930437907,\n",
       "  0.3313275930773393,\n",
       "  0.3239203125214143,\n",
       "  0.32906347859368645,\n",
       "  0.3280028207823857,\n",
       "  0.32283972853103343,\n",
       "  0.3259831035522349,\n",
       "  0.326015743351876,\n",
       "  0.3289286458634696,\n",
       "  0.3364176280094559,\n",
       "  0.3269451525235286,\n",
       "  0.3316124821677015,\n",
       "  0.3223962680658438,\n",
       "  0.3281190624371062,\n",
       "  0.330603325663136,\n",
       "  0.3371941582392067,\n",
       "  0.3339702330867525,\n",
       "  0.3299716536357226,\n",
       "  0.32596190988192114,\n",
       "  0.33493177878801633,\n",
       "  0.3282275160952104,\n",
       "  0.33099619064711694,\n",
       "  0.33061607852922065,\n",
       "  0.3288123160446917,\n",
       "  0.3315332435441671,\n",
       "  0.33137159111575526,\n",
       "  0.32737417320437384,\n",
       "  0.33968476987616575,\n",
       "  0.3368666350323643],\n",
       " 'val_acc': [0.7309801792017377,\n",
       "  0.7720336682052674,\n",
       "  0.7914200380124898,\n",
       "  0.7773011132229161,\n",
       "  0.7812652728753733,\n",
       "  0.8147705674721695,\n",
       "  0.8093402117838718,\n",
       "  0.8173228346456692,\n",
       "  0.8215042085256584,\n",
       "  0.8184089057833288,\n",
       "  0.8228074938908498,\n",
       "  0.8206353516155308,\n",
       "  0.8199294053760521,\n",
       "  0.8219386369807222,\n",
       "  0.8285093673635623,\n",
       "  0.8249253326092859,\n",
       "  0.8251968503937008,\n",
       "  0.8263372250882433,\n",
       "  0.8244366005973391,\n",
       "  0.8278577246809666,\n",
       "  0.8263915286451262,\n",
       "  0.8280206353516155,\n",
       "  0.8051588379038827,\n",
       "  0.8285636709204452,\n",
       "  0.8329622590279663,\n",
       "  0.8352430084170513,\n",
       "  0.8259571001900624,\n",
       "  0.8338311159380939,\n",
       "  0.8293239207168069,\n",
       "  0.8342112408362747,\n",
       "  0.8376866684767852,\n",
       "  0.8329622590279663,\n",
       "  0.8319304914471898,\n",
       "  0.833613901710562,\n",
       "  0.8367092044528917,\n",
       "  0.8392071680695086,\n",
       "  0.8400760249796362,\n",
       "  0.8387184360575618,\n",
       "  0.8385012218300298,\n",
       "  0.8396415965245724,\n",
       "  0.8388813467282107,\n",
       "  0.8423024708118382,\n",
       "  0.8405104534347,\n",
       "  0.8375237578061363,\n",
       "  0.8436057561770296,\n",
       "  0.8428455063806679,\n",
       "  0.8395329894108065,\n",
       "  0.8423567743687211,\n",
       "  0.8409448818897638,\n",
       "  0.8406190605484659,\n",
       "  0.8417051316861255,\n",
       "  0.8421395601411893,\n",
       "  0.8394243822970404,\n",
       "  0.8376323649199022,\n",
       "  0.8439858810752104,\n",
       "  0.8451262557697529,\n",
       "  0.84251968503937,\n",
       "  0.8444746130871572,\n",
       "  0.8420852565843062,\n",
       "  0.8387727396144448,\n",
       "  0.8441487917458593,\n",
       "  0.8419223459136573,\n",
       "  0.8475699158294868,\n",
       "  0.8452891664404019,\n",
       "  0.8466467553624762,\n",
       "  0.8420852565843062,\n",
       "  0.8461580233505295,\n",
       "  0.8451262557697529,\n",
       "  0.8465924518055933,\n",
       "  0.8421938636980723,\n",
       "  0.8429541134944338,\n",
       "  0.8464295411349443,\n",
       "  0.8419766494705403,\n",
       "  0.8459951126798805,\n",
       "  0.8472983980450719,\n",
       "  0.8464295411349443,\n",
       "  0.8466467553624762,\n",
       "  0.842736899266902,\n",
       "  0.8474070051588379,\n",
       "  0.8448004344284551,\n",
       "  0.8484387727396144,\n",
       "  0.8444746130871572,\n",
       "  0.8474613087157209,\n",
       "  0.8522400217214228,\n",
       "  0.8459408091229975,\n",
       "  0.8494705403203909,\n",
       "  0.849361933206625,\n",
       "  0.8485473798533804,\n",
       "  0.8459951126798805,\n",
       "  0.8472983980450719,\n",
       "  0.8478957371707847,\n",
       "  0.849307629649742,\n",
       "  0.8489818083084442,\n",
       "  0.8482758620689655,\n",
       "  0.8499592723323378,\n",
       "  0.8481129513983166,\n",
       "  0.8478414336139017,\n",
       "  0.8508281292424654,\n",
       "  0.8513168612544122,\n",
       "  0.8503937007874016,\n",
       "  0.8477871300570188,\n",
       "  0.8478414336139017,\n",
       "  0.8501764865598697,\n",
       "  0.8496877545479229,\n",
       "  0.8519685039370078,\n",
       "  0.8498506652185718,\n",
       "  0.8502850936736356,\n",
       "  0.8490904154222102,\n",
       "  0.8482215585120826,\n",
       "  0.8483844691827315,\n",
       "  0.8489818083084442,\n",
       "  0.8492533260928591,\n",
       "  0.8499049687754547,\n",
       "  0.8494705403203909,\n",
       "  0.8463209340211784,\n",
       "  0.8479500407276677,\n",
       "  0.8507738256855824,\n",
       "  0.8496877545479229,\n",
       "  0.8507195221286994,\n",
       "  0.8544664675536248,\n",
       "  0.8486559869671464,\n",
       "  0.849579147434157,\n",
       "  0.8480043442845506,\n",
       "  0.8479500407276677,\n",
       "  0.8513711648112952,\n",
       "  0.8482758620689655,\n",
       "  0.8490361118653271,\n",
       "  0.849307629649742,\n",
       "  0.8437143632907955,\n",
       "  0.849579147434157,\n",
       "  0.8487645940809123,\n",
       "  0.8513168612544122,\n",
       "  0.8467010589193592,\n",
       "  0.8496877545479229,\n",
       "  0.849307629649742,\n",
       "  0.8476242193863698,\n",
       "  0.8448004344284551,\n",
       "  0.8484930762964974,\n",
       "  0.8476242193863698,\n",
       "  0.8461580233505295,\n",
       "  0.847135487374423,\n",
       "  0.8502850936736356,\n",
       "  0.8502850936736356,\n",
       "  0.8508281292424654,\n",
       "  0.849307629649742,\n",
       "  0.8535433070866142,\n",
       "  0.8499049687754547,\n",
       "  0.8500678794461037,\n",
       "  0.8463752375780613,\n",
       "  0.8520771110507738,\n",
       "  0.8499049687754547,\n",
       "  0.8499592723323378,\n",
       "  0.8509910399131143,\n",
       "  0.8457235948954657,\n",
       "  0.8484930762964974,\n",
       "  0.8507738256855824,\n",
       "  0.8524029323920717,\n",
       "  0.8523486288351887,\n",
       "  0.8499049687754547,\n",
       "  0.8509367363562313,\n",
       "  0.8502850936736356,\n",
       "  0.8509367363562313,\n",
       "  0.8453977735541678,\n",
       "  0.8503393972305185,\n",
       "  0.8525658430627207,\n",
       "  0.8487102905240294,\n",
       "  0.8514254683681781,\n",
       "  0.8485473798533804,\n",
       "  0.8481129513983166,\n",
       "  0.8505566114580505,\n",
       "  0.8496877545479229,\n",
       "  0.853814824871029,\n",
       "  0.8529459679609014,\n",
       "  0.8503393972305185,\n",
       "  0.8510996470268802,\n",
       "  0.8518598968232419,\n",
       "  0.8502850936736356,\n",
       "  0.849361933206625,\n",
       "  0.8491990225359761,\n",
       "  0.851588379038827,\n",
       "  0.8508824327993484,\n",
       "  0.851751289709476,\n",
       "  0.8524029323920717,\n",
       "  0.8505566114580505,\n",
       "  0.8484930762964974,\n",
       "  0.8479500407276677,\n",
       "  0.8509910399131143,\n",
       "  0.851696986152593,\n",
       "  0.8514797719250611,\n",
       "  0.85164268259571,\n",
       "  0.8505566114580505,\n",
       "  0.8524572359489546,\n",
       "  0.8514797719250611,\n",
       "  0.849524843877274,\n",
       "  0.8494705403203909,\n",
       "  0.8499592723323378,\n",
       "  0.8531088786315504,\n",
       "  0.8501221830029867,\n",
       "  0.8485473798533804,\n",
       "  0.8526744501764866,\n",
       "  0.8543578604398588,\n",
       "  0.8521314146076568,\n",
       "  0.8505566114580505,\n",
       "  0.8505566114580505,\n",
       "  0.8463209340211784,\n",
       "  0.8525115395058376,\n",
       "  0.8486016834102633,\n",
       "  0.8507195221286994,\n",
       "  0.8531631821884333,\n",
       "  0.849579147434157,\n",
       "  0.8527287537333695,\n",
       "  0.8505023079011675,\n",
       "  0.8530545750746674,\n",
       "  0.8534890035297312,\n",
       "  0.8509910399131143,\n",
       "  0.8533260928590822,\n",
       "  0.851751289709476,\n",
       "  0.8520771110507738,\n",
       "  0.8524029323920717,\n",
       "  0.8509367363562313,\n",
       "  0.849307629649742,\n",
       "  0.85164268259571,\n",
       "  0.8481672549551995,\n",
       "  0.8499049687754547,\n",
       "  0.8520228074938908,\n",
       "  0.851534075481944,\n",
       "  0.8518598968232419,\n",
       "  0.8532717893021993,\n",
       "  0.8506109150149335,\n",
       "  0.8512625576975292,\n",
       "  0.8530545750746674,\n",
       "  0.8524029323920717,\n",
       "  0.851805593266359,\n",
       "  0.8598425196850393,\n",
       "  0.8536519142003801,\n",
       "  0.8508824327993484,\n",
       "  0.85164268259571,\n",
       "  0.85164268259571,\n",
       "  0.8509367363562313,\n",
       "  0.8533803964159652,\n",
       "  0.8501221830029867,\n",
       "  0.8530545750746674,\n",
       "  0.8530545750746674,\n",
       "  0.8531088786315504,\n",
       "  0.8535433070866142,\n",
       "  0.8503937007874016,\n",
       "  0.8528373608471355,\n",
       "  0.851805593266359,\n",
       "  0.8529459679609014,\n",
       "  0.8530545750746674,\n",
       "  0.8504480043442846,\n",
       "  0.8519685039370078,\n",
       "  0.8514254683681781,\n",
       "  0.8547379853380397,\n",
       "  0.8532717893021993,\n",
       "  0.8524572359489546,\n",
       "  0.853760521314146,\n",
       "  0.8497420581048059,\n",
       "  0.8524572359489546,\n",
       "  0.8530545750746674,\n",
       "  0.8506109150149335,\n",
       "  0.8526744501764866,\n",
       "  0.8510453434699973,\n",
       "  0.851534075481944,\n",
       "  0.8524029323920717,\n",
       "  0.8536519142003801,\n",
       "  0.8500135758892208,\n",
       "  0.853760521314146,\n",
       "  0.8518598968232419,\n",
       "  0.849307629649742,\n",
       "  0.8506652185718164,\n",
       "  0.851805593266359,\n",
       "  0.8521857181645398,\n",
       "  0.8552267173499865,\n",
       "  0.851696986152593,\n",
       "  0.8519685039370078,\n",
       "  0.8514797719250611,\n",
       "  0.8526744501764866,\n",
       "  0.8513711648112952,\n",
       "  0.8533260928590822,\n",
       "  0.8534890035297312,\n",
       "  0.8503937007874016,\n",
       "  0.8538691284279121,\n",
       "  0.8510453434699973,\n",
       "  0.8540863426554439,\n",
       "  0.8521314146076568,\n",
       "  0.8520228074938908,\n",
       "  0.8525658430627207,\n",
       "  0.8518598968232419,\n",
       "  0.8541949497692098,\n",
       "  0.8527287537333695,\n",
       "  0.8520228074938908,\n",
       "  0.8526201466196036,\n",
       "  0.8519142003801249,\n",
       "  0.851696986152593,\n",
       "  0.8520771110507738,\n",
       "  0.8536519142003801,\n",
       "  0.851751289709476,\n",
       "  0.85164268259571,\n",
       "  0.8507738256855824,\n",
       "  0.8525658430627207,\n",
       "  0.8557697529188162,\n",
       "  0.85620418137388,\n",
       "  0.8551724137931035,\n",
       "  0.8512082541406462,\n",
       "  0.8520771110507738,\n",
       "  0.8529459679609014,\n",
       "  0.8540863426554439,\n",
       "  0.8535976106434972,\n",
       "  0.8538691284279121,\n",
       "  0.8527830572902525,\n",
       "  0.851696986152593,\n",
       "  0.8506652185718164,\n",
       "  0.8548465924518056,\n",
       "  0.8521314146076568,\n",
       "  0.8512082541406462,\n",
       "  0.8508824327993484,\n",
       "  0.8499049687754547,\n",
       "  0.8509910399131143,\n",
       "  0.8577789845234863,\n",
       "  0.8521314146076568,\n",
       "  0.8510996470268802,\n",
       "  0.8545207711105077,\n",
       "  0.8508824327993484,\n",
       "  0.8514254683681781],\n",
       " 'precision': [0.7445871050722288,\n",
       "  0.7722186598296866,\n",
       "  0.7892364828766106,\n",
       "  0.8046069728467825,\n",
       "  0.7804588521514456,\n",
       "  0.8170819561082628,\n",
       "  0.8247451856605393,\n",
       "  0.8234033001486071,\n",
       "  0.8327636796536749,\n",
       "  0.8179935124735286,\n",
       "  0.8260351246307596,\n",
       "  0.8259579663052602,\n",
       "  0.8278074383799517,\n",
       "  0.8326857715445736,\n",
       "  0.8314911759243091,\n",
       "  0.8271502783910849,\n",
       "  0.8265772248891019,\n",
       "  0.8252914053796186,\n",
       "  0.8247189615436786,\n",
       "  0.8345776113816693,\n",
       "  0.8430270751188328,\n",
       "  0.8430970748752237,\n",
       "  0.8553794779948175,\n",
       "  0.842644527564044,\n",
       "  0.8325737870700176,\n",
       "  0.8455815379609005,\n",
       "  0.8258584392298367,\n",
       "  0.8357044849048275,\n",
       "  0.8275681072697688,\n",
       "  0.8369390673960654,\n",
       "  0.8401053257389673,\n",
       "  0.8365560856956527,\n",
       "  0.8355410703007892,\n",
       "  0.8422421836296085,\n",
       "  0.8470972414255861,\n",
       "  0.8378309598518409,\n",
       "  0.8397492220352586,\n",
       "  0.8375381137519847,\n",
       "  0.8501088491081693,\n",
       "  0.8494890899650609,\n",
       "  0.8483590609023314,\n",
       "  0.842585223139035,\n",
       "  0.840943026601673,\n",
       "  0.8372843799798089,\n",
       "  0.844098599961319,\n",
       "  0.843553142888032,\n",
       "  0.8394020387956383,\n",
       "  0.850317846078848,\n",
       "  0.8477143368879451,\n",
       "  0.8478220018024959,\n",
       "  0.8514603232407514,\n",
       "  0.8442728178096129,\n",
       "  0.8499249747751225,\n",
       "  0.8517669576217832,\n",
       "  0.8446071032293504,\n",
       "  0.8501766691352385,\n",
       "  0.850429270604317,\n",
       "  0.8556703311159432,\n",
       "  0.8487634702936498,\n",
       "  0.8471179439878185,\n",
       "  0.8451399530665427,\n",
       "  0.8465196412178632,\n",
       "  0.8515259303424795,\n",
       "  0.8510412686061882,\n",
       "  0.8519124663993694,\n",
       "  0.8512720868009166,\n",
       "  0.8433180689897936,\n",
       "  0.8462678358908168,\n",
       "  0.8470051776747699,\n",
       "  0.8534973324970275,\n",
       "  0.8426620523740816,\n",
       "  0.8440943403421062,\n",
       "  0.8435198119388335,\n",
       "  0.8453396630131688,\n",
       "  0.8454045641924762,\n",
       "  0.8449681693897771,\n",
       "  0.8491060495294317,\n",
       "  0.8439982736493938,\n",
       "  0.8534688628816472,\n",
       "  0.8474928838845707,\n",
       "  0.8497661296617759,\n",
       "  0.8557792370402574,\n",
       "  0.8460289822460166,\n",
       "  0.8530897878432198,\n",
       "  0.8493320162246647,\n",
       "  0.8484575673162086,\n",
       "  0.8486608234180325,\n",
       "  0.8461053255448804,\n",
       "  0.8534949108539905,\n",
       "  0.8470638650877341,\n",
       "  0.8551282892503979,\n",
       "  0.8524485741835347,\n",
       "  0.8547663174930152,\n",
       "  0.8488960987139063,\n",
       "  0.8494008827588494,\n",
       "  0.8555922018360992,\n",
       "  0.8471998638966127,\n",
       "  0.8496088097346375,\n",
       "  0.8565967307907947,\n",
       "  0.8555948308786224,\n",
       "  0.8464300606939759,\n",
       "  0.8529200624515837,\n",
       "  0.8494902167328777,\n",
       "  0.8598757105343197,\n",
       "  0.855621033403953,\n",
       "  0.8536474546866377,\n",
       "  0.8503274146114655,\n",
       "  0.8487096827523714,\n",
       "  0.8490449930746281,\n",
       "  0.8564154681344208,\n",
       "  0.8479867315252066,\n",
       "  0.8472603855850918,\n",
       "  0.8600253723429995,\n",
       "  0.8591298398286004,\n",
       "  0.844800721433669,\n",
       "  0.8545771066653388,\n",
       "  0.8557734862222016,\n",
       "  0.8487746902305368,\n",
       "  0.8572495986678493,\n",
       "  0.8555704224798649,\n",
       "  0.8474005663665514,\n",
       "  0.8480776662532213,\n",
       "  0.8570983739067013,\n",
       "  0.8530950321192917,\n",
       "  0.8580076077210507,\n",
       "  0.8582620052610012,\n",
       "  0.8484809566229125,\n",
       "  0.8547716023754601,\n",
       "  0.8462088152381783,\n",
       "  0.8585386856214104,\n",
       "  0.8456079785636477,\n",
       "  0.854778167476726,\n",
       "  0.8521361572013463,\n",
       "  0.8481716690385938,\n",
       "  0.849261250231514,\n",
       "  0.8470584614972122,\n",
       "  0.856365650629082,\n",
       "  0.8539488617040094,\n",
       "  0.8509686704467244,\n",
       "  0.8446868692231785,\n",
       "  0.8462477545270813,\n",
       "  0.8524448120678878,\n",
       "  0.84860521058449,\n",
       "  0.8596589121367133,\n",
       "  0.8501456615732689,\n",
       "  0.8539108337483103,\n",
       "  0.8581431982306321,\n",
       "  0.8494593699428149,\n",
       "  0.8460318319801582,\n",
       "  0.858236165074285,\n",
       "  0.8478565870838741,\n",
       "  0.8490351702464211,\n",
       "  0.8535048471838306,\n",
       "  0.8561530689651228,\n",
       "  0.8498022595180607,\n",
       "  0.8492526284359042,\n",
       "  0.8605461452009943,\n",
       "  0.8512580589388747,\n",
       "  0.8527586831489082,\n",
       "  0.8490852314706846,\n",
       "  0.8518823882433445,\n",
       "  0.8494318291549803,\n",
       "  0.8442392258716319,\n",
       "  0.859351975847376,\n",
       "  0.8504310986943233,\n",
       "  0.8471679315716957,\n",
       "  0.8591876981009622,\n",
       "  0.8470331822725571,\n",
       "  0.8470492249902367,\n",
       "  0.8599736620628745,\n",
       "  0.851069040159597,\n",
       "  0.8588902607599561,\n",
       "  0.8595566449734928,\n",
       "  0.8571312960121534,\n",
       "  0.8492375636393396,\n",
       "  0.8500331360696144,\n",
       "  0.8544713067724993,\n",
       "  0.857151549425026,\n",
       "  0.8470231166193546,\n",
       "  0.8544645658733464,\n",
       "  0.8571283504901585,\n",
       "  0.8598896758539508,\n",
       "  0.8613547938431863,\n",
       "  0.8490979375287068,\n",
       "  0.8511404385878567,\n",
       "  0.8552853992492593,\n",
       "  0.8582701552399598,\n",
       "  0.8594330703377784,\n",
       "  0.8500348237828135,\n",
       "  0.8495079959413342,\n",
       "  0.8519491776909378,\n",
       "  0.8579939465009975,\n",
       "  0.8580505239603604,\n",
       "  0.8484027212659506,\n",
       "  0.8500374382811032,\n",
       "  0.8501170597109812,\n",
       "  0.8552312695316573,\n",
       "  0.8487027838365838,\n",
       "  0.8583722518315957,\n",
       "  0.8582031585500004,\n",
       "  0.8540824142404954,\n",
       "  0.8615955114098185,\n",
       "  0.8563940313979356,\n",
       "  0.8516534547465056,\n",
       "  0.8589560073153333,\n",
       "  0.8588639072254307,\n",
       "  0.8475610447253771,\n",
       "  0.8607324625858839,\n",
       "  0.8603736904743818,\n",
       "  0.8571955960232743,\n",
       "  0.8580782138997747,\n",
       "  0.849499761926817,\n",
       "  0.8589898361615433,\n",
       "  0.8519359142896517,\n",
       "  0.8519217972499944,\n",
       "  0.857609177923027,\n",
       "  0.8598958491696952,\n",
       "  0.8498608486336866,\n",
       "  0.859896002808069,\n",
       "  0.8598801955071913,\n",
       "  0.8582505354248883,\n",
       "  0.8583795342076854,\n",
       "  0.8541220634674529,\n",
       "  0.8632794506900928,\n",
       "  0.8600293248674936,\n",
       "  0.8523457387685394,\n",
       "  0.8507543928554896,\n",
       "  0.8605077439158314,\n",
       "  0.8493482888587828,\n",
       "  0.8534129515018388,\n",
       "  0.8573059922738603,\n",
       "  0.8582285304151226,\n",
       "  0.8602791448094912,\n",
       "  0.8595830438174058,\n",
       "  0.8562146868831411,\n",
       "  0.8572841826637867,\n",
       "  0.8586067198426829,\n",
       "  0.8525053172004845,\n",
       "  0.8489279060662157,\n",
       "  0.8610273776575195,\n",
       "  0.8504128347764541,\n",
       "  0.8525527943969351,\n",
       "  0.8612234372541344,\n",
       "  0.858913849930763,\n",
       "  0.8611212816640575,\n",
       "  0.8547566427722133,\n",
       "  0.8517332196141005,\n",
       "  0.8590088667017347,\n",
       "  0.8594028025389329,\n",
       "  0.8516143341984627,\n",
       "  0.8501848525144383,\n",
       "  0.8542419518940657,\n",
       "  0.851470917928669,\n",
       "  0.8588450440196777,\n",
       "  0.8588307682953685,\n",
       "  0.850798940552372,\n",
       "  0.8588594467430556,\n",
       "  0.8500245088046215,\n",
       "  0.85721000294737,\n",
       "  0.8568153686768945,\n",
       "  0.860226934578455,\n",
       "  0.8544936701234275,\n",
       "  0.8514364273006126,\n",
       "  0.8490206738297076,\n",
       "  0.8561466580798857,\n",
       "  0.8624625985963043,\n",
       "  0.8522260173338534,\n",
       "  0.8611509803055682,\n",
       "  0.8491238881455986,\n",
       "  0.8499692398916596,\n",
       "  0.8571706299930156,\n",
       "  0.8504316041761362,\n",
       "  0.8505792621087029,\n",
       "  0.8616315048763918,\n",
       "  0.8537934543510876,\n",
       "  0.8573650171947426,\n",
       "  0.8522224566137329,\n",
       "  0.8573382448977712,\n",
       "  0.8509054765932261,\n",
       "  0.8583649964204758,\n",
       "  0.8560982569736231,\n",
       "  0.8514411381723735,\n",
       "  0.8538540550343147,\n",
       "  0.8500582504013975,\n",
       "  0.8536044862748061,\n",
       "  0.8554910152970652,\n",
       "  0.8581035539219859,\n",
       "  0.8505164106501132,\n",
       "  0.8503094925323991,\n",
       "  0.8603104983632289,\n",
       "  0.858974842338736,\n",
       "  0.8569860536646878,\n",
       "  0.851152082267474,\n",
       "  0.8516143035710374,\n",
       "  0.8504541886417405,\n",
       "  0.8576277048228937,\n",
       "  0.8521393405836469,\n",
       "  0.854515882134992,\n",
       "  0.8491639614343023,\n",
       "  0.8486684065679376,\n",
       "  0.8545592839706632,\n",
       "  0.8569855614278067,\n",
       "  0.8561480103754521,\n",
       "  0.8558842574836394,\n",
       "  0.8516518016222226,\n",
       "  0.8610497572724132,\n",
       "  0.8528465595291291,\n",
       "  0.8617221111081895,\n",
       "  0.8596929555723228,\n",
       "  0.8607860581298535,\n",
       "  0.85517676970916,\n",
       "  0.8490235319019552,\n",
       "  0.85741495236135,\n",
       "  0.8588775481192475,\n",
       "  0.8537927760821566,\n",
       "  0.8493624362182809,\n",
       "  0.8479521612465232,\n",
       "  0.8503967221756915,\n",
       "  0.8524090495926874,\n",
       "  0.8568874631335214,\n",
       "  0.8496085190041699,\n",
       "  0.8513264099810941,\n",
       "  0.8540844564279794,\n",
       "  0.8490476257300608,\n",
       "  0.8502750891125993],\n",
       " 'recall': [0.7264808543973732,\n",
       "  0.7661340218015823,\n",
       "  0.7842759377829814,\n",
       "  0.7655968055540437,\n",
       "  0.7703788124403919,\n",
       "  0.8082569924918016,\n",
       "  0.7999396924341163,\n",
       "  0.8092205744238327,\n",
       "  0.8131499224756794,\n",
       "  0.8130788409188269,\n",
       "  0.815248413984688,\n",
       "  0.8126471112518053,\n",
       "  0.8141454103461289,\n",
       "  0.8144728331979204,\n",
       "  0.8222479338298873,\n",
       "  0.8195966569783874,\n",
       "  0.8203263140749941,\n",
       "  0.8204939189786812,\n",
       "  0.8174987523485863,\n",
       "  0.8202449552915648,\n",
       "  0.8183963633406068,\n",
       "  0.8196114081211047,\n",
       "  0.8024284750673653,\n",
       "  0.8206368089837223,\n",
       "  0.8275933440698787,\n",
       "  0.8272926533438989,\n",
       "  0.818546138749071,\n",
       "  0.827172443441384,\n",
       "  0.8238537566226966,\n",
       "  0.8265723046918768,\n",
       "  0.8311155289932189,\n",
       "  0.8277145988886034,\n",
       "  0.826727910196495,\n",
       "  0.8257096675450226,\n",
       "  0.8288767915108344,\n",
       "  0.8333117779985503,\n",
       "  0.8343166239494452,\n",
       "  0.832583406871183,\n",
       "  0.8303334778258966,\n",
       "  0.8317591374904065,\n",
       "  0.8312246422373243,\n",
       "  0.8363892064998294,\n",
       "  0.8345690217997083,\n",
       "  0.8324924855912977,\n",
       "  0.8378333598967168,\n",
       "  0.8366803740031172,\n",
       "  0.8341290287882673,\n",
       "  0.834426063868186,\n",
       "  0.8333668269085087,\n",
       "  0.832866792655055,\n",
       "  0.8337169181245679,\n",
       "  0.8365888247858572,\n",
       "  0.8311485449473778,\n",
       "  0.8288136835153477,\n",
       "  0.8382414160855175,\n",
       "  0.8385309668062814,\n",
       "  0.8348542096035875,\n",
       "  0.836677624224,\n",
       "  0.8340845002693599,\n",
       "  0.8307217640261575,\n",
       "  0.8380962765632086,\n",
       "  0.8343343638698122,\n",
       "  0.8407207887017717,\n",
       "  0.8378036705672177,\n",
       "  0.8396709499637184,\n",
       "  0.8340666225746051,\n",
       "  0.8404788097662385,\n",
       "  0.8391998523134534,\n",
       "  0.8409708230332879,\n",
       "  0.8338530737926146,\n",
       "  0.837901553659746,\n",
       "  0.8403767443107624,\n",
       "  0.8367848146604078,\n",
       "  0.8402231644055926,\n",
       "  0.8414977320792411,\n",
       "  0.8400977766614118,\n",
       "  0.8396377887076856,\n",
       "  0.8372906169114044,\n",
       "  0.8401557532981218,\n",
       "  0.8378077523670581,\n",
       "  0.8424578602555522,\n",
       "  0.836946034471079,\n",
       "  0.8415473966937387,\n",
       "  0.8460327141291267,\n",
       "  0.8385765366535365,\n",
       "  0.8438087677999091,\n",
       "  0.8434071460867453,\n",
       "  0.842535460771118,\n",
       "  0.8382126008595444,\n",
       "  0.841547319675288,\n",
       "  0.8402898381219029,\n",
       "  0.8422043774693875,\n",
       "  0.841674192128412,\n",
       "  0.8420882492281588,\n",
       "  0.8440880114296373,\n",
       "  0.8404933866701757,\n",
       "  0.8422489915204568,\n",
       "  0.8445769710307139,\n",
       "  0.8440603113298625,\n",
       "  0.843175102474242,\n",
       "  0.8419430242336291,\n",
       "  0.8403013942254539,\n",
       "  0.844221147591338,\n",
       "  0.8419779230966379,\n",
       "  0.8456715847135815,\n",
       "  0.8425097043636341,\n",
       "  0.8442673952436679,\n",
       "  0.843332542768776,\n",
       "  0.8425684718896803,\n",
       "  0.8406549590182261,\n",
       "  0.8432617291340528,\n",
       "  0.8433271953580533,\n",
       "  0.8423291940415304,\n",
       "  0.8417019866158695,\n",
       "  0.8404843253578792,\n",
       "  0.8403715317029451,\n",
       "  0.8434695247333001,\n",
       "  0.8440631983196485,\n",
       "  0.8433682603547243,\n",
       "  0.848413733874246,\n",
       "  0.8428005628700698,\n",
       "  0.8438492156189824,\n",
       "  0.8407851830365652,\n",
       "  0.8404469186373141,\n",
       "  0.8438827242613179,\n",
       "  0.8404558569175994,\n",
       "  0.8423452788325719,\n",
       "  0.8420567840641769,\n",
       "  0.8389074285146703,\n",
       "  0.8418406363542504,\n",
       "  0.8430442491200357,\n",
       "  0.8445228850318002,\n",
       "  0.8392313157673746,\n",
       "  0.8436493626138653,\n",
       "  0.8438112826485124,\n",
       "  0.8418908621980008,\n",
       "  0.8365492717798774,\n",
       "  0.8414349008317789,\n",
       "  0.8409323468567712,\n",
       "  0.840617029255612,\n",
       "  0.8408667856797059,\n",
       "  0.8436815623815644,\n",
       "  0.8442050936339047,\n",
       "  0.8430660762485599,\n",
       "  0.8426389952690051,\n",
       "  0.8473974090434583,\n",
       "  0.8421985398412227,\n",
       "  0.8443802993285453,\n",
       "  0.8409438615240501,\n",
       "  0.8447827456685021,\n",
       "  0.8442928437857505,\n",
       "  0.8444766737682393,\n",
       "  0.8438861662416838,\n",
       "  0.8373866451963868,\n",
       "  0.8419013986665383,\n",
       "  0.8452108480635618,\n",
       "  0.845024894545091,\n",
       "  0.846728684425944,\n",
       "  0.8439976244696492,\n",
       "  0.8450541830902777,\n",
       "  0.843199311144836,\n",
       "  0.845342951802115,\n",
       "  0.8398445619783672,\n",
       "  0.8426030457657818,\n",
       "  0.8470256912173144,\n",
       "  0.8432778140681808,\n",
       "  0.8436945028181336,\n",
       "  0.8428273335529206,\n",
       "  0.842450322798213,\n",
       "  0.8426266865723054,\n",
       "  0.8430921163276203,\n",
       "  0.8466926641291647,\n",
       "  0.8455126617591655,\n",
       "  0.8427445385794113,\n",
       "  0.8455662937965164,\n",
       "  0.8458637728825762,\n",
       "  0.8430838332577543,\n",
       "  0.8415189860821365,\n",
       "  0.8437598312807534,\n",
       "  0.8448632268066937,\n",
       "  0.8437725530518755,\n",
       "  0.8442173191023362,\n",
       "  0.8448991157059025,\n",
       "  0.8448621483464035,\n",
       "  0.841903210446374,\n",
       "  0.8401181199019273,\n",
       "  0.8437081294918433,\n",
       "  0.8440667746089766,\n",
       "  0.8457606077622347,\n",
       "  0.8458501861115386,\n",
       "  0.8438520527903557,\n",
       "  0.8454854312585488,\n",
       "  0.8443941970833224,\n",
       "  0.8434264752430473,\n",
       "  0.8426947356365608,\n",
       "  0.8439545527277691,\n",
       "  0.8461240618071759,\n",
       "  0.8444895796613999,\n",
       "  0.840708952856612,\n",
       "  0.8453782728402488,\n",
       "  0.8482600645996512,\n",
       "  0.8443564131854039,\n",
       "  0.8433698613859132,\n",
       "  0.8437141447192033,\n",
       "  0.8378974983649372,\n",
       "  0.8453056583263409,\n",
       "  0.8433838324183278,\n",
       "  0.8434299186644584,\n",
       "  0.8460200122366116,\n",
       "  0.8421309007366679,\n",
       "  0.8453689207958291,\n",
       "  0.843897767299909,\n",
       "  0.8459438795752955,\n",
       "  0.8478436762652816,\n",
       "  0.8445263240797302,\n",
       "  0.8461678437600706,\n",
       "  0.8440767347196938,\n",
       "  0.8463718282571873,\n",
       "  0.8450450576074818,\n",
       "  0.8431335847355012,\n",
       "  0.8413363213759615,\n",
       "  0.8441039212170479,\n",
       "  0.8406034271454512,\n",
       "  0.8418347594784619,\n",
       "  0.8443254551858965,\n",
       "  0.8449025602883792,\n",
       "  0.846181357733513,\n",
       "  0.8459811731282861,\n",
       "  0.8448762251277263,\n",
       "  0.8441243401730717,\n",
       "  0.8458175204349693,\n",
       "  0.845132705742345,\n",
       "  0.8441867901356396,\n",
       "  0.8542434890926329,\n",
       "  0.8467199479295545,\n",
       "  0.8434417598722623,\n",
       "  0.8441607037587086,\n",
       "  0.8450995934082869,\n",
       "  0.8451985443952891,\n",
       "  0.8460633930227554,\n",
       "  0.8447823181865655,\n",
       "  0.8465139587942349,\n",
       "  0.8455627254232084,\n",
       "  0.8457750339627493,\n",
       "  0.8460233331893443,\n",
       "  0.8434776050851606,\n",
       "  0.8474647827886738,\n",
       "  0.8444329285433939,\n",
       "  0.8457473189430219,\n",
       "  0.8475222267116697,\n",
       "  0.8443695877915522,\n",
       "  0.8451162298602416,\n",
       "  0.8447282910379418,\n",
       "  0.8476612068462984,\n",
       "  0.8458772528132673,\n",
       "  0.846980302585313,\n",
       "  0.8463546640478273,\n",
       "  0.8427901719525978,\n",
       "  0.84516107684964,\n",
       "  0.8459444765905676,\n",
       "  0.8427205117279973,\n",
       "  0.8460645636993179,\n",
       "  0.844162920336441,\n",
       "  0.845818357368978,\n",
       "  0.8456444375929135,\n",
       "  0.8460413947157034,\n",
       "  0.8433151661190698,\n",
       "  0.8460934325426783,\n",
       "  0.8460697578853583,\n",
       "  0.8426531053591245,\n",
       "  0.8437213739565649,\n",
       "  0.8463027234572206,\n",
       "  0.846598178066205,\n",
       "  0.8479579104042889,\n",
       "  0.8453023638696125,\n",
       "  0.8445298244446953,\n",
       "  0.8448111138813023,\n",
       "  0.8455018449184936,\n",
       "  0.8459640370309742,\n",
       "  0.8462637604499094,\n",
       "  0.8464671904287421,\n",
       "  0.8436287417085094,\n",
       "  0.8474739528304033,\n",
       "  0.8444262100498103,\n",
       "  0.8478328740603732,\n",
       "  0.8451166352990663,\n",
       "  0.844613693750198,\n",
       "  0.8468635456470052,\n",
       "  0.8462745090661551,\n",
       "  0.8470221089557012,\n",
       "  0.84542007232886,\n",
       "  0.8448251004234107,\n",
       "  0.846886668644737,\n",
       "  0.8464340098766715,\n",
       "  0.8451623930135783,\n",
       "  0.844819471646472,\n",
       "  0.8481957067454159,\n",
       "  0.8447069576818458,\n",
       "  0.8459306919536819,\n",
       "  0.8451710590148909,\n",
       "  0.845542180399966,\n",
       "  0.849600950466526,\n",
       "  0.8502790885453891,\n",
       "  0.8493064594945864,\n",
       "  0.8446513214114937,\n",
       "  0.8447187496306497,\n",
       "  0.8465930663326701,\n",
       "  0.8466664854963105,\n",
       "  0.8463417574821407,\n",
       "  0.8463838094179499,\n",
       "  0.8459712574862267,\n",
       "  0.8461023349898121,\n",
       "  0.8431455426814871,\n",
       "  0.8477882535370119,\n",
       "  0.8453889077967881,\n",
       "  0.8454455511878138,\n",
       "  0.84538959063728,\n",
       "  0.8429983204652042,\n",
       "  0.8442967596476524,\n",
       "  0.8518156065544208,\n",
       "  0.8466329327258749,\n",
       "  0.8446235217229964,\n",
       "  0.8479569862171518,\n",
       "  0.8454818199890437,\n",
       "  0.8451971998017204],\n",
       " 'f1': [0.7272807408325495,\n",
       "  0.7661816774737217,\n",
       "  0.7845048390651844,\n",
       "  0.7350650557714956,\n",
       "  0.7611753472868683,\n",
       "  0.8094846347036752,\n",
       "  0.7902192420877178,\n",
       "  0.8059212215285707,\n",
       "  0.8086709199528723,\n",
       "  0.8135676393893525,\n",
       "  0.814116810590613,\n",
       "  0.8098205524297302,\n",
       "  0.8154200968034507,\n",
       "  0.8123091509424453,\n",
       "  0.8234160337149462,\n",
       "  0.8201032742809571,\n",
       "  0.8193985231834054,\n",
       "  0.8217269613134888,\n",
       "  0.8180648962336926,\n",
       "  0.818861952731509,\n",
       "  0.8146135203299112,\n",
       "  0.8148586937795881,\n",
       "  0.779014631036474,\n",
       "  0.8175254011648244,\n",
       "  0.828271417121663,\n",
       "  0.8242906698702853,\n",
       "  0.8183986647800401,\n",
       "  0.8279340718684337,\n",
       "  0.8247435355553127,\n",
       "  0.8252666365423234,\n",
       "  0.8317632991657343,\n",
       "  0.8282318633722797,\n",
       "  0.8270759886649889,\n",
       "  0.8226991340370797,\n",
       "  0.8258317189007408,\n",
       "  0.8342639167117872,\n",
       "  0.8353624498960123,\n",
       "  0.8335404143359076,\n",
       "  0.826367916463982,\n",
       "  0.8286319949530411,\n",
       "  0.8292294083863218,\n",
       "  0.8374723068971456,\n",
       "  0.8355424478428656,\n",
       "  0.8322455830920245,\n",
       "  0.8389682534994155,\n",
       "  0.837677647156049,\n",
       "  0.8348144022195192,\n",
       "  0.8316534097221684,\n",
       "  0.8318609328455217,\n",
       "  0.8310176121671281,\n",
       "  0.8306139171385777,\n",
       "  0.8374375037619138,\n",
       "  0.8271524400440912,\n",
       "  0.82270021450186,\n",
       "  0.8392731304160237,\n",
       "  0.8388863067260094,\n",
       "  0.8330929799640617,\n",
       "  0.8337662168536709,\n",
       "  0.8309953940397342,\n",
       "  0.8272386190400578,\n",
       "  0.8390870553839967,\n",
       "  0.832167490939714,\n",
       "  0.8404606792800914,\n",
       "  0.8358765700822586,\n",
       "  0.8390242666001343,\n",
       "  0.8313409135011969,\n",
       "  0.8411863604653691,\n",
       "  0.8402557136067955,\n",
       "  0.8420146955617734,\n",
       "  0.8297324082740895,\n",
       "  0.8377716224806729,\n",
       "  0.8412351841882617,\n",
       "  0.8372078744630124,\n",
       "  0.8412551466318036,\n",
       "  0.842518224494046,\n",
       "  0.8409267405669746,\n",
       "  0.8394767236510253,\n",
       "  0.837810347298789,\n",
       "  0.8389421605623871,\n",
       "  0.8376127070693893,\n",
       "  0.8432112054261015,\n",
       "  0.8347449099108438,\n",
       "  0.8424423341906432,\n",
       "  0.846619426395229,\n",
       "  0.837014810183367,\n",
       "  0.8447272427706675,\n",
       "  0.8442996712709432,\n",
       "  0.8433734457012827,\n",
       "  0.8352381338592023,\n",
       "  0.8425811760730665,\n",
       "  0.8383091703206734,\n",
       "  0.8411220772225181,\n",
       "  0.8402854971181504,\n",
       "  0.8429945991999425,\n",
       "  0.8449676737899695,\n",
       "  0.8383873763658829,\n",
       "  0.8431098045360169,\n",
       "  0.8450213301412495,\n",
       "  0.8427234138387671,\n",
       "  0.8419874790812133,\n",
       "  0.8428255315105235,\n",
       "  0.8382149916024869,\n",
       "  0.8452185722267457,\n",
       "  0.8392561001865569,\n",
       "  0.8462526216282168,\n",
       "  0.8413102605649482,\n",
       "  0.8452108027886457,\n",
       "  0.8441499350626289,\n",
       "  0.8435653763601453,\n",
       "  0.838213612072431,\n",
       "  0.8441678955467312,\n",
       "  0.8442254927480426,\n",
       "  0.8400583885073972,\n",
       "  0.8390642408775654,\n",
       "  0.8412725819056527,\n",
       "  0.8384407934247257,\n",
       "  0.8420696437058762,\n",
       "  0.8449391535441507,\n",
       "  0.8418275067576091,\n",
       "  0.8490538234226498,\n",
       "  0.8437698146103836,\n",
       "  0.8447828519969365,\n",
       "  0.8392017365410385,\n",
       "  0.8381547327713476,\n",
       "  0.8421013807323265,\n",
       "  0.8376164459988629,\n",
       "  0.8427120549828594,\n",
       "  0.8402165874561058,\n",
       "  0.8383031222148292,\n",
       "  0.8392249632684873,\n",
       "  0.8438611015206329,\n",
       "  0.8440002126852587,\n",
       "  0.8371492361599752,\n",
       "  0.8445463609393672,\n",
       "  0.8446926056634082,\n",
       "  0.8427972510760525,\n",
       "  0.8316265459970191,\n",
       "  0.8407519199714817,\n",
       "  0.8411037804495551,\n",
       "  0.841267400182967,\n",
       "  0.8413607954126961,\n",
       "  0.8438551136280693,\n",
       "  0.8450633379333491,\n",
       "  0.8404072002492398,\n",
       "  0.842612935591892,\n",
       "  0.8480895925391089,\n",
       "  0.8398876064596692,\n",
       "  0.8452719281532585,\n",
       "  0.8417026119165735,\n",
       "  0.8433855744299121,\n",
       "  0.8451687792547732,\n",
       "  0.845155620248269,\n",
       "  0.8431043841949718,\n",
       "  0.8329423559689326,\n",
       "  0.841980545783284,\n",
       "  0.8460333880217824,\n",
       "  0.8433618018989438,\n",
       "  0.8476033796811491,\n",
       "  0.8447322874759936,\n",
       "  0.8458911613546256,\n",
       "  0.842676633706998,\n",
       "  0.846188622807614,\n",
       "  0.8403273446836148,\n",
       "  0.8399354458217972,\n",
       "  0.8478754101255911,\n",
       "  0.8440248269193479,\n",
       "  0.8411363014510046,\n",
       "  0.8437057967317695,\n",
       "  0.8431669818171907,\n",
       "  0.839572700449963,\n",
       "  0.8431829590441383,\n",
       "  0.845597236857006,\n",
       "  0.8436504873381564,\n",
       "  0.8404903010815131,\n",
       "  0.846257987098845,\n",
       "  0.8466990156303787,\n",
       "  0.841896532039148,\n",
       "  0.838415801029707,\n",
       "  0.8444358475930135,\n",
       "  0.844567490716219,\n",
       "  0.8425308238033026,\n",
       "  0.8420046537328792,\n",
       "  0.8427198489434691,\n",
       "  0.845693035273591,\n",
       "  0.842129814711821,\n",
       "  0.8370195495482257,\n",
       "  0.8423992448393095,\n",
       "  0.841793557123009,\n",
       "  0.8466384805383842,\n",
       "  0.8466672799204581,\n",
       "  0.8441156035673827,\n",
       "  0.8446781703569596,\n",
       "  0.8432329097129762,\n",
       "  0.844289949082416,\n",
       "  0.8426488340167784,\n",
       "  0.844932618775618,\n",
       "  0.8454690598906095,\n",
       "  0.845306021294903,\n",
       "  0.8374511107139764,\n",
       "  0.8438336859582397,\n",
       "  0.8488032924231428,\n",
       "  0.8414682685092386,\n",
       "  0.8419806939161124,\n",
       "  0.843630065498223,\n",
       "  0.8332251597119651,\n",
       "  0.8438386668946104,\n",
       "  0.8436963719750462,\n",
       "  0.8416102188497114,\n",
       "  0.8446477787318999,\n",
       "  0.8400471430566125,\n",
       "  0.8437174074232366,\n",
       "  0.8439754391851718,\n",
       "  0.8448102474957363,\n",
       "  0.8486911769391496,\n",
       "  0.8448787556186188,\n",
       "  0.8451560261580099,\n",
       "  0.8414917591504129,\n",
       "  0.8471480305937619,\n",
       "  0.8434649591152787,\n",
       "  0.8402484607929609,\n",
       "  0.8378791990042264,\n",
       "  0.8419638179475428,\n",
       "  0.8376122973983534,\n",
       "  0.838011701618926,\n",
       "  0.841715661214621,\n",
       "  0.8449556684047758,\n",
       "  0.8470538107222984,\n",
       "  0.8445463235158864,\n",
       "  0.8457889821177781,\n",
       "  0.8434309083059423,\n",
       "  0.8444083186959275,\n",
       "  0.84360117561988,\n",
       "  0.8418719717969805,\n",
       "  0.8549394290819586,\n",
       "  0.8457946289899536,\n",
       "  0.8413948911581017,\n",
       "  0.84209033443787,\n",
       "  0.8453391816560872,\n",
       "  0.846027130223245,\n",
       "  0.84436358245254,\n",
       "  0.8455421162029971,\n",
       "  0.8466183069339394,\n",
       "  0.8432612618828839,\n",
       "  0.8439887477471512,\n",
       "  0.8437370748617901,\n",
       "  0.842597937922697,\n",
       "  0.8482201819290841,\n",
       "  0.8423909878655998,\n",
       "  0.8441139983147892,\n",
       "  0.8482510437636351,\n",
       "  0.8452656016109389,\n",
       "  0.8446364655383528,\n",
       "  0.8448556577394813,\n",
       "  0.8467863669217858,\n",
       "  0.8442688688537531,\n",
       "  0.8476943747754925,\n",
       "  0.8446230816756056,\n",
       "  0.8421710645620042,\n",
       "  0.8434761112414945,\n",
       "  0.8449361165726931,\n",
       "  0.8394521198371964,\n",
       "  0.8461841963921123,\n",
       "  0.8439222214473605,\n",
       "  0.8466033053447162,\n",
       "  0.8451786538600977,\n",
       "  0.8436884121995792,\n",
       "  0.8433175290069741,\n",
       "  0.8437699515737203,\n",
       "  0.846868931154826,\n",
       "  0.8429351626413607,\n",
       "  0.8428026147572991,\n",
       "  0.8470741586634061,\n",
       "  0.8474688001656442,\n",
       "  0.8464539740775322,\n",
       "  0.8455488236324643,\n",
       "  0.84265775131116,\n",
       "  0.8448750726923705,\n",
       "  0.8443139118081945,\n",
       "  0.8467234116776782,\n",
       "  0.8450791906456095,\n",
       "  0.8456284159817805,\n",
       "  0.8433676266598696,\n",
       "  0.8478345698553685,\n",
       "  0.844626082723298,\n",
       "  0.8483358713282935,\n",
       "  0.8436826343536136,\n",
       "  0.8425609680667474,\n",
       "  0.8476354713936157,\n",
       "  0.8470644911619627,\n",
       "  0.84576944161226,\n",
       "  0.8436439022109198,\n",
       "  0.8435787300152869,\n",
       "  0.8477179732273555,\n",
       "  0.8471221335215512,\n",
       "  0.8453186719572702,\n",
       "  0.8433355046955217,\n",
       "  0.8489646012988055,\n",
       "  0.8437799322090308,\n",
       "  0.8467939174057711,\n",
       "  0.845877942896963,\n",
       "  0.8448431231589346,\n",
       "  0.8500882478874906,\n",
       "  0.8508340251739773,\n",
       "  0.8499373776021398,\n",
       "  0.8448707004794386,\n",
       "  0.8426473388193116,\n",
       "  0.8471175087779731,\n",
       "  0.8445626432907765,\n",
       "  0.8449590552316283,\n",
       "  0.8444068676862151,\n",
       "  0.8453606441226461,\n",
       "  0.8468607317540126,\n",
       "  0.8408300093458232,\n",
       "  0.8468070491708676,\n",
       "  0.8452478740032551,\n",
       "  0.8462098758355486,\n",
       "  0.846060569941356,\n",
       "  0.8425380880795141,\n",
       "  0.8441922165573353,\n",
       "  0.8524413390036852,\n",
       "  0.8473059167130851,\n",
       "  0.8449488102399602,\n",
       "  0.8481379711184663,\n",
       "  0.8461718587463225,\n",
       "  0.8456465611942033]}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "982f6425",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "94c47a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../metrics/torch1/1dcnn.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2cca6e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../artifacts/torch/onedcnn_model_cuda.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bedd30a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, input_len, num_classes):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "    \n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=64, batch_first=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x: (batch, seq_len, 1)\n",
    "        x = x.permute(0, 2, 1)                   # → (batch, 1, seq_len)\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))  # (batch, 64, L-2)\n",
    "        x = torch.relu(self.conv2(x))            # (batch, 128, L-4)\n",
    "        x = self.pool(x)                         # (batch, 128, (L-4)//2)\n",
    "        \n",
    "        # LSTM expects (batch, seq_len, features)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        x = h_n[-1]                              # take last hidden state (batch, 64)\n",
    "        \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "73391455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_LSTM(\n",
      "  (conv1): Conv1d(1, 64, kernel_size=(3,), stride=(1,))\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv1d(64, 128, kernel_size=(3,), stride=(1,))\n",
      "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (lstm): LSTM(128, 64, batch_first=True)\n",
      "  (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_len = X_train_scaled.shape[1]\n",
    "model = CNN_LSTM(input_len, num_classes).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9e7bcebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e57ea7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n",
    "    \"epoch\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "    \"precision\": [],\n",
    "    \"recall\": [],\n",
    "    \"f1\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f6fc7872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300] | Train Loss: 0.3480, Train Acc: 0.8326 | Val Loss: 0.3441, Val Acc: 0.8363 | F1: 0.8315\n",
      "Epoch [2/300] | Train Loss: 0.3456, Train Acc: 0.8339 | Val Loss: 0.3515, Val Acc: 0.8337 | F1: 0.8289\n",
      "Epoch [3/300] | Train Loss: 0.3422, Train Acc: 0.8347 | Val Loss: 0.3523, Val Acc: 0.8301 | F1: 0.8255\n",
      "Epoch [4/300] | Train Loss: 0.3382, Train Acc: 0.8365 | Val Loss: 0.3363, Val Acc: 0.8383 | F1: 0.8334\n",
      "Epoch [5/300] | Train Loss: 0.3367, Train Acc: 0.8369 | Val Loss: 0.3373, Val Acc: 0.8386 | F1: 0.8337\n",
      "Epoch [6/300] | Train Loss: 0.3351, Train Acc: 0.8388 | Val Loss: 0.3395, Val Acc: 0.8368 | F1: 0.8319\n",
      "Epoch [7/300] | Train Loss: 0.3318, Train Acc: 0.8396 | Val Loss: 0.3371, Val Acc: 0.8351 | F1: 0.8238\n",
      "Epoch [8/300] | Train Loss: 0.3285, Train Acc: 0.8419 | Val Loss: 0.3306, Val Acc: 0.8403 | F1: 0.8350\n",
      "Epoch [9/300] | Train Loss: 0.3271, Train Acc: 0.8410 | Val Loss: 0.3308, Val Acc: 0.8430 | F1: 0.8379\n",
      "Epoch [10/300] | Train Loss: 0.3241, Train Acc: 0.8432 | Val Loss: 0.3328, Val Acc: 0.8415 | F1: 0.8317\n",
      "Epoch [11/300] | Train Loss: 0.3208, Train Acc: 0.8449 | Val Loss: 0.3288, Val Acc: 0.8426 | F1: 0.8376\n",
      "Epoch [12/300] | Train Loss: 0.3207, Train Acc: 0.8448 | Val Loss: 0.3230, Val Acc: 0.8445 | F1: 0.8344\n",
      "Epoch [13/300] | Train Loss: 0.3193, Train Acc: 0.8466 | Val Loss: 0.3237, Val Acc: 0.8426 | F1: 0.8305\n",
      "Epoch [14/300] | Train Loss: 0.3188, Train Acc: 0.8477 | Val Loss: 0.3288, Val Acc: 0.8393 | F1: 0.8246\n",
      "Epoch [15/300] | Train Loss: 0.3148, Train Acc: 0.8483 | Val Loss: 0.3219, Val Acc: 0.8443 | F1: 0.8393\n",
      "Epoch [16/300] | Train Loss: 0.3185, Train Acc: 0.8459 | Val Loss: 0.3266, Val Acc: 0.8430 | F1: 0.8306\n",
      "Epoch [17/300] | Train Loss: 0.3131, Train Acc: 0.8489 | Val Loss: 0.3200, Val Acc: 0.8495 | F1: 0.8446\n",
      "Epoch [18/300] | Train Loss: 0.3132, Train Acc: 0.8485 | Val Loss: 0.3260, Val Acc: 0.8443 | F1: 0.8395\n",
      "Epoch [19/300] | Train Loss: 0.3113, Train Acc: 0.8492 | Val Loss: 0.3177, Val Acc: 0.8486 | F1: 0.8437\n",
      "Epoch [20/300] | Train Loss: 0.3088, Train Acc: 0.8509 | Val Loss: 0.3276, Val Acc: 0.8433 | F1: 0.8385\n",
      "Epoch [21/300] | Train Loss: 0.3085, Train Acc: 0.8525 | Val Loss: 0.3244, Val Acc: 0.8450 | F1: 0.8402\n",
      "Epoch [22/300] | Train Loss: 0.3086, Train Acc: 0.8499 | Val Loss: 0.3171, Val Acc: 0.8490 | F1: 0.8441\n",
      "Epoch [23/300] | Train Loss: 0.3053, Train Acc: 0.8532 | Val Loss: 0.3184, Val Acc: 0.8492 | F1: 0.8406\n",
      "Epoch [24/300] | Train Loss: 0.3060, Train Acc: 0.8519 | Val Loss: 0.3214, Val Acc: 0.8466 | F1: 0.8418\n",
      "Epoch [25/300] | Train Loss: 0.3082, Train Acc: 0.8508 | Val Loss: 0.3189, Val Acc: 0.8462 | F1: 0.8350\n",
      "Epoch [26/300] | Train Loss: 0.3047, Train Acc: 0.8520 | Val Loss: 0.3153, Val Acc: 0.8486 | F1: 0.8437\n",
      "Epoch [27/300] | Train Loss: 0.3040, Train Acc: 0.8526 | Val Loss: 0.3161, Val Acc: 0.8493 | F1: 0.8445\n",
      "Epoch [28/300] | Train Loss: 0.3026, Train Acc: 0.8528 | Val Loss: 0.3145, Val Acc: 0.8502 | F1: 0.8452\n",
      "Epoch [29/300] | Train Loss: 0.3037, Train Acc: 0.8518 | Val Loss: 0.3185, Val Acc: 0.8484 | F1: 0.8375\n",
      "Epoch [30/300] | Train Loss: 0.3021, Train Acc: 0.8540 | Val Loss: 0.3148, Val Acc: 0.8496 | F1: 0.8447\n",
      "Epoch [31/300] | Train Loss: 0.3006, Train Acc: 0.8560 | Val Loss: 0.3119, Val Acc: 0.8502 | F1: 0.8419\n",
      "Epoch [32/300] | Train Loss: 0.3024, Train Acc: 0.8527 | Val Loss: 0.3154, Val Acc: 0.8500 | F1: 0.8451\n",
      "Epoch [33/300] | Train Loss: 0.2986, Train Acc: 0.8554 | Val Loss: 0.3205, Val Acc: 0.8484 | F1: 0.8402\n",
      "Epoch [34/300] | Train Loss: 0.2991, Train Acc: 0.8556 | Val Loss: 0.3138, Val Acc: 0.8484 | F1: 0.8374\n",
      "Epoch [35/300] | Train Loss: 0.2993, Train Acc: 0.8567 | Val Loss: 0.3179, Val Acc: 0.8492 | F1: 0.8439\n",
      "Epoch [36/300] | Train Loss: 0.3035, Train Acc: 0.8522 | Val Loss: 0.3154, Val Acc: 0.8494 | F1: 0.8446\n",
      "Epoch [37/300] | Train Loss: 0.2972, Train Acc: 0.8565 | Val Loss: 0.3194, Val Acc: 0.8482 | F1: 0.8432\n",
      "Epoch [38/300] | Train Loss: 0.2940, Train Acc: 0.8572 | Val Loss: 0.3112, Val Acc: 0.8508 | F1: 0.8434\n",
      "Epoch [39/300] | Train Loss: 0.2934, Train Acc: 0.8572 | Val Loss: 0.3137, Val Acc: 0.8496 | F1: 0.8398\n",
      "Epoch [40/300] | Train Loss: 0.2972, Train Acc: 0.8554 | Val Loss: 0.3165, Val Acc: 0.8514 | F1: 0.8449\n",
      "Epoch [41/300] | Train Loss: 0.2933, Train Acc: 0.8560 | Val Loss: 0.3119, Val Acc: 0.8518 | F1: 0.8429\n",
      "Epoch [42/300] | Train Loss: 0.2938, Train Acc: 0.8576 | Val Loss: 0.3114, Val Acc: 0.8525 | F1: 0.8444\n",
      "Epoch [43/300] | Train Loss: 0.2917, Train Acc: 0.8577 | Val Loss: 0.3158, Val Acc: 0.8499 | F1: 0.8414\n",
      "Epoch [44/300] | Train Loss: 0.2925, Train Acc: 0.8590 | Val Loss: 0.3155, Val Acc: 0.8514 | F1: 0.8463\n",
      "Epoch [45/300] | Train Loss: 0.2923, Train Acc: 0.8586 | Val Loss: 0.3131, Val Acc: 0.8504 | F1: 0.8408\n",
      "Epoch [46/300] | Train Loss: 0.2914, Train Acc: 0.8585 | Val Loss: 0.3166, Val Acc: 0.8484 | F1: 0.8435\n",
      "Epoch [47/300] | Train Loss: 0.2912, Train Acc: 0.8572 | Val Loss: 0.3142, Val Acc: 0.8550 | F1: 0.8496\n",
      "Epoch [48/300] | Train Loss: 0.2918, Train Acc: 0.8589 | Val Loss: 0.3123, Val Acc: 0.8490 | F1: 0.8440\n",
      "Epoch [49/300] | Train Loss: 0.2892, Train Acc: 0.8586 | Val Loss: 0.3139, Val Acc: 0.8502 | F1: 0.8453\n",
      "Epoch [50/300] | Train Loss: 0.2905, Train Acc: 0.8591 | Val Loss: 0.3127, Val Acc: 0.8510 | F1: 0.8463\n",
      "Epoch [51/300] | Train Loss: 0.2906, Train Acc: 0.8578 | Val Loss: 0.3115, Val Acc: 0.8518 | F1: 0.8440\n",
      "Epoch [52/300] | Train Loss: 0.2872, Train Acc: 0.8594 | Val Loss: 0.3136, Val Acc: 0.8516 | F1: 0.8443\n",
      "Epoch [53/300] | Train Loss: 0.2872, Train Acc: 0.8617 | Val Loss: 0.3177, Val Acc: 0.8499 | F1: 0.8439\n",
      "Epoch [54/300] | Train Loss: 0.2891, Train Acc: 0.8586 | Val Loss: 0.3147, Val Acc: 0.8535 | F1: 0.8447\n",
      "Epoch [55/300] | Train Loss: 0.2853, Train Acc: 0.8613 | Val Loss: 0.3138, Val Acc: 0.8524 | F1: 0.8472\n",
      "Epoch [56/300] | Train Loss: 0.2888, Train Acc: 0.8580 | Val Loss: 0.3131, Val Acc: 0.8515 | F1: 0.8465\n",
      "Epoch [57/300] | Train Loss: 0.2871, Train Acc: 0.8599 | Val Loss: 0.3154, Val Acc: 0.8503 | F1: 0.8453\n",
      "Epoch [58/300] | Train Loss: 0.2893, Train Acc: 0.8588 | Val Loss: 0.3146, Val Acc: 0.8493 | F1: 0.8443\n",
      "Epoch [59/300] | Train Loss: 0.2855, Train Acc: 0.8595 | Val Loss: 0.3168, Val Acc: 0.8561 | F1: 0.8508\n",
      "Epoch [60/300] | Train Loss: 0.2838, Train Acc: 0.8611 | Val Loss: 0.3155, Val Acc: 0.8530 | F1: 0.8437\n",
      "Epoch [61/300] | Train Loss: 0.2882, Train Acc: 0.8586 | Val Loss: 0.3143, Val Acc: 0.8522 | F1: 0.8472\n",
      "Epoch [62/300] | Train Loss: 0.2827, Train Acc: 0.8608 | Val Loss: 0.3134, Val Acc: 0.8539 | F1: 0.8460\n",
      "Epoch [63/300] | Train Loss: 0.2829, Train Acc: 0.8605 | Val Loss: 0.3227, Val Acc: 0.8445 | F1: 0.8388\n",
      "Epoch [64/300] | Train Loss: 0.2865, Train Acc: 0.8598 | Val Loss: 0.3129, Val Acc: 0.8512 | F1: 0.8463\n",
      "Epoch [65/300] | Train Loss: 0.2821, Train Acc: 0.8621 | Val Loss: 0.3123, Val Acc: 0.8532 | F1: 0.8441\n",
      "Epoch [66/300] | Train Loss: 0.2815, Train Acc: 0.8625 | Val Loss: 0.3158, Val Acc: 0.8517 | F1: 0.8465\n",
      "Epoch [67/300] | Train Loss: 0.2800, Train Acc: 0.8626 | Val Loss: 0.3135, Val Acc: 0.8534 | F1: 0.8485\n",
      "Epoch [68/300] | Train Loss: 0.2818, Train Acc: 0.8615 | Val Loss: 0.3240, Val Acc: 0.8523 | F1: 0.8435\n",
      "Epoch [69/300] | Train Loss: 0.2806, Train Acc: 0.8621 | Val Loss: 0.3123, Val Acc: 0.8527 | F1: 0.8476\n",
      "Epoch [70/300] | Train Loss: 0.2807, Train Acc: 0.8610 | Val Loss: 0.3155, Val Acc: 0.8513 | F1: 0.8464\n",
      "Epoch [71/300] | Train Loss: 0.2808, Train Acc: 0.8618 | Val Loss: 0.3181, Val Acc: 0.8515 | F1: 0.8466\n",
      "Epoch [72/300] | Train Loss: 0.2802, Train Acc: 0.8625 | Val Loss: 0.3124, Val Acc: 0.8553 | F1: 0.8471\n",
      "Epoch [73/300] | Train Loss: 0.2778, Train Acc: 0.8649 | Val Loss: 0.3190, Val Acc: 0.8518 | F1: 0.8423\n",
      "Epoch [74/300] | Train Loss: 0.2775, Train Acc: 0.8639 | Val Loss: 0.3136, Val Acc: 0.8545 | F1: 0.8469\n",
      "Epoch [75/300] | Train Loss: 0.2776, Train Acc: 0.8628 | Val Loss: 0.3212, Val Acc: 0.8516 | F1: 0.8429\n",
      "Epoch [76/300] | Train Loss: 0.2780, Train Acc: 0.8645 | Val Loss: 0.3179, Val Acc: 0.8527 | F1: 0.8429\n",
      "Epoch [77/300] | Train Loss: 0.2790, Train Acc: 0.8629 | Val Loss: 0.3149, Val Acc: 0.8522 | F1: 0.8473\n",
      "Epoch [78/300] | Train Loss: 0.2778, Train Acc: 0.8631 | Val Loss: 0.3145, Val Acc: 0.8538 | F1: 0.8487\n",
      "Epoch [79/300] | Train Loss: 0.2757, Train Acc: 0.8635 | Val Loss: 0.3208, Val Acc: 0.8521 | F1: 0.8425\n",
      "Epoch [80/300] | Train Loss: 0.2765, Train Acc: 0.8643 | Val Loss: 0.3144, Val Acc: 0.8526 | F1: 0.8479\n",
      "Epoch [81/300] | Train Loss: 0.2755, Train Acc: 0.8641 | Val Loss: 0.3203, Val Acc: 0.8513 | F1: 0.8412\n",
      "Epoch [82/300] | Train Loss: 0.2764, Train Acc: 0.8641 | Val Loss: 0.3131, Val Acc: 0.8548 | F1: 0.8463\n",
      "Epoch [83/300] | Train Loss: 0.2754, Train Acc: 0.8641 | Val Loss: 0.3227, Val Acc: 0.8517 | F1: 0.8424\n",
      "Epoch [84/300] | Train Loss: 0.2765, Train Acc: 0.8643 | Val Loss: 0.3174, Val Acc: 0.8547 | F1: 0.8455\n",
      "Epoch [85/300] | Train Loss: 0.2735, Train Acc: 0.8653 | Val Loss: 0.3193, Val Acc: 0.8527 | F1: 0.8480\n",
      "Epoch [86/300] | Train Loss: 0.2751, Train Acc: 0.8645 | Val Loss: 0.3314, Val Acc: 0.8486 | F1: 0.8436\n",
      "Epoch [87/300] | Train Loss: 0.2731, Train Acc: 0.8654 | Val Loss: 0.3182, Val Acc: 0.8500 | F1: 0.8427\n",
      "Epoch [88/300] | Train Loss: 0.2733, Train Acc: 0.8660 | Val Loss: 0.3152, Val Acc: 0.8521 | F1: 0.8461\n",
      "Epoch [89/300] | Train Loss: 0.2730, Train Acc: 0.8654 | Val Loss: 0.3190, Val Acc: 0.8538 | F1: 0.8442\n",
      "Epoch [90/300] | Train Loss: 0.2725, Train Acc: 0.8652 | Val Loss: 0.3182, Val Acc: 0.8542 | F1: 0.8450\n",
      "Epoch [91/300] | Train Loss: 0.2704, Train Acc: 0.8660 | Val Loss: 0.3197, Val Acc: 0.8556 | F1: 0.8474\n",
      "Epoch [92/300] | Train Loss: 0.2702, Train Acc: 0.8666 | Val Loss: 0.3184, Val Acc: 0.8527 | F1: 0.8431\n",
      "Epoch [93/300] | Train Loss: 0.2700, Train Acc: 0.8674 | Val Loss: 0.3160, Val Acc: 0.8542 | F1: 0.8459\n",
      "Epoch [94/300] | Train Loss: 0.2706, Train Acc: 0.8665 | Val Loss: 0.3191, Val Acc: 0.8539 | F1: 0.8453\n",
      "Epoch [95/300] | Train Loss: 0.2710, Train Acc: 0.8648 | Val Loss: 0.3212, Val Acc: 0.8524 | F1: 0.8476\n",
      "Epoch [96/300] | Train Loss: 0.2709, Train Acc: 0.8655 | Val Loss: 0.3181, Val Acc: 0.8520 | F1: 0.8473\n",
      "Epoch [97/300] | Train Loss: 0.2691, Train Acc: 0.8664 | Val Loss: 0.3201, Val Acc: 0.8526 | F1: 0.8478\n",
      "Epoch [98/300] | Train Loss: 0.2689, Train Acc: 0.8668 | Val Loss: 0.3242, Val Acc: 0.8521 | F1: 0.8420\n",
      "Epoch [99/300] | Train Loss: 0.2691, Train Acc: 0.8658 | Val Loss: 0.3285, Val Acc: 0.8544 | F1: 0.8459\n",
      "Epoch [100/300] | Train Loss: 0.2676, Train Acc: 0.8680 | Val Loss: 0.3257, Val Acc: 0.8531 | F1: 0.8438\n",
      "Epoch [101/300] | Train Loss: 0.2680, Train Acc: 0.8671 | Val Loss: 0.3250, Val Acc: 0.8518 | F1: 0.8439\n",
      "Epoch [102/300] | Train Loss: 0.2703, Train Acc: 0.8673 | Val Loss: 0.3264, Val Acc: 0.8541 | F1: 0.8459\n",
      "Epoch [103/300] | Train Loss: 0.2670, Train Acc: 0.8673 | Val Loss: 0.3266, Val Acc: 0.8511 | F1: 0.8462\n",
      "Epoch [104/300] | Train Loss: 0.2676, Train Acc: 0.8681 | Val Loss: 0.3236, Val Acc: 0.8554 | F1: 0.8473\n",
      "Epoch [105/300] | Train Loss: 0.2667, Train Acc: 0.8680 | Val Loss: 0.3280, Val Acc: 0.8518 | F1: 0.8465\n",
      "Epoch [106/300] | Train Loss: 0.2687, Train Acc: 0.8670 | Val Loss: 0.3215, Val Acc: 0.8541 | F1: 0.8453\n",
      "Epoch [107/300] | Train Loss: 0.2656, Train Acc: 0.8680 | Val Loss: 0.3275, Val Acc: 0.8544 | F1: 0.8452\n",
      "Epoch [108/300] | Train Loss: 0.2640, Train Acc: 0.8686 | Val Loss: 0.3273, Val Acc: 0.8537 | F1: 0.8488\n",
      "Epoch [109/300] | Train Loss: 0.2667, Train Acc: 0.8678 | Val Loss: 0.3296, Val Acc: 0.8564 | F1: 0.8494\n",
      "Epoch [110/300] | Train Loss: 0.2645, Train Acc: 0.8691 | Val Loss: 0.3257, Val Acc: 0.8529 | F1: 0.8480\n",
      "Epoch [111/300] | Train Loss: 0.2658, Train Acc: 0.8683 | Val Loss: 0.3270, Val Acc: 0.8557 | F1: 0.8473\n",
      "Epoch [112/300] | Train Loss: 0.2629, Train Acc: 0.8691 | Val Loss: 0.3302, Val Acc: 0.8531 | F1: 0.8443\n",
      "Epoch [113/300] | Train Loss: 0.2649, Train Acc: 0.8668 | Val Loss: 0.3300, Val Acc: 0.8550 | F1: 0.8468\n",
      "Epoch [114/300] | Train Loss: 0.2644, Train Acc: 0.8694 | Val Loss: 0.3318, Val Acc: 0.8557 | F1: 0.8483\n",
      "Epoch [115/300] | Train Loss: 0.2635, Train Acc: 0.8687 | Val Loss: 0.3367, Val Acc: 0.8530 | F1: 0.8430\n",
      "Epoch [116/300] | Train Loss: 0.2637, Train Acc: 0.8684 | Val Loss: 0.3223, Val Acc: 0.8559 | F1: 0.8497\n",
      "Epoch [117/300] | Train Loss: 0.2613, Train Acc: 0.8703 | Val Loss: 0.3236, Val Acc: 0.8557 | F1: 0.8513\n",
      "Epoch [118/300] | Train Loss: 0.2632, Train Acc: 0.8691 | Val Loss: 0.3349, Val Acc: 0.8555 | F1: 0.8464\n",
      "Epoch [119/300] | Train Loss: 0.2604, Train Acc: 0.8714 | Val Loss: 0.3338, Val Acc: 0.8512 | F1: 0.8413\n",
      "Epoch [120/300] | Train Loss: 0.2633, Train Acc: 0.8688 | Val Loss: 0.3348, Val Acc: 0.8549 | F1: 0.8463\n",
      "Epoch [121/300] | Train Loss: 0.2618, Train Acc: 0.8696 | Val Loss: 0.3342, Val Acc: 0.8534 | F1: 0.8438\n",
      "Epoch [122/300] | Train Loss: 0.2603, Train Acc: 0.8712 | Val Loss: 0.3318, Val Acc: 0.8533 | F1: 0.8486\n",
      "Epoch [123/300] | Train Loss: 0.2599, Train Acc: 0.8717 | Val Loss: 0.3271, Val Acc: 0.8537 | F1: 0.8489\n",
      "Epoch [124/300] | Train Loss: 0.2585, Train Acc: 0.8709 | Val Loss: 0.3334, Val Acc: 0.8532 | F1: 0.8453\n",
      "Epoch [125/300] | Train Loss: 0.2604, Train Acc: 0.8705 | Val Loss: 0.3306, Val Acc: 0.8539 | F1: 0.8458\n",
      "Epoch [126/300] | Train Loss: 0.2591, Train Acc: 0.8717 | Val Loss: 0.3326, Val Acc: 0.8597 | F1: 0.8534\n",
      "Epoch [127/300] | Train Loss: 0.2621, Train Acc: 0.8689 | Val Loss: 0.3381, Val Acc: 0.8563 | F1: 0.8483\n",
      "Epoch [128/300] | Train Loss: 0.2602, Train Acc: 0.8708 | Val Loss: 0.3407, Val Acc: 0.8477 | F1: 0.8427\n",
      "Epoch [129/300] | Train Loss: 0.2586, Train Acc: 0.8712 | Val Loss: 0.3372, Val Acc: 0.8554 | F1: 0.8509\n",
      "Epoch [130/300] | Train Loss: 0.2585, Train Acc: 0.8721 | Val Loss: 0.3293, Val Acc: 0.8533 | F1: 0.8485\n",
      "Epoch [131/300] | Train Loss: 0.2609, Train Acc: 0.8706 | Val Loss: 0.3333, Val Acc: 0.8611 | F1: 0.8560\n",
      "Epoch [132/300] | Train Loss: 0.2583, Train Acc: 0.8714 | Val Loss: 0.3346, Val Acc: 0.8614 | F1: 0.8559\n",
      "Epoch [133/300] | Train Loss: 0.2586, Train Acc: 0.8717 | Val Loss: 0.3386, Val Acc: 0.8561 | F1: 0.8493\n",
      "Epoch [134/300] | Train Loss: 0.2578, Train Acc: 0.8713 | Val Loss: 0.3377, Val Acc: 0.8543 | F1: 0.8495\n",
      "Epoch [135/300] | Train Loss: 0.2569, Train Acc: 0.8716 | Val Loss: 0.3494, Val Acc: 0.8506 | F1: 0.8414\n",
      "Epoch [136/300] | Train Loss: 0.2593, Train Acc: 0.8734 | Val Loss: 0.3441, Val Acc: 0.8494 | F1: 0.8444\n",
      "Epoch [137/300] | Train Loss: 0.2563, Train Acc: 0.8725 | Val Loss: 0.3450, Val Acc: 0.8464 | F1: 0.8399\n",
      "Epoch [138/300] | Train Loss: 0.2568, Train Acc: 0.8726 | Val Loss: 0.3399, Val Acc: 0.8544 | F1: 0.8460\n",
      "Epoch [139/300] | Train Loss: 0.2559, Train Acc: 0.8736 | Val Loss: 0.3448, Val Acc: 0.8564 | F1: 0.8490\n",
      "Epoch [140/300] | Train Loss: 0.2549, Train Acc: 0.8746 | Val Loss: 0.3463, Val Acc: 0.8455 | F1: 0.8403\n",
      "Epoch [141/300] | Train Loss: 0.2548, Train Acc: 0.8723 | Val Loss: 0.3497, Val Acc: 0.8457 | F1: 0.8393\n",
      "Epoch [142/300] | Train Loss: 0.2564, Train Acc: 0.8717 | Val Loss: 0.3414, Val Acc: 0.8571 | F1: 0.8512\n",
      "Epoch [143/300] | Train Loss: 0.2598, Train Acc: 0.8697 | Val Loss: 0.3475, Val Acc: 0.8504 | F1: 0.8416\n",
      "Epoch [144/300] | Train Loss: 0.2559, Train Acc: 0.8711 | Val Loss: 0.3458, Val Acc: 0.8482 | F1: 0.8432\n",
      "Epoch [145/300] | Train Loss: 0.2577, Train Acc: 0.8726 | Val Loss: 0.3377, Val Acc: 0.8575 | F1: 0.8523\n",
      "Epoch [146/300] | Train Loss: 0.2565, Train Acc: 0.8729 | Val Loss: 0.3382, Val Acc: 0.8576 | F1: 0.8514\n",
      "Epoch [147/300] | Train Loss: 0.2523, Train Acc: 0.8750 | Val Loss: 0.3479, Val Acc: 0.8593 | F1: 0.8531\n",
      "Epoch [148/300] | Train Loss: 0.2535, Train Acc: 0.8730 | Val Loss: 0.3488, Val Acc: 0.8589 | F1: 0.8542\n",
      "Epoch [149/300] | Train Loss: 0.2543, Train Acc: 0.8732 | Val Loss: 0.3513, Val Acc: 0.8584 | F1: 0.8540\n",
      "Epoch [150/300] | Train Loss: 0.2558, Train Acc: 0.8723 | Val Loss: 0.3399, Val Acc: 0.8557 | F1: 0.8491\n",
      "Epoch [151/300] | Train Loss: 0.2540, Train Acc: 0.8734 | Val Loss: 0.3465, Val Acc: 0.8541 | F1: 0.8453\n",
      "Epoch [152/300] | Train Loss: 0.2553, Train Acc: 0.8735 | Val Loss: 0.3384, Val Acc: 0.8533 | F1: 0.8486\n",
      "Epoch [153/300] | Train Loss: 0.2527, Train Acc: 0.8750 | Val Loss: 0.3542, Val Acc: 0.8533 | F1: 0.8490\n",
      "Epoch [154/300] | Train Loss: 0.2563, Train Acc: 0.8742 | Val Loss: 0.3524, Val Acc: 0.8535 | F1: 0.8444\n",
      "Epoch [155/300] | Train Loss: 0.2541, Train Acc: 0.8738 | Val Loss: 0.3534, Val Acc: 0.8535 | F1: 0.8489\n",
      "Epoch [156/300] | Train Loss: 0.2526, Train Acc: 0.8734 | Val Loss: 0.3457, Val Acc: 0.8552 | F1: 0.8476\n",
      "Epoch [157/300] | Train Loss: 0.2491, Train Acc: 0.8759 | Val Loss: 0.3478, Val Acc: 0.8579 | F1: 0.8519\n",
      "Epoch [158/300] | Train Loss: 0.2512, Train Acc: 0.8759 | Val Loss: 0.3538, Val Acc: 0.8540 | F1: 0.8464\n",
      "Epoch [159/300] | Train Loss: 0.2511, Train Acc: 0.8752 | Val Loss: 0.3587, Val Acc: 0.8557 | F1: 0.8498\n",
      "Epoch [160/300] | Train Loss: 0.2516, Train Acc: 0.8742 | Val Loss: 0.3576, Val Acc: 0.8499 | F1: 0.8451\n",
      "Epoch [161/300] | Train Loss: 0.2509, Train Acc: 0.8754 | Val Loss: 0.3549, Val Acc: 0.8512 | F1: 0.8428\n",
      "Epoch [162/300] | Train Loss: 0.2509, Train Acc: 0.8727 | Val Loss: 0.3598, Val Acc: 0.8521 | F1: 0.8432\n",
      "Epoch [163/300] | Train Loss: 0.2500, Train Acc: 0.8759 | Val Loss: 0.3550, Val Acc: 0.8538 | F1: 0.8489\n",
      "Epoch [164/300] | Train Loss: 0.2517, Train Acc: 0.8746 | Val Loss: 0.3591, Val Acc: 0.8573 | F1: 0.8526\n",
      "Epoch [165/300] | Train Loss: 0.2510, Train Acc: 0.8755 | Val Loss: 0.3432, Val Acc: 0.8624 | F1: 0.8571\n",
      "Epoch [166/300] | Train Loss: 0.2492, Train Acc: 0.8754 | Val Loss: 0.3589, Val Acc: 0.8475 | F1: 0.8423\n",
      "Epoch [167/300] | Train Loss: 0.2501, Train Acc: 0.8756 | Val Loss: 0.3591, Val Acc: 0.8546 | F1: 0.8481\n",
      "Epoch [168/300] | Train Loss: 0.2509, Train Acc: 0.8750 | Val Loss: 0.3496, Val Acc: 0.8545 | F1: 0.8466\n",
      "Epoch [169/300] | Train Loss: 0.2516, Train Acc: 0.8749 | Val Loss: 0.3553, Val Acc: 0.8540 | F1: 0.8455\n",
      "Epoch [170/300] | Train Loss: 0.2515, Train Acc: 0.8744 | Val Loss: 0.3549, Val Acc: 0.8584 | F1: 0.8521\n",
      "Epoch [171/300] | Train Loss: 0.2512, Train Acc: 0.8738 | Val Loss: 0.3599, Val Acc: 0.8521 | F1: 0.8440\n",
      "Epoch [172/300] | Train Loss: 0.2499, Train Acc: 0.8745 | Val Loss: 0.3621, Val Acc: 0.8599 | F1: 0.8545\n",
      "Epoch [173/300] | Train Loss: 0.2484, Train Acc: 0.8764 | Val Loss: 0.3609, Val Acc: 0.8573 | F1: 0.8504\n",
      "Epoch [174/300] | Train Loss: 0.2517, Train Acc: 0.8729 | Val Loss: 0.3574, Val Acc: 0.8570 | F1: 0.8527\n",
      "Epoch [175/300] | Train Loss: 0.2509, Train Acc: 0.8756 | Val Loss: 0.3499, Val Acc: 0.8532 | F1: 0.8483\n",
      "Epoch [176/300] | Train Loss: 0.2510, Train Acc: 0.8746 | Val Loss: 0.3658, Val Acc: 0.8500 | F1: 0.8417\n",
      "Epoch [177/300] | Train Loss: 0.2500, Train Acc: 0.8752 | Val Loss: 0.3599, Val Acc: 0.8603 | F1: 0.8546\n",
      "Epoch [178/300] | Train Loss: 0.2481, Train Acc: 0.8761 | Val Loss: 0.3666, Val Acc: 0.8529 | F1: 0.8480\n",
      "Epoch [179/300] | Train Loss: 0.2490, Train Acc: 0.8759 | Val Loss: 0.3502, Val Acc: 0.8525 | F1: 0.8477\n",
      "Epoch [180/300] | Train Loss: 0.2474, Train Acc: 0.8749 | Val Loss: 0.3543, Val Acc: 0.8552 | F1: 0.8473\n",
      "Epoch [181/300] | Train Loss: 0.2472, Train Acc: 0.8764 | Val Loss: 0.3570, Val Acc: 0.8545 | F1: 0.8466\n",
      "Epoch [182/300] | Train Loss: 0.2488, Train Acc: 0.8767 | Val Loss: 0.3637, Val Acc: 0.8546 | F1: 0.8454\n",
      "Epoch [183/300] | Train Loss: 0.2477, Train Acc: 0.8755 | Val Loss: 0.3623, Val Acc: 0.8528 | F1: 0.8481\n",
      "Epoch [184/300] | Train Loss: 0.2469, Train Acc: 0.8753 | Val Loss: 0.3628, Val Acc: 0.8553 | F1: 0.8476\n",
      "Epoch [185/300] | Train Loss: 0.2494, Train Acc: 0.8746 | Val Loss: 0.3714, Val Acc: 0.8507 | F1: 0.8433\n",
      "Epoch [186/300] | Train Loss: 0.2460, Train Acc: 0.8774 | Val Loss: 0.3736, Val Acc: 0.8575 | F1: 0.8505\n",
      "Epoch [187/300] | Train Loss: 0.2457, Train Acc: 0.8767 | Val Loss: 0.3759, Val Acc: 0.8581 | F1: 0.8507\n",
      "Epoch [188/300] | Train Loss: 0.2462, Train Acc: 0.8763 | Val Loss: 0.3714, Val Acc: 0.8526 | F1: 0.8478\n",
      "Epoch [189/300] | Train Loss: 0.2460, Train Acc: 0.8779 | Val Loss: 0.3633, Val Acc: 0.8522 | F1: 0.8466\n",
      "Epoch [190/300] | Train Loss: 0.2470, Train Acc: 0.8766 | Val Loss: 0.3760, Val Acc: 0.8525 | F1: 0.8448\n",
      "Epoch [191/300] | Train Loss: 0.2473, Train Acc: 0.8784 | Val Loss: 0.3750, Val Acc: 0.8581 | F1: 0.8519\n",
      "Epoch [192/300] | Train Loss: 0.2474, Train Acc: 0.8766 | Val Loss: 0.3737, Val Acc: 0.8582 | F1: 0.8539\n",
      "Epoch [193/300] | Train Loss: 0.2498, Train Acc: 0.8738 | Val Loss: 0.3788, Val Acc: 0.8543 | F1: 0.8460\n",
      "Epoch [194/300] | Train Loss: 0.2442, Train Acc: 0.8772 | Val Loss: 0.3686, Val Acc: 0.8544 | F1: 0.8496\n",
      "Epoch [195/300] | Train Loss: 0.2437, Train Acc: 0.8756 | Val Loss: 0.3794, Val Acc: 0.8558 | F1: 0.8514\n",
      "Epoch [196/300] | Train Loss: 0.2457, Train Acc: 0.8764 | Val Loss: 0.3739, Val Acc: 0.8568 | F1: 0.8524\n",
      "Epoch [197/300] | Train Loss: 0.2434, Train Acc: 0.8793 | Val Loss: 0.3892, Val Acc: 0.8540 | F1: 0.8457\n",
      "Epoch [198/300] | Train Loss: 0.2459, Train Acc: 0.8784 | Val Loss: 0.3667, Val Acc: 0.8613 | F1: 0.8558\n",
      "Epoch [199/300] | Train Loss: 0.2463, Train Acc: 0.8749 | Val Loss: 0.3752, Val Acc: 0.8533 | F1: 0.8486\n",
      "Epoch [200/300] | Train Loss: 0.2474, Train Acc: 0.8746 | Val Loss: 0.3800, Val Acc: 0.8520 | F1: 0.8470\n",
      "Epoch [201/300] | Train Loss: 0.2451, Train Acc: 0.8779 | Val Loss: 0.3660, Val Acc: 0.8588 | F1: 0.8530\n",
      "Epoch [202/300] | Train Loss: 0.2438, Train Acc: 0.8758 | Val Loss: 0.3793, Val Acc: 0.8481 | F1: 0.8429\n",
      "Epoch [203/300] | Train Loss: 0.2454, Train Acc: 0.8769 | Val Loss: 0.3716, Val Acc: 0.8615 | F1: 0.8554\n",
      "Epoch [204/300] | Train Loss: 0.2441, Train Acc: 0.8768 | Val Loss: 0.3698, Val Acc: 0.8582 | F1: 0.8518\n",
      "Epoch [205/300] | Train Loss: 0.2413, Train Acc: 0.8816 | Val Loss: 0.3745, Val Acc: 0.8607 | F1: 0.8554\n",
      "Epoch [206/300] | Train Loss: 0.2439, Train Acc: 0.8779 | Val Loss: 0.3821, Val Acc: 0.8521 | F1: 0.8471\n",
      "Epoch [207/300] | Train Loss: 0.2440, Train Acc: 0.8776 | Val Loss: 0.3704, Val Acc: 0.8576 | F1: 0.8529\n",
      "Epoch [208/300] | Train Loss: 0.2460, Train Acc: 0.8758 | Val Loss: 0.3735, Val Acc: 0.8531 | F1: 0.8449\n",
      "Epoch [209/300] | Train Loss: 0.2435, Train Acc: 0.8782 | Val Loss: 0.3826, Val Acc: 0.8537 | F1: 0.8487\n",
      "Epoch [210/300] | Train Loss: 0.2427, Train Acc: 0.8768 | Val Loss: 0.3901, Val Acc: 0.8540 | F1: 0.8458\n",
      "Epoch [211/300] | Train Loss: 0.2412, Train Acc: 0.8788 | Val Loss: 0.3740, Val Acc: 0.8614 | F1: 0.8558\n",
      "Epoch [212/300] | Train Loss: 0.2434, Train Acc: 0.8782 | Val Loss: 0.3885, Val Acc: 0.8546 | F1: 0.8462\n",
      "Epoch [213/300] | Train Loss: 0.2438, Train Acc: 0.8762 | Val Loss: 0.3884, Val Acc: 0.8575 | F1: 0.8527\n",
      "Epoch [214/300] | Train Loss: 0.2425, Train Acc: 0.8771 | Val Loss: 0.3946, Val Acc: 0.8544 | F1: 0.8466\n",
      "Epoch [215/300] | Train Loss: 0.2440, Train Acc: 0.8772 | Val Loss: 0.3751, Val Acc: 0.8523 | F1: 0.8475\n",
      "Epoch [216/300] | Train Loss: 0.2420, Train Acc: 0.8773 | Val Loss: 0.3847, Val Acc: 0.8605 | F1: 0.8551\n",
      "Epoch [217/300] | Train Loss: 0.2403, Train Acc: 0.8799 | Val Loss: 0.3809, Val Acc: 0.8501 | F1: 0.8451\n",
      "Epoch [218/300] | Train Loss: 0.2402, Train Acc: 0.8807 | Val Loss: 0.3756, Val Acc: 0.8566 | F1: 0.8523\n",
      "Epoch [219/300] | Train Loss: 0.2475, Train Acc: 0.8768 | Val Loss: 0.3809, Val Acc: 0.8518 | F1: 0.8438\n",
      "Epoch [220/300] | Train Loss: 0.2436, Train Acc: 0.8775 | Val Loss: 0.3887, Val Acc: 0.8521 | F1: 0.8457\n",
      "Epoch [221/300] | Train Loss: 0.2406, Train Acc: 0.8784 | Val Loss: 0.3813, Val Acc: 0.8581 | F1: 0.8519\n",
      "Epoch [222/300] | Train Loss: 0.2418, Train Acc: 0.8783 | Val Loss: 0.3875, Val Acc: 0.8535 | F1: 0.8455\n",
      "Epoch [223/300] | Train Loss: 0.2437, Train Acc: 0.8770 | Val Loss: 0.3794, Val Acc: 0.8527 | F1: 0.8436\n",
      "Epoch [224/300] | Train Loss: 0.2474, Train Acc: 0.8769 | Val Loss: 0.3911, Val Acc: 0.8541 | F1: 0.8494\n",
      "Epoch [225/300] | Train Loss: 0.2415, Train Acc: 0.8765 | Val Loss: 0.3863, Val Acc: 0.8554 | F1: 0.8470\n",
      "Epoch [226/300] | Train Loss: 0.2433, Train Acc: 0.8776 | Val Loss: 0.3805, Val Acc: 0.8469 | F1: 0.8417\n",
      "Epoch [227/300] | Train Loss: 0.2404, Train Acc: 0.8779 | Val Loss: 0.3941, Val Acc: 0.8614 | F1: 0.8549\n",
      "Epoch [228/300] | Train Loss: 0.2425, Train Acc: 0.8770 | Val Loss: 0.3900, Val Acc: 0.8504 | F1: 0.8414\n",
      "Epoch [229/300] | Train Loss: 0.2450, Train Acc: 0.8763 | Val Loss: 0.3956, Val Acc: 0.8520 | F1: 0.8430\n",
      "Epoch [230/300] | Train Loss: 0.2405, Train Acc: 0.8782 | Val Loss: 0.3871, Val Acc: 0.8552 | F1: 0.8470\n",
      "Epoch [231/300] | Train Loss: 0.2399, Train Acc: 0.8787 | Val Loss: 0.3882, Val Acc: 0.8525 | F1: 0.8475\n",
      "Epoch [232/300] | Train Loss: 0.2393, Train Acc: 0.8786 | Val Loss: 0.3943, Val Acc: 0.8556 | F1: 0.8486\n",
      "Epoch [233/300] | Train Loss: 0.2395, Train Acc: 0.8796 | Val Loss: 0.3960, Val Acc: 0.8597 | F1: 0.8542\n",
      "Epoch [234/300] | Train Loss: 0.2403, Train Acc: 0.8782 | Val Loss: 0.3956, Val Acc: 0.8517 | F1: 0.8424\n",
      "Epoch [235/300] | Train Loss: 0.2396, Train Acc: 0.8785 | Val Loss: 0.3880, Val Acc: 0.8537 | F1: 0.8457\n",
      "Epoch [236/300] | Train Loss: 0.2396, Train Acc: 0.8780 | Val Loss: 0.3963, Val Acc: 0.8547 | F1: 0.8472\n",
      "Epoch [237/300] | Train Loss: 0.2409, Train Acc: 0.8789 | Val Loss: 0.3912, Val Acc: 0.8542 | F1: 0.8456\n",
      "Epoch [238/300] | Train Loss: 0.2416, Train Acc: 0.8793 | Val Loss: 0.3827, Val Acc: 0.8491 | F1: 0.8419\n",
      "Epoch [239/300] | Train Loss: 0.2398, Train Acc: 0.8785 | Val Loss: 0.3893, Val Acc: 0.8542 | F1: 0.8462\n",
      "Epoch [240/300] | Train Loss: 0.2409, Train Acc: 0.8778 | Val Loss: 0.4021, Val Acc: 0.8531 | F1: 0.8445\n",
      "Epoch [241/300] | Train Loss: 0.2382, Train Acc: 0.8783 | Val Loss: 0.3942, Val Acc: 0.8535 | F1: 0.8448\n",
      "Epoch [242/300] | Train Loss: 0.2393, Train Acc: 0.8783 | Val Loss: 0.3989, Val Acc: 0.8526 | F1: 0.8447\n",
      "Epoch [243/300] | Train Loss: 0.2440, Train Acc: 0.8759 | Val Loss: 0.3929, Val Acc: 0.8528 | F1: 0.8442\n",
      "Epoch [244/300] | Train Loss: 0.2371, Train Acc: 0.8796 | Val Loss: 0.3972, Val Acc: 0.8515 | F1: 0.8434\n",
      "Epoch [245/300] | Train Loss: 0.2378, Train Acc: 0.8793 | Val Loss: 0.4082, Val Acc: 0.8520 | F1: 0.8442\n",
      "Epoch [246/300] | Train Loss: 0.2408, Train Acc: 0.8781 | Val Loss: 0.4067, Val Acc: 0.8495 | F1: 0.8388\n",
      "Epoch [247/300] | Train Loss: 0.2414, Train Acc: 0.8782 | Val Loss: 0.3940, Val Acc: 0.8537 | F1: 0.8456\n",
      "Epoch [248/300] | Train Loss: 0.2395, Train Acc: 0.8793 | Val Loss: 0.3934, Val Acc: 0.8537 | F1: 0.8490\n",
      "Epoch [249/300] | Train Loss: 0.2368, Train Acc: 0.8789 | Val Loss: 0.4110, Val Acc: 0.8534 | F1: 0.8454\n",
      "Epoch [250/300] | Train Loss: 0.2411, Train Acc: 0.8777 | Val Loss: 0.3994, Val Acc: 0.8553 | F1: 0.8509\n",
      "Epoch [251/300] | Train Loss: 0.2431, Train Acc: 0.8787 | Val Loss: 0.3903, Val Acc: 0.8576 | F1: 0.8530\n",
      "Epoch [252/300] | Train Loss: 0.2426, Train Acc: 0.8784 | Val Loss: 0.3898, Val Acc: 0.8550 | F1: 0.8458\n",
      "Epoch [253/300] | Train Loss: 0.2370, Train Acc: 0.8800 | Val Loss: 0.3988, Val Acc: 0.8477 | F1: 0.8408\n",
      "Epoch [254/300] | Train Loss: 0.2388, Train Acc: 0.8796 | Val Loss: 0.4029, Val Acc: 0.8516 | F1: 0.8466\n",
      "Epoch [255/300] | Train Loss: 0.2364, Train Acc: 0.8791 | Val Loss: 0.4007, Val Acc: 0.8527 | F1: 0.8449\n",
      "Epoch [256/300] | Train Loss: 0.2366, Train Acc: 0.8796 | Val Loss: 0.4015, Val Acc: 0.8550 | F1: 0.8474\n",
      "Epoch [257/300] | Train Loss: 0.2382, Train Acc: 0.8804 | Val Loss: 0.3952, Val Acc: 0.8564 | F1: 0.8486\n",
      "Epoch [258/300] | Train Loss: 0.2370, Train Acc: 0.8803 | Val Loss: 0.3946, Val Acc: 0.8544 | F1: 0.8462\n",
      "Epoch [259/300] | Train Loss: 0.2358, Train Acc: 0.8818 | Val Loss: 0.4065, Val Acc: 0.8539 | F1: 0.8489\n",
      "Epoch [260/300] | Train Loss: 0.2364, Train Acc: 0.8801 | Val Loss: 0.4064, Val Acc: 0.8545 | F1: 0.8467\n",
      "Epoch [261/300] | Train Loss: 0.2373, Train Acc: 0.8797 | Val Loss: 0.4023, Val Acc: 0.8552 | F1: 0.8502\n",
      "Epoch [262/300] | Train Loss: 0.2359, Train Acc: 0.8809 | Val Loss: 0.4096, Val Acc: 0.8506 | F1: 0.8457\n",
      "Epoch [263/300] | Train Loss: 0.2368, Train Acc: 0.8790 | Val Loss: 0.4015, Val Acc: 0.8547 | F1: 0.8476\n",
      "Epoch [264/300] | Train Loss: 0.2353, Train Acc: 0.8814 | Val Loss: 0.4108, Val Acc: 0.8539 | F1: 0.8456\n",
      "Epoch [265/300] | Train Loss: 0.2389, Train Acc: 0.8795 | Val Loss: 0.4117, Val Acc: 0.8506 | F1: 0.8459\n",
      "Epoch [266/300] | Train Loss: 0.2382, Train Acc: 0.8781 | Val Loss: 0.4052, Val Acc: 0.8539 | F1: 0.8456\n",
      "Epoch [267/300] | Train Loss: 0.2372, Train Acc: 0.8802 | Val Loss: 0.3977, Val Acc: 0.8632 | F1: 0.8576\n",
      "Epoch [268/300] | Train Loss: 0.2373, Train Acc: 0.8809 | Val Loss: 0.4044, Val Acc: 0.8531 | F1: 0.8452\n",
      "Epoch [269/300] | Train Loss: 0.2380, Train Acc: 0.8797 | Val Loss: 0.3920, Val Acc: 0.8562 | F1: 0.8483\n",
      "Epoch [270/300] | Train Loss: 0.2354, Train Acc: 0.8820 | Val Loss: 0.4030, Val Acc: 0.8588 | F1: 0.8523\n",
      "Epoch [271/300] | Train Loss: 0.2368, Train Acc: 0.8797 | Val Loss: 0.4105, Val Acc: 0.8527 | F1: 0.8444\n",
      "Epoch [272/300] | Train Loss: 0.2368, Train Acc: 0.8809 | Val Loss: 0.4049, Val Acc: 0.8528 | F1: 0.8478\n",
      "Epoch [273/300] | Train Loss: 0.2389, Train Acc: 0.8792 | Val Loss: 0.4008, Val Acc: 0.8543 | F1: 0.8467\n",
      "Epoch [274/300] | Train Loss: 0.2350, Train Acc: 0.8819 | Val Loss: 0.4187, Val Acc: 0.8537 | F1: 0.8481\n",
      "Epoch [275/300] | Train Loss: 0.2389, Train Acc: 0.8799 | Val Loss: 0.4023, Val Acc: 0.8519 | F1: 0.8469\n",
      "Epoch [276/300] | Train Loss: 0.2397, Train Acc: 0.8784 | Val Loss: 0.3980, Val Acc: 0.8586 | F1: 0.8526\n",
      "Epoch [277/300] | Train Loss: 0.2396, Train Acc: 0.8819 | Val Loss: 0.4049, Val Acc: 0.8487 | F1: 0.8369\n",
      "Epoch [278/300] | Train Loss: 0.2352, Train Acc: 0.8798 | Val Loss: 0.4273, Val Acc: 0.8498 | F1: 0.8449\n",
      "Epoch [279/300] | Train Loss: 0.2368, Train Acc: 0.8804 | Val Loss: 0.4010, Val Acc: 0.8521 | F1: 0.8473\n",
      "Epoch [280/300] | Train Loss: 0.2365, Train Acc: 0.8823 | Val Loss: 0.4069, Val Acc: 0.8527 | F1: 0.8453\n",
      "Epoch [281/300] | Train Loss: 0.2362, Train Acc: 0.8808 | Val Loss: 0.4091, Val Acc: 0.8561 | F1: 0.8491\n",
      "Epoch [282/300] | Train Loss: 0.2324, Train Acc: 0.8846 | Val Loss: 0.4111, Val Acc: 0.8541 | F1: 0.8494\n",
      "Epoch [283/300] | Train Loss: 0.2365, Train Acc: 0.8785 | Val Loss: 0.4167, Val Acc: 0.8536 | F1: 0.8457\n",
      "Epoch [284/300] | Train Loss: 0.2350, Train Acc: 0.8797 | Val Loss: 0.4239, Val Acc: 0.8573 | F1: 0.8522\n",
      "Epoch [285/300] | Train Loss: 0.2357, Train Acc: 0.8806 | Val Loss: 0.4119, Val Acc: 0.8608 | F1: 0.8545\n",
      "Epoch [286/300] | Train Loss: 0.2356, Train Acc: 0.8803 | Val Loss: 0.4174, Val Acc: 0.8533 | F1: 0.8486\n",
      "Epoch [287/300] | Train Loss: 0.2337, Train Acc: 0.8817 | Val Loss: 0.4141, Val Acc: 0.8531 | F1: 0.8444\n",
      "Epoch [288/300] | Train Loss: 0.2353, Train Acc: 0.8822 | Val Loss: 0.4026, Val Acc: 0.8523 | F1: 0.8459\n",
      "Epoch [289/300] | Train Loss: 0.2366, Train Acc: 0.8814 | Val Loss: 0.4021, Val Acc: 0.8551 | F1: 0.8474\n",
      "Epoch [290/300] | Train Loss: 0.2338, Train Acc: 0.8800 | Val Loss: 0.4087, Val Acc: 0.8530 | F1: 0.8458\n",
      "Epoch [291/300] | Train Loss: 0.2330, Train Acc: 0.8811 | Val Loss: 0.4156, Val Acc: 0.8620 | F1: 0.8568\n",
      "Epoch [292/300] | Train Loss: 0.2329, Train Acc: 0.8834 | Val Loss: 0.4049, Val Acc: 0.8583 | F1: 0.8540\n",
      "Epoch [293/300] | Train Loss: 0.2306, Train Acc: 0.8868 | Val Loss: 0.4158, Val Acc: 0.8573 | F1: 0.8502\n",
      "Epoch [294/300] | Train Loss: 0.2324, Train Acc: 0.8815 | Val Loss: 0.4257, Val Acc: 0.8530 | F1: 0.8453\n",
      "Epoch [295/300] | Train Loss: 0.2347, Train Acc: 0.8797 | Val Loss: 0.4017, Val Acc: 0.8545 | F1: 0.8465\n",
      "Epoch [296/300] | Train Loss: 0.2336, Train Acc: 0.8836 | Val Loss: 0.4098, Val Acc: 0.8580 | F1: 0.8522\n",
      "Epoch [297/300] | Train Loss: 0.2348, Train Acc: 0.8826 | Val Loss: 0.4127, Val Acc: 0.8513 | F1: 0.8464\n",
      "Epoch [298/300] | Train Loss: 0.2351, Train Acc: 0.8795 | Val Loss: 0.4178, Val Acc: 0.8545 | F1: 0.8467\n",
      "Epoch [299/300] | Train Loss: 0.2328, Train Acc: 0.8818 | Val Loss: 0.4229, Val Acc: 0.8494 | F1: 0.8429\n",
      "Epoch [300/300] | Train Loss: 0.2335, Train Acc: 0.8857 | Val Loss: 0.4156, Val Acc: 0.8606 | F1: 0.8548\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss, train_correct, total = 0.0, 0, 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * X_batch.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total += y_batch.size(0)\n",
    "        train_correct += (preds == y_batch).sum().item()\n",
    "    \n",
    "    avg_train_loss = train_loss / total\n",
    "    train_acc = train_correct / total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "    all_preds, all_labels = [], []   # <-- NEW\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in test_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            outputs = model(X_val)\n",
    "            \n",
    "            # collect validation loss\n",
    "            loss = criterion(outputs, y_val)\n",
    "            val_loss += loss.item() * X_val.size(0)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_total += y_val.size(0)\n",
    "            val_correct += (preds == y_val).sum().item()\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_val.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / val_total\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    # Compute metrics\n",
    "    precision = precision_score(all_labels, all_preds, average=\"macro\")\n",
    "    recall = recall_score(all_labels, all_preds, average=\"macro\")\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "    # Store\n",
    "    history[\"epoch\"].append(epoch + 1)\n",
    "    history[\"train_loss\"].append(avg_train_loss)     # <-- (fix) store avg, not sum\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    history[\"val_loss\"].append(avg_val_loss)         # <-- NEW\n",
    "    history[\"precision\"].append(precision)           # <-- NEW\n",
    "    history[\"recall\"].append(recall)                 # <-- NEW\n",
    "    history[\"f1\"].append(f1)                         # <-- NEW\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f} | \"\n",
    "          f\"F1: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6d4b2cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d36d0f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9254    0.8614    0.8922      3210\n",
      "           1     0.7700    0.5975    0.6729      2773\n",
      "           2     0.6880    0.8322    0.7532      2962\n",
      "           3     0.9953    0.9807    0.9879      3209\n",
      "           4     0.9610    0.9353    0.9480      3059\n",
      "           5     0.8311    0.9222    0.8743      3202\n",
      "\n",
      "    accuracy                         0.8606     18415\n",
      "   macro avg     0.8618    0.8549    0.8548     18415\n",
      "weighted avg     0.8655    0.8606    0.8597     18415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_val, y_val in test_loader:\n",
    "        X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "        outputs = model(X_val)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_val.cpu().numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d50c5a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "436752da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../metrics/torch/rclnet.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "187a7975",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../artifacts/torch/rclnet_cuda.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "31658f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_BiLSTM_Attention(nn.Module):\n",
    "    def __init__(self, input_len, num_classes, lstm_hidden=64, num_heads=4):\n",
    "        super(CNN_BiLSTM_Attention, self).__init__()\n",
    "        \n",
    "        # CNN feature extractor\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding='same')\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding='same')\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        \n",
    "        # BiLSTM block\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=128,\n",
    "            hidden_size=lstm_hidden,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Multi-Head Self-Attention\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=2*lstm_hidden, num_heads=num_heads, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(2*lstm_hidden)\n",
    "\n",
    "        # Dense layers\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc1 = nn.Linear(2*lstm_hidden, 128)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch, seq_len, 1)\n",
    "        x = x.permute(0, 2, 1)                 # (batch, 1, seq_len)\n",
    "        x = torch.relu(self.conv1(x))          # (batch, 64, seq_len)\n",
    "        x = torch.relu(self.conv2(x))          # (batch, 128, seq_len)\n",
    "        x = self.pool(x)                       # (batch, 128, seq_len//2)\n",
    "        \n",
    "        # Prepare for LSTM: (batch, seq_len, features)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.bilstm(x)                  # (batch, seq_len//2, 2*lstm_hidden)\n",
    "        \n",
    "        # Multihead Self-Attention with residual connection\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = self.norm(x + attn_output)\n",
    "\n",
    "        # Global average pooling over the sequence dimension\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.global_pool(x).squeeze(-1)    # (batch, 2*lstm_hidden)\n",
    "        \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)                        # logits\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "82c6acd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_BiLSTM_Attention(\n",
      "  (conv1): Conv1d(1, 64, kernel_size=(3,), stride=(1,), padding=same)\n",
      "  (conv2): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
      "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bilstm): LSTM(128, 64, batch_first=True, bidirectional=True)\n",
      "  (attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_len = X_train_scaled.shape[1]\n",
    "model = CNN_BiLSTM_Attention(input_len, num_classes).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0809d38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "df33404f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300] | Train Loss: 0.2427 | Train Acc: 0.8776 | Val Loss: 0.3999 | Val Acc: 0.8519 | F1: 0.8432\n",
      "Epoch [2/300] | Train Loss: 0.2430 | Train Acc: 0.8784 | Val Loss: 0.3858 | Val Acc: 0.8515 | F1: 0.8427\n",
      "Epoch [3/300] | Train Loss: 0.2409 | Train Acc: 0.8781 | Val Loss: 0.3888 | Val Acc: 0.8557 | F1: 0.8483\n",
      "Epoch [4/300] | Train Loss: 0.2412 | Train Acc: 0.8790 | Val Loss: 0.3926 | Val Acc: 0.8528 | F1: 0.8437\n",
      "Epoch [5/300] | Train Loss: 0.2411 | Train Acc: 0.8779 | Val Loss: 0.4033 | Val Acc: 0.8509 | F1: 0.8458\n",
      "Epoch [6/300] | Train Loss: 0.2412 | Train Acc: 0.8779 | Val Loss: 0.4062 | Val Acc: 0.8510 | F1: 0.8418\n",
      "Epoch [7/300] | Train Loss: 0.2418 | Train Acc: 0.8780 | Val Loss: 0.3986 | Val Acc: 0.8519 | F1: 0.8431\n",
      "Epoch [8/300] | Train Loss: 0.2405 | Train Acc: 0.8783 | Val Loss: 0.4206 | Val Acc: 0.8515 | F1: 0.8463\n",
      "Epoch [9/300] | Train Loss: 0.2399 | Train Acc: 0.8790 | Val Loss: 0.3968 | Val Acc: 0.8507 | F1: 0.8457\n",
      "Epoch [10/300] | Train Loss: 0.2405 | Train Acc: 0.8782 | Val Loss: 0.4127 | Val Acc: 0.8521 | F1: 0.8435\n",
      "Epoch [11/300] | Train Loss: 0.2375 | Train Acc: 0.8791 | Val Loss: 0.4050 | Val Acc: 0.8493 | F1: 0.8445\n",
      "Epoch [12/300] | Train Loss: 0.2375 | Train Acc: 0.8796 | Val Loss: 0.4093 | Val Acc: 0.8463 | F1: 0.8411\n",
      "Epoch [13/300] | Train Loss: 0.2389 | Train Acc: 0.8782 | Val Loss: 0.4124 | Val Acc: 0.8482 | F1: 0.8430\n",
      "Epoch [14/300] | Train Loss: 0.2373 | Train Acc: 0.8796 | Val Loss: 0.4162 | Val Acc: 0.8487 | F1: 0.8386\n",
      "Epoch [15/300] | Train Loss: 0.2406 | Train Acc: 0.8781 | Val Loss: 0.4037 | Val Acc: 0.8510 | F1: 0.8449\n",
      "Epoch [16/300] | Train Loss: 0.2389 | Train Acc: 0.8784 | Val Loss: 0.4070 | Val Acc: 0.8535 | F1: 0.8465\n",
      "Epoch [17/300] | Train Loss: 0.2381 | Train Acc: 0.8800 | Val Loss: 0.4132 | Val Acc: 0.8522 | F1: 0.8449\n",
      "Epoch [18/300] | Train Loss: 0.2368 | Train Acc: 0.8794 | Val Loss: 0.4163 | Val Acc: 0.8437 | F1: 0.8384\n",
      "Epoch [19/300] | Train Loss: 0.2377 | Train Acc: 0.8790 | Val Loss: 0.4039 | Val Acc: 0.8531 | F1: 0.8449\n",
      "Epoch [20/300] | Train Loss: 0.2381 | Train Acc: 0.8789 | Val Loss: 0.4014 | Val Acc: 0.8560 | F1: 0.8510\n",
      "Epoch [21/300] | Train Loss: 0.2385 | Train Acc: 0.8786 | Val Loss: 0.3985 | Val Acc: 0.8531 | F1: 0.8481\n",
      "Epoch [22/300] | Train Loss: 0.2370 | Train Acc: 0.8792 | Val Loss: 0.4221 | Val Acc: 0.8501 | F1: 0.8411\n",
      "Epoch [23/300] | Train Loss: 0.2341 | Train Acc: 0.8804 | Val Loss: 0.4131 | Val Acc: 0.8502 | F1: 0.8452\n",
      "Epoch [24/300] | Train Loss: 0.2362 | Train Acc: 0.8788 | Val Loss: 0.4243 | Val Acc: 0.8568 | F1: 0.8518\n",
      "Epoch [25/300] | Train Loss: 0.2378 | Train Acc: 0.8794 | Val Loss: 0.4290 | Val Acc: 0.8489 | F1: 0.8405\n",
      "Epoch [26/300] | Train Loss: 0.2402 | Train Acc: 0.8785 | Val Loss: 0.4060 | Val Acc: 0.8549 | F1: 0.8473\n",
      "Epoch [27/300] | Train Loss: 0.2344 | Train Acc: 0.8811 | Val Loss: 0.4171 | Val Acc: 0.8524 | F1: 0.8470\n",
      "Epoch [28/300] | Train Loss: 0.2337 | Train Acc: 0.8814 | Val Loss: 0.4226 | Val Acc: 0.8564 | F1: 0.8519\n",
      "Epoch [29/300] | Train Loss: 0.2360 | Train Acc: 0.8801 | Val Loss: 0.4281 | Val Acc: 0.8500 | F1: 0.8448\n",
      "Epoch [30/300] | Train Loss: 0.2337 | Train Acc: 0.8803 | Val Loss: 0.4285 | Val Acc: 0.8504 | F1: 0.8412\n",
      "Epoch [31/300] | Train Loss: 0.2366 | Train Acc: 0.8802 | Val Loss: 0.4251 | Val Acc: 0.8516 | F1: 0.8468\n",
      "Epoch [32/300] | Train Loss: 0.2356 | Train Acc: 0.8802 | Val Loss: 0.4261 | Val Acc: 0.8544 | F1: 0.8460\n",
      "Epoch [33/300] | Train Loss: 0.2368 | Train Acc: 0.8800 | Val Loss: 0.4246 | Val Acc: 0.8531 | F1: 0.8453\n",
      "Epoch [34/300] | Train Loss: 0.2332 | Train Acc: 0.8815 | Val Loss: 0.4325 | Val Acc: 0.8504 | F1: 0.8456\n",
      "Epoch [35/300] | Train Loss: 0.2359 | Train Acc: 0.8812 | Val Loss: 0.4149 | Val Acc: 0.8577 | F1: 0.8529\n",
      "Epoch [36/300] | Train Loss: 0.2343 | Train Acc: 0.8806 | Val Loss: 0.4149 | Val Acc: 0.8519 | F1: 0.8471\n",
      "Epoch [37/300] | Train Loss: 0.2346 | Train Acc: 0.8798 | Val Loss: 0.4171 | Val Acc: 0.8519 | F1: 0.8433\n",
      "Epoch [38/300] | Train Loss: 0.2324 | Train Acc: 0.8808 | Val Loss: 0.4308 | Val Acc: 0.8559 | F1: 0.8499\n",
      "Epoch [39/300] | Train Loss: 0.2339 | Train Acc: 0.8803 | Val Loss: 0.4155 | Val Acc: 0.8567 | F1: 0.8498\n",
      "Epoch [40/300] | Train Loss: 0.2320 | Train Acc: 0.8828 | Val Loss: 0.4381 | Val Acc: 0.8523 | F1: 0.8470\n",
      "Epoch [41/300] | Train Loss: 0.2323 | Train Acc: 0.8821 | Val Loss: 0.4274 | Val Acc: 0.8523 | F1: 0.8475\n",
      "Epoch [42/300] | Train Loss: 0.2315 | Train Acc: 0.8811 | Val Loss: 0.4225 | Val Acc: 0.8523 | F1: 0.8437\n",
      "Epoch [43/300] | Train Loss: 0.2312 | Train Acc: 0.8819 | Val Loss: 0.4397 | Val Acc: 0.8527 | F1: 0.8442\n",
      "Epoch [44/300] | Train Loss: 0.2309 | Train Acc: 0.8827 | Val Loss: 0.4284 | Val Acc: 0.8577 | F1: 0.8509\n",
      "Epoch [45/300] | Train Loss: 0.2318 | Train Acc: 0.8804 | Val Loss: 0.4233 | Val Acc: 0.8527 | F1: 0.8448\n",
      "Epoch [46/300] | Train Loss: 0.2373 | Train Acc: 0.8801 | Val Loss: 0.4073 | Val Acc: 0.8518 | F1: 0.8431\n",
      "Epoch [47/300] | Train Loss: 0.2359 | Train Acc: 0.8813 | Val Loss: 0.4159 | Val Acc: 0.8519 | F1: 0.8442\n",
      "Epoch [48/300] | Train Loss: 0.2312 | Train Acc: 0.8823 | Val Loss: 0.4273 | Val Acc: 0.8520 | F1: 0.8435\n",
      "Epoch [49/300] | Train Loss: 0.2292 | Train Acc: 0.8838 | Val Loss: 0.4363 | Val Acc: 0.8525 | F1: 0.8435\n",
      "Epoch [50/300] | Train Loss: 0.2301 | Train Acc: 0.8835 | Val Loss: 0.4418 | Val Acc: 0.8605 | F1: 0.8550\n",
      "Epoch [51/300] | Train Loss: 0.2298 | Train Acc: 0.8835 | Val Loss: 0.4409 | Val Acc: 0.8549 | F1: 0.8484\n",
      "Epoch [52/300] | Train Loss: 0.2302 | Train Acc: 0.8836 | Val Loss: 0.4274 | Val Acc: 0.8536 | F1: 0.8451\n",
      "Epoch [53/300] | Train Loss: 0.2320 | Train Acc: 0.8818 | Val Loss: 0.4352 | Val Acc: 0.8508 | F1: 0.8449\n",
      "Epoch [54/300] | Train Loss: 0.2293 | Train Acc: 0.8831 | Val Loss: 0.4201 | Val Acc: 0.8538 | F1: 0.8460\n",
      "Epoch [55/300] | Train Loss: 0.2290 | Train Acc: 0.8818 | Val Loss: 0.4412 | Val Acc: 0.8489 | F1: 0.8392\n",
      "Epoch [56/300] | Train Loss: 0.2307 | Train Acc: 0.8828 | Val Loss: 0.4452 | Val Acc: 0.8560 | F1: 0.8515\n",
      "Epoch [57/300] | Train Loss: 0.2348 | Train Acc: 0.8816 | Val Loss: 0.4281 | Val Acc: 0.8497 | F1: 0.8413\n",
      "Epoch [58/300] | Train Loss: 0.2321 | Train Acc: 0.8823 | Val Loss: 0.4264 | Val Acc: 0.8533 | F1: 0.8447\n",
      "Epoch [59/300] | Train Loss: 0.2291 | Train Acc: 0.8828 | Val Loss: 0.4311 | Val Acc: 0.8538 | F1: 0.8462\n",
      "Epoch [60/300] | Train Loss: 0.2291 | Train Acc: 0.8825 | Val Loss: 0.4404 | Val Acc: 0.8527 | F1: 0.8447\n",
      "Epoch [61/300] | Train Loss: 0.2283 | Train Acc: 0.8827 | Val Loss: 0.4542 | Val Acc: 0.8530 | F1: 0.8451\n",
      "Epoch [62/300] | Train Loss: 0.2280 | Train Acc: 0.8831 | Val Loss: 0.4355 | Val Acc: 0.8522 | F1: 0.8444\n",
      "Epoch [63/300] | Train Loss: 0.2289 | Train Acc: 0.8839 | Val Loss: 0.4340 | Val Acc: 0.8517 | F1: 0.8437\n",
      "Epoch [64/300] | Train Loss: 0.2301 | Train Acc: 0.8827 | Val Loss: 0.4355 | Val Acc: 0.8456 | F1: 0.8387\n",
      "Epoch [65/300] | Train Loss: 0.2287 | Train Acc: 0.8835 | Val Loss: 0.4431 | Val Acc: 0.8575 | F1: 0.8517\n",
      "Epoch [66/300] | Train Loss: 0.2278 | Train Acc: 0.8840 | Val Loss: 0.4428 | Val Acc: 0.8542 | F1: 0.8460\n",
      "Epoch [67/300] | Train Loss: 0.2277 | Train Acc: 0.8829 | Val Loss: 0.4453 | Val Acc: 0.8508 | F1: 0.8459\n",
      "Epoch [68/300] | Train Loss: 0.2274 | Train Acc: 0.8833 | Val Loss: 0.4332 | Val Acc: 0.8435 | F1: 0.8348\n",
      "Epoch [69/300] | Train Loss: 0.2290 | Train Acc: 0.8824 | Val Loss: 0.4402 | Val Acc: 0.8554 | F1: 0.8509\n",
      "Epoch [70/300] | Train Loss: 0.2283 | Train Acc: 0.8834 | Val Loss: 0.4486 | Val Acc: 0.8550 | F1: 0.8504\n",
      "Epoch [71/300] | Train Loss: 0.2289 | Train Acc: 0.8824 | Val Loss: 0.4299 | Val Acc: 0.8487 | F1: 0.8391\n",
      "Epoch [72/300] | Train Loss: 0.2281 | Train Acc: 0.8829 | Val Loss: 0.4687 | Val Acc: 0.8500 | F1: 0.8416\n",
      "Epoch [73/300] | Train Loss: 0.2293 | Train Acc: 0.8817 | Val Loss: 0.4470 | Val Acc: 0.8516 | F1: 0.8434\n",
      "Epoch [74/300] | Train Loss: 0.2280 | Train Acc: 0.8826 | Val Loss: 0.4500 | Val Acc: 0.8494 | F1: 0.8445\n",
      "Epoch [75/300] | Train Loss: 0.2279 | Train Acc: 0.8837 | Val Loss: 0.4451 | Val Acc: 0.8439 | F1: 0.8385\n",
      "Epoch [76/300] | Train Loss: 0.2285 | Train Acc: 0.8823 | Val Loss: 0.4307 | Val Acc: 0.8496 | F1: 0.8428\n",
      "Epoch [77/300] | Train Loss: 0.2256 | Train Acc: 0.8840 | Val Loss: 0.4606 | Val Acc: 0.8490 | F1: 0.8409\n",
      "Epoch [78/300] | Train Loss: 0.2239 | Train Acc: 0.8854 | Val Loss: 0.4560 | Val Acc: 0.8564 | F1: 0.8502\n",
      "Epoch [79/300] | Train Loss: 0.2255 | Train Acc: 0.8836 | Val Loss: 0.4433 | Val Acc: 0.8511 | F1: 0.8420\n",
      "Epoch [80/300] | Train Loss: 0.2230 | Train Acc: 0.8881 | Val Loss: 0.4705 | Val Acc: 0.8448 | F1: 0.8394\n",
      "Epoch [81/300] | Train Loss: 0.2261 | Train Acc: 0.8838 | Val Loss: 0.4571 | Val Acc: 0.8442 | F1: 0.8379\n",
      "Epoch [82/300] | Train Loss: 0.2252 | Train Acc: 0.8858 | Val Loss: 0.4568 | Val Acc: 0.8576 | F1: 0.8517\n",
      "Epoch [83/300] | Train Loss: 0.2275 | Train Acc: 0.8852 | Val Loss: 0.4542 | Val Acc: 0.8597 | F1: 0.8549\n",
      "Epoch [84/300] | Train Loss: 0.2237 | Train Acc: 0.8860 | Val Loss: 0.4639 | Val Acc: 0.8506 | F1: 0.8419\n",
      "Epoch [85/300] | Train Loss: 0.2230 | Train Acc: 0.8856 | Val Loss: 0.4710 | Val Acc: 0.8515 | F1: 0.8443\n",
      "Epoch [86/300] | Train Loss: 0.2242 | Train Acc: 0.8847 | Val Loss: 0.4567 | Val Acc: 0.8530 | F1: 0.8483\n",
      "Epoch [87/300] | Train Loss: 0.2265 | Train Acc: 0.8837 | Val Loss: 0.4524 | Val Acc: 0.8552 | F1: 0.8491\n",
      "Epoch [88/300] | Train Loss: 0.2260 | Train Acc: 0.8826 | Val Loss: 0.4478 | Val Acc: 0.8548 | F1: 0.8475\n",
      "Epoch [89/300] | Train Loss: 0.2260 | Train Acc: 0.8866 | Val Loss: 0.4512 | Val Acc: 0.8594 | F1: 0.8550\n",
      "Epoch [90/300] | Train Loss: 0.2222 | Train Acc: 0.8882 | Val Loss: 0.4521 | Val Acc: 0.8506 | F1: 0.8426\n",
      "Epoch [91/300] | Train Loss: 0.2231 | Train Acc: 0.8850 | Val Loss: 0.4583 | Val Acc: 0.8535 | F1: 0.8464\n",
      "Epoch [92/300] | Train Loss: 0.2223 | Train Acc: 0.8871 | Val Loss: 0.4776 | Val Acc: 0.8579 | F1: 0.8535\n",
      "Epoch [93/300] | Train Loss: 0.2269 | Train Acc: 0.8845 | Val Loss: 0.4500 | Val Acc: 0.8511 | F1: 0.8439\n",
      "Epoch [94/300] | Train Loss: 0.2243 | Train Acc: 0.8866 | Val Loss: 0.4399 | Val Acc: 0.8589 | F1: 0.8537\n",
      "Epoch [95/300] | Train Loss: 0.2258 | Train Acc: 0.8870 | Val Loss: 0.4671 | Val Acc: 0.8523 | F1: 0.8462\n",
      "Epoch [96/300] | Train Loss: 0.2219 | Train Acc: 0.8874 | Val Loss: 0.4528 | Val Acc: 0.8520 | F1: 0.8442\n",
      "Epoch [97/300] | Train Loss: 0.2236 | Train Acc: 0.8874 | Val Loss: 0.4475 | Val Acc: 0.8602 | F1: 0.8560\n",
      "Epoch [98/300] | Train Loss: 0.2231 | Train Acc: 0.8883 | Val Loss: 0.4560 | Val Acc: 0.8579 | F1: 0.8519\n",
      "Epoch [99/300] | Train Loss: 0.2207 | Train Acc: 0.8878 | Val Loss: 0.4710 | Val Acc: 0.8525 | F1: 0.8455\n",
      "Epoch [100/300] | Train Loss: 0.2214 | Train Acc: 0.8866 | Val Loss: 0.4646 | Val Acc: 0.8585 | F1: 0.8542\n",
      "Epoch [101/300] | Train Loss: 0.2216 | Train Acc: 0.8865 | Val Loss: 0.4762 | Val Acc: 0.8508 | F1: 0.8422\n",
      "Epoch [102/300] | Train Loss: 0.2219 | Train Acc: 0.8865 | Val Loss: 0.4733 | Val Acc: 0.8502 | F1: 0.8417\n",
      "Epoch [103/300] | Train Loss: 0.2232 | Train Acc: 0.8853 | Val Loss: 0.4594 | Val Acc: 0.8546 | F1: 0.8496\n",
      "Epoch [104/300] | Train Loss: 0.2268 | Train Acc: 0.8844 | Val Loss: 0.4371 | Val Acc: 0.8523 | F1: 0.8440\n",
      "Epoch [105/300] | Train Loss: 0.2236 | Train Acc: 0.8852 | Val Loss: 0.4484 | Val Acc: 0.8527 | F1: 0.8452\n",
      "Epoch [106/300] | Train Loss: 0.2222 | Train Acc: 0.8863 | Val Loss: 0.4758 | Val Acc: 0.8524 | F1: 0.8437\n",
      "Epoch [107/300] | Train Loss: 0.2217 | Train Acc: 0.8859 | Val Loss: 0.4700 | Val Acc: 0.8480 | F1: 0.8428\n",
      "Epoch [108/300] | Train Loss: 0.2219 | Train Acc: 0.8866 | Val Loss: 0.4692 | Val Acc: 0.8487 | F1: 0.8437\n",
      "Epoch [109/300] | Train Loss: 0.2235 | Train Acc: 0.8863 | Val Loss: 0.4581 | Val Acc: 0.8512 | F1: 0.8434\n",
      "Epoch [110/300] | Train Loss: 0.2227 | Train Acc: 0.8870 | Val Loss: 0.4492 | Val Acc: 0.8549 | F1: 0.8480\n",
      "Epoch [111/300] | Train Loss: 0.2219 | Train Acc: 0.8867 | Val Loss: 0.4623 | Val Acc: 0.8588 | F1: 0.8533\n",
      "Epoch [112/300] | Train Loss: 0.2224 | Train Acc: 0.8859 | Val Loss: 0.4804 | Val Acc: 0.8502 | F1: 0.8453\n",
      "Epoch [113/300] | Train Loss: 0.2232 | Train Acc: 0.8847 | Val Loss: 0.4657 | Val Acc: 0.8512 | F1: 0.8426\n",
      "Epoch [114/300] | Train Loss: 0.2213 | Train Acc: 0.8868 | Val Loss: 0.4711 | Val Acc: 0.8559 | F1: 0.8514\n",
      "Epoch [115/300] | Train Loss: 0.2264 | Train Acc: 0.8845 | Val Loss: 0.4531 | Val Acc: 0.8520 | F1: 0.8444\n",
      "Epoch [116/300] | Train Loss: 0.2220 | Train Acc: 0.8870 | Val Loss: 0.4775 | Val Acc: 0.8516 | F1: 0.8439\n",
      "Epoch [117/300] | Train Loss: 0.2204 | Train Acc: 0.8867 | Val Loss: 0.4758 | Val Acc: 0.8522 | F1: 0.8449\n",
      "Epoch [118/300] | Train Loss: 0.2195 | Train Acc: 0.8880 | Val Loss: 0.4795 | Val Acc: 0.8558 | F1: 0.8514\n",
      "Epoch [119/300] | Train Loss: 0.2226 | Train Acc: 0.8843 | Val Loss: 0.4824 | Val Acc: 0.8500 | F1: 0.8413\n",
      "Epoch [120/300] | Train Loss: 0.2195 | Train Acc: 0.8868 | Val Loss: 0.4682 | Val Acc: 0.8554 | F1: 0.8491\n",
      "Epoch [121/300] | Train Loss: 0.2194 | Train Acc: 0.8892 | Val Loss: 0.4778 | Val Acc: 0.8490 | F1: 0.8406\n",
      "Epoch [122/300] | Train Loss: 0.2205 | Train Acc: 0.8858 | Val Loss: 0.4816 | Val Acc: 0.8443 | F1: 0.8390\n",
      "Epoch [123/300] | Train Loss: 0.2208 | Train Acc: 0.8877 | Val Loss: 0.4908 | Val Acc: 0.8590 | F1: 0.8530\n",
      "Epoch [124/300] | Train Loss: 0.2217 | Train Acc: 0.8877 | Val Loss: 0.4878 | Val Acc: 0.8512 | F1: 0.8426\n",
      "Epoch [125/300] | Train Loss: 0.2198 | Train Acc: 0.8872 | Val Loss: 0.4722 | Val Acc: 0.8587 | F1: 0.8534\n",
      "Epoch [126/300] | Train Loss: 0.2221 | Train Acc: 0.8847 | Val Loss: 0.4572 | Val Acc: 0.8540 | F1: 0.8465\n",
      "Epoch [127/300] | Train Loss: 0.2221 | Train Acc: 0.8862 | Val Loss: 0.4753 | Val Acc: 0.8564 | F1: 0.8500\n",
      "Epoch [128/300] | Train Loss: 0.2213 | Train Acc: 0.8877 | Val Loss: 0.4610 | Val Acc: 0.8535 | F1: 0.8463\n",
      "Epoch [129/300] | Train Loss: 0.2204 | Train Acc: 0.8869 | Val Loss: 0.4909 | Val Acc: 0.8522 | F1: 0.8444\n",
      "Epoch [130/300] | Train Loss: 0.2204 | Train Acc: 0.8875 | Val Loss: 0.4679 | Val Acc: 0.8503 | F1: 0.8411\n",
      "Epoch [131/300] | Train Loss: 0.2185 | Train Acc: 0.8899 | Val Loss: 0.4791 | Val Acc: 0.8561 | F1: 0.8493\n",
      "Epoch [132/300] | Train Loss: 0.2187 | Train Acc: 0.8876 | Val Loss: 0.4866 | Val Acc: 0.8439 | F1: 0.8386\n",
      "Epoch [133/300] | Train Loss: 0.2200 | Train Acc: 0.8873 | Val Loss: 0.4794 | Val Acc: 0.8522 | F1: 0.8451\n",
      "Epoch [134/300] | Train Loss: 0.2204 | Train Acc: 0.8874 | Val Loss: 0.4713 | Val Acc: 0.8594 | F1: 0.8538\n",
      "Epoch [135/300] | Train Loss: 0.2181 | Train Acc: 0.8891 | Val Loss: 0.4853 | Val Acc: 0.8478 | F1: 0.8392\n",
      "Epoch [136/300] | Train Loss: 0.2196 | Train Acc: 0.8872 | Val Loss: 0.4717 | Val Acc: 0.8513 | F1: 0.8429\n",
      "Epoch [137/300] | Train Loss: 0.2181 | Train Acc: 0.8871 | Val Loss: 0.4797 | Val Acc: 0.8557 | F1: 0.8496\n",
      "Epoch [138/300] | Train Loss: 0.2178 | Train Acc: 0.8890 | Val Loss: 0.4918 | Val Acc: 0.8431 | F1: 0.8376\n",
      "Epoch [139/300] | Train Loss: 0.2198 | Train Acc: 0.8861 | Val Loss: 0.4859 | Val Acc: 0.8572 | F1: 0.8529\n",
      "Epoch [140/300] | Train Loss: 0.2181 | Train Acc: 0.8888 | Val Loss: 0.4848 | Val Acc: 0.8536 | F1: 0.8490\n",
      "Epoch [141/300] | Train Loss: 0.2165 | Train Acc: 0.8859 | Val Loss: 0.4995 | Val Acc: 0.8617 | F1: 0.8576\n",
      "Epoch [142/300] | Train Loss: 0.2191 | Train Acc: 0.8863 | Val Loss: 0.4969 | Val Acc: 0.8515 | F1: 0.8435\n",
      "Epoch [143/300] | Train Loss: 0.2178 | Train Acc: 0.8895 | Val Loss: 0.4864 | Val Acc: 0.8535 | F1: 0.8466\n",
      "Epoch [144/300] | Train Loss: 0.2180 | Train Acc: 0.8894 | Val Loss: 0.4632 | Val Acc: 0.8458 | F1: 0.8408\n",
      "Epoch [145/300] | Train Loss: 0.2200 | Train Acc: 0.8874 | Val Loss: 0.4595 | Val Acc: 0.8462 | F1: 0.8411\n",
      "Epoch [146/300] | Train Loss: 0.2165 | Train Acc: 0.8891 | Val Loss: 0.4906 | Val Acc: 0.8596 | F1: 0.8539\n",
      "Epoch [147/300] | Train Loss: 0.2178 | Train Acc: 0.8891 | Val Loss: 0.4734 | Val Acc: 0.8518 | F1: 0.8432\n",
      "Epoch [148/300] | Train Loss: 0.2200 | Train Acc: 0.8868 | Val Loss: 0.4723 | Val Acc: 0.8527 | F1: 0.8452\n",
      "Epoch [149/300] | Train Loss: 0.2168 | Train Acc: 0.8891 | Val Loss: 0.4930 | Val Acc: 0.8600 | F1: 0.8545\n",
      "Epoch [150/300] | Train Loss: 0.2185 | Train Acc: 0.8883 | Val Loss: 0.4777 | Val Acc: 0.8490 | F1: 0.8441\n",
      "Epoch [151/300] | Train Loss: 0.2151 | Train Acc: 0.8905 | Val Loss: 0.4854 | Val Acc: 0.8598 | F1: 0.8555\n",
      "Epoch [152/300] | Train Loss: 0.2137 | Train Acc: 0.8918 | Val Loss: 0.5041 | Val Acc: 0.8532 | F1: 0.8458\n",
      "Epoch [153/300] | Train Loss: 0.2192 | Train Acc: 0.8854 | Val Loss: 0.4704 | Val Acc: 0.8543 | F1: 0.8492\n",
      "Epoch [154/300] | Train Loss: 0.2178 | Train Acc: 0.8867 | Val Loss: 0.4925 | Val Acc: 0.8511 | F1: 0.8431\n",
      "Epoch [155/300] | Train Loss: 0.2154 | Train Acc: 0.8890 | Val Loss: 0.5192 | Val Acc: 0.8500 | F1: 0.8414\n",
      "Epoch [156/300] | Train Loss: 0.2161 | Train Acc: 0.8896 | Val Loss: 0.4983 | Val Acc: 0.8502 | F1: 0.8420\n",
      "Epoch [157/300] | Train Loss: 0.2185 | Train Acc: 0.8891 | Val Loss: 0.4760 | Val Acc: 0.8585 | F1: 0.8528\n",
      "Epoch [158/300] | Train Loss: 0.2161 | Train Acc: 0.8909 | Val Loss: 0.4915 | Val Acc: 0.8575 | F1: 0.8522\n",
      "Epoch [159/300] | Train Loss: 0.2177 | Train Acc: 0.8897 | Val Loss: 0.4959 | Val Acc: 0.8576 | F1: 0.8512\n",
      "Epoch [160/300] | Train Loss: 0.2168 | Train Acc: 0.8880 | Val Loss: 0.5021 | Val Acc: 0.8507 | F1: 0.8426\n",
      "Epoch [161/300] | Train Loss: 0.2190 | Train Acc: 0.8875 | Val Loss: 0.4620 | Val Acc: 0.8520 | F1: 0.8475\n",
      "Epoch [162/300] | Train Loss: 0.2192 | Train Acc: 0.8882 | Val Loss: 0.4721 | Val Acc: 0.8576 | F1: 0.8533\n",
      "Epoch [163/300] | Train Loss: 0.2175 | Train Acc: 0.8871 | Val Loss: 0.4698 | Val Acc: 0.8516 | F1: 0.8433\n",
      "Epoch [164/300] | Train Loss: 0.2171 | Train Acc: 0.8892 | Val Loss: 0.4871 | Val Acc: 0.8509 | F1: 0.8438\n",
      "Epoch [165/300] | Train Loss: 0.2152 | Train Acc: 0.8888 | Val Loss: 0.4933 | Val Acc: 0.8564 | F1: 0.8516\n",
      "Epoch [166/300] | Train Loss: 0.2140 | Train Acc: 0.8926 | Val Loss: 0.4904 | Val Acc: 0.8501 | F1: 0.8419\n",
      "Epoch [167/300] | Train Loss: 0.2164 | Train Acc: 0.8872 | Val Loss: 0.5124 | Val Acc: 0.8469 | F1: 0.8421\n",
      "Epoch [168/300] | Train Loss: 0.2148 | Train Acc: 0.8884 | Val Loss: 0.4984 | Val Acc: 0.8515 | F1: 0.8445\n",
      "Epoch [169/300] | Train Loss: 0.2150 | Train Acc: 0.8887 | Val Loss: 0.4882 | Val Acc: 0.8534 | F1: 0.8480\n",
      "Epoch [170/300] | Train Loss: 0.2168 | Train Acc: 0.8867 | Val Loss: 0.4980 | Val Acc: 0.8523 | F1: 0.8441\n",
      "Epoch [171/300] | Train Loss: 0.2171 | Train Acc: 0.8888 | Val Loss: 0.5130 | Val Acc: 0.8540 | F1: 0.8464\n",
      "Epoch [172/300] | Train Loss: 0.2155 | Train Acc: 0.8909 | Val Loss: 0.4976 | Val Acc: 0.8566 | F1: 0.8504\n",
      "Epoch [173/300] | Train Loss: 0.2183 | Train Acc: 0.8893 | Val Loss: 0.4827 | Val Acc: 0.8505 | F1: 0.8457\n",
      "Epoch [174/300] | Train Loss: 0.2182 | Train Acc: 0.8872 | Val Loss: 0.4619 | Val Acc: 0.8536 | F1: 0.8462\n",
      "Epoch [175/300] | Train Loss: 0.2150 | Train Acc: 0.8913 | Val Loss: 0.4868 | Val Acc: 0.8480 | F1: 0.8416\n",
      "Epoch [176/300] | Train Loss: 0.2150 | Train Acc: 0.8882 | Val Loss: 0.5025 | Val Acc: 0.8501 | F1: 0.8429\n",
      "Epoch [177/300] | Train Loss: 0.2152 | Train Acc: 0.8892 | Val Loss: 0.5034 | Val Acc: 0.8505 | F1: 0.8418\n",
      "Epoch [178/300] | Train Loss: 0.2139 | Train Acc: 0.8906 | Val Loss: 0.5157 | Val Acc: 0.8599 | F1: 0.8544\n",
      "Epoch [179/300] | Train Loss: 0.2188 | Train Acc: 0.8893 | Val Loss: 0.4719 | Val Acc: 0.8516 | F1: 0.8444\n",
      "Epoch [180/300] | Train Loss: 0.2171 | Train Acc: 0.8900 | Val Loss: 0.5025 | Val Acc: 0.8428 | F1: 0.8360\n",
      "Epoch [181/300] | Train Loss: 0.2173 | Train Acc: 0.8901 | Val Loss: 0.4919 | Val Acc: 0.8588 | F1: 0.8532\n",
      "Epoch [182/300] | Train Loss: 0.2166 | Train Acc: 0.8910 | Val Loss: 0.4670 | Val Acc: 0.8467 | F1: 0.8415\n",
      "Epoch [183/300] | Train Loss: 0.2149 | Train Acc: 0.8892 | Val Loss: 0.4870 | Val Acc: 0.8595 | F1: 0.8538\n",
      "Epoch [184/300] | Train Loss: 0.2128 | Train Acc: 0.8934 | Val Loss: 0.4836 | Val Acc: 0.8566 | F1: 0.8520\n",
      "Epoch [185/300] | Train Loss: 0.2119 | Train Acc: 0.8924 | Val Loss: 0.4989 | Val Acc: 0.8603 | F1: 0.8548\n",
      "Epoch [186/300] | Train Loss: 0.2124 | Train Acc: 0.8912 | Val Loss: 0.5124 | Val Acc: 0.8512 | F1: 0.8429\n",
      "Epoch [187/300] | Train Loss: 0.2114 | Train Acc: 0.8927 | Val Loss: 0.4856 | Val Acc: 0.8547 | F1: 0.8493\n",
      "Epoch [188/300] | Train Loss: 0.2152 | Train Acc: 0.8890 | Val Loss: 0.5004 | Val Acc: 0.8535 | F1: 0.8460\n",
      "Epoch [189/300] | Train Loss: 0.2142 | Train Acc: 0.8890 | Val Loss: 0.5025 | Val Acc: 0.8510 | F1: 0.8437\n",
      "Epoch [190/300] | Train Loss: 0.2142 | Train Acc: 0.8919 | Val Loss: 0.5067 | Val Acc: 0.8488 | F1: 0.8410\n",
      "Epoch [191/300] | Train Loss: 0.2106 | Train Acc: 0.8939 | Val Loss: 0.4962 | Val Acc: 0.8512 | F1: 0.8457\n",
      "Epoch [192/300] | Train Loss: 0.2120 | Train Acc: 0.8913 | Val Loss: 0.5271 | Val Acc: 0.8496 | F1: 0.8421\n",
      "Epoch [193/300] | Train Loss: 0.2138 | Train Acc: 0.8914 | Val Loss: 0.4954 | Val Acc: 0.8567 | F1: 0.8519\n",
      "Epoch [194/300] | Train Loss: 0.2141 | Train Acc: 0.8897 | Val Loss: 0.5018 | Val Acc: 0.8562 | F1: 0.8517\n",
      "Epoch [195/300] | Train Loss: 0.2148 | Train Acc: 0.8894 | Val Loss: 0.4794 | Val Acc: 0.8503 | F1: 0.8455\n",
      "Epoch [196/300] | Train Loss: 0.2135 | Train Acc: 0.8906 | Val Loss: 0.5103 | Val Acc: 0.8567 | F1: 0.8511\n",
      "Epoch [197/300] | Train Loss: 0.2155 | Train Acc: 0.8889 | Val Loss: 0.5076 | Val Acc: 0.8526 | F1: 0.8476\n",
      "Epoch [198/300] | Train Loss: 0.2099 | Train Acc: 0.8937 | Val Loss: 0.5084 | Val Acc: 0.8564 | F1: 0.8503\n",
      "Epoch [199/300] | Train Loss: 0.2120 | Train Acc: 0.8923 | Val Loss: 0.4806 | Val Acc: 0.8563 | F1: 0.8521\n",
      "Epoch [200/300] | Train Loss: 0.2123 | Train Acc: 0.8938 | Val Loss: 0.4866 | Val Acc: 0.8474 | F1: 0.8420\n",
      "Epoch [201/300] | Train Loss: 0.2133 | Train Acc: 0.8914 | Val Loss: 0.4826 | Val Acc: 0.8515 | F1: 0.8428\n",
      "Epoch [202/300] | Train Loss: 0.2115 | Train Acc: 0.8921 | Val Loss: 0.5102 | Val Acc: 0.8590 | F1: 0.8538\n",
      "Epoch [203/300] | Train Loss: 0.2126 | Train Acc: 0.8920 | Val Loss: 0.4954 | Val Acc: 0.8564 | F1: 0.8514\n",
      "Epoch [204/300] | Train Loss: 0.2095 | Train Acc: 0.8945 | Val Loss: 0.5145 | Val Acc: 0.8504 | F1: 0.8445\n",
      "Epoch [205/300] | Train Loss: 0.2147 | Train Acc: 0.8889 | Val Loss: 0.5061 | Val Acc: 0.8544 | F1: 0.8475\n",
      "Epoch [206/300] | Train Loss: 0.2093 | Train Acc: 0.8945 | Val Loss: 0.5381 | Val Acc: 0.8509 | F1: 0.8452\n",
      "Epoch [207/300] | Train Loss: 0.2153 | Train Acc: 0.8897 | Val Loss: 0.5072 | Val Acc: 0.8592 | F1: 0.8539\n",
      "Epoch [208/300] | Train Loss: 0.2142 | Train Acc: 0.8897 | Val Loss: 0.5060 | Val Acc: 0.8583 | F1: 0.8540\n",
      "Epoch [209/300] | Train Loss: 0.2111 | Train Acc: 0.8908 | Val Loss: 0.5412 | Val Acc: 0.8539 | F1: 0.8495\n",
      "Epoch [210/300] | Train Loss: 0.2123 | Train Acc: 0.8916 | Val Loss: 0.5212 | Val Acc: 0.8581 | F1: 0.8525\n",
      "Epoch [211/300] | Train Loss: 0.2099 | Train Acc: 0.8937 | Val Loss: 0.5107 | Val Acc: 0.8561 | F1: 0.8499\n",
      "Epoch [212/300] | Train Loss: 0.2128 | Train Acc: 0.8904 | Val Loss: 0.5219 | Val Acc: 0.8545 | F1: 0.8502\n",
      "Epoch [213/300] | Train Loss: 0.2143 | Train Acc: 0.8911 | Val Loss: 0.5052 | Val Acc: 0.8489 | F1: 0.8439\n",
      "Epoch [214/300] | Train Loss: 0.2116 | Train Acc: 0.8929 | Val Loss: 0.5212 | Val Acc: 0.8517 | F1: 0.8471\n",
      "Epoch [215/300] | Train Loss: 0.2120 | Train Acc: 0.8932 | Val Loss: 0.5052 | Val Acc: 0.8586 | F1: 0.8528\n",
      "Epoch [216/300] | Train Loss: 0.2131 | Train Acc: 0.8893 | Val Loss: 0.4954 | Val Acc: 0.8501 | F1: 0.8416\n",
      "Epoch [217/300] | Train Loss: 0.2157 | Train Acc: 0.8872 | Val Loss: 0.4919 | Val Acc: 0.8581 | F1: 0.8540\n",
      "Epoch [218/300] | Train Loss: 0.2133 | Train Acc: 0.8902 | Val Loss: 0.4976 | Val Acc: 0.8574 | F1: 0.8517\n",
      "Epoch [219/300] | Train Loss: 0.2122 | Train Acc: 0.8932 | Val Loss: 0.5201 | Val Acc: 0.8572 | F1: 0.8526\n",
      "Epoch [220/300] | Train Loss: 0.2145 | Train Acc: 0.8921 | Val Loss: 0.4883 | Val Acc: 0.8595 | F1: 0.8540\n",
      "Epoch [221/300] | Train Loss: 0.2146 | Train Acc: 0.8903 | Val Loss: 0.5117 | Val Acc: 0.8499 | F1: 0.8417\n",
      "Epoch [222/300] | Train Loss: 0.2123 | Train Acc: 0.8913 | Val Loss: 0.5033 | Val Acc: 0.8528 | F1: 0.8482\n",
      "Epoch [223/300] | Train Loss: 0.2117 | Train Acc: 0.8909 | Val Loss: 0.5126 | Val Acc: 0.8525 | F1: 0.8448\n",
      "Epoch [224/300] | Train Loss: 0.2087 | Train Acc: 0.8938 | Val Loss: 0.5361 | Val Acc: 0.8551 | F1: 0.8508\n",
      "Epoch [225/300] | Train Loss: 0.2093 | Train Acc: 0.8926 | Val Loss: 0.5181 | Val Acc: 0.8560 | F1: 0.8507\n",
      "Epoch [226/300] | Train Loss: 0.2121 | Train Acc: 0.8919 | Val Loss: 0.5110 | Val Acc: 0.8502 | F1: 0.8421\n",
      "Epoch [227/300] | Train Loss: 0.2130 | Train Acc: 0.8907 | Val Loss: 0.5063 | Val Acc: 0.8514 | F1: 0.8468\n",
      "Epoch [228/300] | Train Loss: 0.2126 | Train Acc: 0.8930 | Val Loss: 0.4987 | Val Acc: 0.8487 | F1: 0.8425\n",
      "Epoch [229/300] | Train Loss: 0.2104 | Train Acc: 0.8930 | Val Loss: 0.5195 | Val Acc: 0.8575 | F1: 0.8532\n",
      "Epoch [230/300] | Train Loss: 0.2122 | Train Acc: 0.8917 | Val Loss: 0.5199 | Val Acc: 0.8523 | F1: 0.8446\n",
      "Epoch [231/300] | Train Loss: 0.2144 | Train Acc: 0.8889 | Val Loss: 0.5031 | Val Acc: 0.8564 | F1: 0.8499\n",
      "Epoch [232/300] | Train Loss: 0.2131 | Train Acc: 0.8923 | Val Loss: 0.4861 | Val Acc: 0.8504 | F1: 0.8424\n",
      "Epoch [233/300] | Train Loss: 0.2107 | Train Acc: 0.8905 | Val Loss: 0.5321 | Val Acc: 0.8572 | F1: 0.8507\n",
      "Epoch [234/300] | Train Loss: 0.2098 | Train Acc: 0.8924 | Val Loss: 0.5232 | Val Acc: 0.8519 | F1: 0.8446\n",
      "Epoch [235/300] | Train Loss: 0.2102 | Train Acc: 0.8898 | Val Loss: 0.5196 | Val Acc: 0.8500 | F1: 0.8442\n",
      "Epoch [236/300] | Train Loss: 0.2122 | Train Acc: 0.8900 | Val Loss: 0.5017 | Val Acc: 0.8522 | F1: 0.8474\n",
      "Epoch [237/300] | Train Loss: 0.2126 | Train Acc: 0.8907 | Val Loss: 0.5090 | Val Acc: 0.8496 | F1: 0.8449\n",
      "Epoch [238/300] | Train Loss: 0.2112 | Train Acc: 0.8918 | Val Loss: 0.5193 | Val Acc: 0.8484 | F1: 0.8409\n",
      "Epoch [239/300] | Train Loss: 0.2101 | Train Acc: 0.8919 | Val Loss: 0.5164 | Val Acc: 0.8463 | F1: 0.8401\n",
      "Epoch [240/300] | Train Loss: 0.2093 | Train Acc: 0.8943 | Val Loss: 0.5138 | Val Acc: 0.8573 | F1: 0.8513\n",
      "Epoch [241/300] | Train Loss: 0.2145 | Train Acc: 0.8886 | Val Loss: 0.5010 | Val Acc: 0.8513 | F1: 0.8462\n",
      "Epoch [242/300] | Train Loss: 0.2125 | Train Acc: 0.8917 | Val Loss: 0.4903 | Val Acc: 0.8490 | F1: 0.8407\n",
      "Epoch [243/300] | Train Loss: 0.2106 | Train Acc: 0.8920 | Val Loss: 0.5093 | Val Acc: 0.8544 | F1: 0.8495\n",
      "Epoch [244/300] | Train Loss: 0.2086 | Train Acc: 0.8938 | Val Loss: 0.5514 | Val Acc: 0.8465 | F1: 0.8403\n",
      "Epoch [245/300] | Train Loss: 0.2106 | Train Acc: 0.8918 | Val Loss: 0.5148 | Val Acc: 0.8509 | F1: 0.8448\n",
      "Epoch [246/300] | Train Loss: 0.2101 | Train Acc: 0.8925 | Val Loss: 0.5006 | Val Acc: 0.8550 | F1: 0.8481\n",
      "Epoch [247/300] | Train Loss: 0.2104 | Train Acc: 0.8922 | Val Loss: 0.5209 | Val Acc: 0.8567 | F1: 0.8511\n",
      "Epoch [248/300] | Train Loss: 0.2111 | Train Acc: 0.8924 | Val Loss: 0.5295 | Val Acc: 0.8529 | F1: 0.8445\n",
      "Epoch [249/300] | Train Loss: 0.2107 | Train Acc: 0.8911 | Val Loss: 0.5319 | Val Acc: 0.8532 | F1: 0.8473\n",
      "Epoch [250/300] | Train Loss: 0.2071 | Train Acc: 0.8953 | Val Loss: 0.5184 | Val Acc: 0.8512 | F1: 0.8436\n",
      "Epoch [251/300] | Train Loss: 0.2086 | Train Acc: 0.8951 | Val Loss: 0.5166 | Val Acc: 0.8609 | F1: 0.8569\n",
      "Epoch [252/300] | Train Loss: 0.2134 | Train Acc: 0.8905 | Val Loss: 0.5265 | Val Acc: 0.8502 | F1: 0.8421\n",
      "Epoch [253/300] | Train Loss: 0.2106 | Train Acc: 0.8923 | Val Loss: 0.5403 | Val Acc: 0.8511 | F1: 0.8435\n",
      "Epoch [254/300] | Train Loss: 0.2080 | Train Acc: 0.8934 | Val Loss: 0.5072 | Val Acc: 0.8561 | F1: 0.8491\n",
      "Epoch [255/300] | Train Loss: 0.2085 | Train Acc: 0.8928 | Val Loss: 0.5260 | Val Acc: 0.8576 | F1: 0.8529\n",
      "Epoch [256/300] | Train Loss: 0.2089 | Train Acc: 0.8943 | Val Loss: 0.5352 | Val Acc: 0.8572 | F1: 0.8522\n",
      "Epoch [257/300] | Train Loss: 0.2099 | Train Acc: 0.8923 | Val Loss: 0.5168 | Val Acc: 0.8605 | F1: 0.8562\n",
      "Epoch [258/300] | Train Loss: 0.2111 | Train Acc: 0.8933 | Val Loss: 0.5233 | Val Acc: 0.8494 | F1: 0.8445\n",
      "Epoch [259/300] | Train Loss: 0.2117 | Train Acc: 0.8902 | Val Loss: 0.5144 | Val Acc: 0.8502 | F1: 0.8455\n",
      "Epoch [260/300] | Train Loss: 0.2102 | Train Acc: 0.8908 | Val Loss: 0.5269 | Val Acc: 0.8566 | F1: 0.8510\n",
      "Epoch [261/300] | Train Loss: 0.2092 | Train Acc: 0.8919 | Val Loss: 0.5263 | Val Acc: 0.8583 | F1: 0.8542\n",
      "Epoch [262/300] | Train Loss: 0.2079 | Train Acc: 0.8943 | Val Loss: 0.5403 | Val Acc: 0.8517 | F1: 0.8447\n",
      "Epoch [263/300] | Train Loss: 0.2111 | Train Acc: 0.8937 | Val Loss: 0.5220 | Val Acc: 0.8568 | F1: 0.8511\n",
      "Epoch [264/300] | Train Loss: 0.2083 | Train Acc: 0.8957 | Val Loss: 0.5267 | Val Acc: 0.8577 | F1: 0.8535\n",
      "Epoch [265/300] | Train Loss: 0.2082 | Train Acc: 0.8940 | Val Loss: 0.5423 | Val Acc: 0.8519 | F1: 0.8452\n",
      "Epoch [266/300] | Train Loss: 0.2097 | Train Acc: 0.8943 | Val Loss: 0.5278 | Val Acc: 0.8486 | F1: 0.8437\n",
      "Epoch [267/300] | Train Loss: 0.2110 | Train Acc: 0.8915 | Val Loss: 0.4871 | Val Acc: 0.8547 | F1: 0.8481\n",
      "Epoch [268/300] | Train Loss: 0.2097 | Train Acc: 0.8951 | Val Loss: 0.5141 | Val Acc: 0.8478 | F1: 0.8397\n",
      "Epoch [269/300] | Train Loss: 0.2050 | Train Acc: 0.8962 | Val Loss: 0.5154 | Val Acc: 0.8604 | F1: 0.8548\n",
      "Epoch [270/300] | Train Loss: 0.2081 | Train Acc: 0.8958 | Val Loss: 0.4882 | Val Acc: 0.8591 | F1: 0.8547\n",
      "Epoch [271/300] | Train Loss: 0.2049 | Train Acc: 0.8978 | Val Loss: 0.5314 | Val Acc: 0.8551 | F1: 0.8480\n",
      "Epoch [272/300] | Train Loss: 0.2066 | Train Acc: 0.8968 | Val Loss: 0.5235 | Val Acc: 0.8584 | F1: 0.8532\n",
      "Epoch [273/300] | Train Loss: 0.2057 | Train Acc: 0.8985 | Val Loss: 0.5115 | Val Acc: 0.8609 | F1: 0.8567\n",
      "Epoch [274/300] | Train Loss: 0.2066 | Train Acc: 0.8962 | Val Loss: 0.5026 | Val Acc: 0.8502 | F1: 0.8423\n",
      "Epoch [275/300] | Train Loss: 0.2090 | Train Acc: 0.8920 | Val Loss: 0.5168 | Val Acc: 0.8531 | F1: 0.8481\n",
      "Epoch [276/300] | Train Loss: 0.2052 | Train Acc: 0.8960 | Val Loss: 0.5271 | Val Acc: 0.8559 | F1: 0.8498\n",
      "Epoch [277/300] | Train Loss: 0.2044 | Train Acc: 0.8971 | Val Loss: 0.5438 | Val Acc: 0.8481 | F1: 0.8400\n",
      "Epoch [278/300] | Train Loss: 0.2060 | Train Acc: 0.8941 | Val Loss: 0.5349 | Val Acc: 0.8511 | F1: 0.8436\n",
      "Epoch [279/300] | Train Loss: 0.2082 | Train Acc: 0.8938 | Val Loss: 0.5326 | Val Acc: 0.8541 | F1: 0.8483\n",
      "Epoch [280/300] | Train Loss: 0.2061 | Train Acc: 0.8966 | Val Loss: 0.5174 | Val Acc: 0.8598 | F1: 0.8540\n",
      "Epoch [281/300] | Train Loss: 0.2050 | Train Acc: 0.8988 | Val Loss: 0.5449 | Val Acc: 0.8622 | F1: 0.8580\n",
      "Epoch [282/300] | Train Loss: 0.2030 | Train Acc: 0.8994 | Val Loss: 0.5241 | Val Acc: 0.8610 | F1: 0.8551\n",
      "Epoch [283/300] | Train Loss: 0.2051 | Train Acc: 0.8961 | Val Loss: 0.5495 | Val Acc: 0.8602 | F1: 0.8537\n",
      "Epoch [284/300] | Train Loss: 0.2111 | Train Acc: 0.8903 | Val Loss: 0.5192 | Val Acc: 0.8495 | F1: 0.8411\n",
      "Epoch [285/300] | Train Loss: 0.2081 | Train Acc: 0.8932 | Val Loss: 0.5138 | Val Acc: 0.8574 | F1: 0.8518\n",
      "Epoch [286/300] | Train Loss: 0.2045 | Train Acc: 0.8964 | Val Loss: 0.5420 | Val Acc: 0.8569 | F1: 0.8526\n",
      "Epoch [287/300] | Train Loss: 0.2025 | Train Acc: 0.8976 | Val Loss: 0.5508 | Val Acc: 0.8525 | F1: 0.8457\n",
      "Epoch [288/300] | Train Loss: 0.2078 | Train Acc: 0.8963 | Val Loss: 0.5361 | Val Acc: 0.8432 | F1: 0.8379\n",
      "Epoch [289/300] | Train Loss: 0.2117 | Train Acc: 0.8902 | Val Loss: 0.5348 | Val Acc: 0.8518 | F1: 0.8454\n",
      "Epoch [290/300] | Train Loss: 0.2088 | Train Acc: 0.8939 | Val Loss: 0.5311 | Val Acc: 0.8570 | F1: 0.8513\n",
      "Epoch [291/300] | Train Loss: 0.2101 | Train Acc: 0.8945 | Val Loss: 0.4803 | Val Acc: 0.8591 | F1: 0.8542\n",
      "Epoch [292/300] | Train Loss: 0.2066 | Train Acc: 0.8966 | Val Loss: 0.5129 | Val Acc: 0.8575 | F1: 0.8529\n",
      "Epoch [293/300] | Train Loss: 0.2023 | Train Acc: 0.8995 | Val Loss: 0.5295 | Val Acc: 0.8583 | F1: 0.8526\n",
      "Epoch [294/300] | Train Loss: 0.2049 | Train Acc: 0.8955 | Val Loss: 0.5620 | Val Acc: 0.8601 | F1: 0.8561\n",
      "Epoch [295/300] | Train Loss: 0.2092 | Train Acc: 0.8951 | Val Loss: 0.5025 | Val Acc: 0.8520 | F1: 0.8469\n",
      "Epoch [296/300] | Train Loss: 0.2091 | Train Acc: 0.8911 | Val Loss: 0.5358 | Val Acc: 0.8532 | F1: 0.8465\n",
      "Epoch [297/300] | Train Loss: 0.2033 | Train Acc: 0.8976 | Val Loss: 0.5447 | Val Acc: 0.8532 | F1: 0.8467\n",
      "Epoch [298/300] | Train Loss: 0.2016 | Train Acc: 0.8994 | Val Loss: 0.5196 | Val Acc: 0.8613 | F1: 0.8573\n",
      "Epoch [299/300] | Train Loss: 0.2075 | Train Acc: 0.8939 | Val Loss: 0.5260 | Val Acc: 0.8514 | F1: 0.8463\n",
      "Epoch [300/300] | Train Loss: 0.2052 | Train Acc: 0.8968 | Val Loss: 0.5209 | Val Acc: 0.8485 | F1: 0.8437\n"
     ]
    }
   ],
   "source": [
    "history = {\n",
    "    \"epoch\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "    \"precision\": [],\n",
    "    \"recall\": [],\n",
    "    \"f1\": []\n",
    "}\n",
    "\n",
    "num_epochs = 300\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss, train_correct, total = 0.0, 0, 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * X_batch.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total += y_batch.size(0)\n",
    "        train_correct += (preds == y_batch).sum().item()\n",
    "    \n",
    "    avg_train_loss = train_loss / total\n",
    "    train_acc = train_correct / total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in test_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            outputs = model(X_val)\n",
    "            loss = criterion(outputs, y_val)\n",
    "\n",
    "            val_loss += loss.item() * X_val.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            val_total += y_val.size(0)\n",
    "            val_correct += (preds == y_val).sum().item()\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_val.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / val_total\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    # Compute metrics\n",
    "    precision = precision_score(all_labels, all_preds, average=\"macro\")\n",
    "    recall = recall_score(all_labels, all_preds, average=\"macro\")\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "    # Store all of them\n",
    "    history[\"epoch\"].append(epoch + 1)\n",
    "    history[\"train_loss\"].append(avg_train_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_loss\"].append(avg_val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    history[\"precision\"].append(precision)\n",
    "    history[\"recall\"].append(recall)\n",
    "    history[\"f1\"].append(f1)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f} | \"\n",
    "          f\"F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "caa507cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9207    0.8611    0.8899      3210\n",
      "           1     0.6655    0.6837    0.6745      2773\n",
      "           2     0.6954    0.6766    0.6858      2962\n",
      "           3     0.9962    0.9822    0.9892      3209\n",
      "           4     0.9653    0.9278    0.9462      3059\n",
      "           5     0.8304    0.9279    0.8764      3202\n",
      "\n",
      "    accuracy                         0.8485     18415\n",
      "   macro avg     0.8456    0.8432    0.8437     18415\n",
      "weighted avg     0.8509    0.8485    0.8489     18415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_val, y_val in test_loader:\n",
    "        X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "        outputs = model(X_val)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_val.cpu().numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ff36950e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "237628e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(history)\n",
    "df.to_csv(\"../metrics/torch/rclnet-attn.csv\", index=False)\n",
    "torch.save(model.state_dict(), \"../artifacts/torch/rclnet-attn_cuda.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "79577b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3dd851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Positional Encoding (optional)\n",
    "# -------------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Residual Block for 1D conv\n",
    "# -------------------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=\"same\")\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=\"same\")\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.shortcut = (\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "            if in_channels != out_channels else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        out = F.leaky_relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += residual\n",
    "        out = F.leaky_relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# RCLNet_Attn_Pro (Tabular-ready)\n",
    "# -------------------------------\n",
    "class RCLNet_Attn_Pro(nn.Module):\n",
    "    def __init__(self, input_features, num_classes):\n",
    "        super(RCLNet_Attn_Pro, self).__init__()\n",
    "\n",
    "        # Treat features as \"sequence length = 1\" if needed\n",
    "        self.expand_dim = nn.Linear(input_features, input_features)\n",
    "        self.res1 = ResidualBlock(1, 64)\n",
    "        self.res2 = ResidualBlock(64, 128)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(32)\n",
    "\n",
    "        # BiLSTM + Attention\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=64, batch_first=True, bidirectional=True)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=128, num_heads=4, dropout=0.2, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(128)\n",
    "\n",
    "        # Dense classifier\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        self.drop = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, F)\n",
    "        x = self.expand_dim(x).unsqueeze(1)  # (B, 1, F)\n",
    "\n",
    "        x = self.res1(x)\n",
    "        x = self.res2(x)\n",
    "        x = self.pool(x)  # (B, 128, seq_len=32)\n",
    "        x = x.permute(0, 2, 1)  # (B, seq_len, F)\n",
    "\n",
    "        # BiLSTM + Attention\n",
    "        x, _ = self.lstm(x)\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm(x + attn_out)\n",
    "\n",
    "        # Global average pooling\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Dense classifier\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4a3286fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "if X_train_t.dim() == 4:\n",
    "    X_train_t = X_train_t.squeeze(-1)\n",
    "elif X_train_t.dim() == 2:\n",
    "    X_train_t = X_train_t.unsqueeze(-1)\n",
    "\n",
    "y_train_t = torch.tensor(y_train_enc, dtype=torch.long)\n",
    "\n",
    "X_test_t = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "if X_test_t.dim() == 4:\n",
    "    X_test_t = X_test_t.squeeze(-1)\n",
    "elif X_test_t.dim() == 2:\n",
    "    X_test_t = X_test_t.unsqueeze(-1)\n",
    "\n",
    "y_test_t = torch.tensor(y_test_enc, dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test_t, y_test_t), batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b49307f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = 1              # last dim from (seq_len, features)\n",
    "seq_len = X_train_scaled.shape[1]\n",
    "num_classes = len(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6881e621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.296006 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = RCLNet_Attn_Pro(input_features, num_classes, max_len=seq_len).to(device)\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0adc8b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 6])\n"
     ]
    }
   ],
   "source": [
    "dummy = torch.randn(8, seq_len, input_features).to(device)\n",
    "out = model(dummy)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "84ab08e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7f50ff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = X_train_scaled.shape[1]\n",
    "num_classes = len(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5e1e6ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RCLNet_Attn_Pro(input_features=1, num_classes=num_classes, max_len=seq_len).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9c949511",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e36648de",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n",
    "    \"epoch\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "    \"precision\": [],\n",
    "    \"recall\": [],\n",
    "    \"f1\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8958081d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300] | Train Loss: 0.4102, Train Acc: 0.7910 | Val Loss: 0.3954, Val Acc: 0.7978 | F1: 0.7720\n",
      "Epoch [2/300] | Train Loss: 0.3825, Train Acc: 0.8105 | Val Loss: 0.3610, Val Acc: 0.8162 | F1: 0.8088\n",
      "Epoch [3/300] | Train Loss: 0.3693, Train Acc: 0.8224 | Val Loss: 0.3688, Val Acc: 0.8292 | F1: 0.8244\n",
      "Epoch [4/300] | Train Loss: 0.3578, Train Acc: 0.8269 | Val Loss: 0.3527, Val Acc: 0.8211 | F1: 0.8136\n",
      "Epoch [5/300] | Train Loss: 0.3492, Train Acc: 0.8311 | Val Loss: 0.3638, Val Acc: 0.8272 | F1: 0.8112\n",
      "Epoch [6/300] | Train Loss: 0.3430, Train Acc: 0.8346 | Val Loss: 0.3367, Val Acc: 0.8302 | F1: 0.8217\n",
      "Epoch [7/300] | Train Loss: 0.3368, Train Acc: 0.8370 | Val Loss: 0.3330, Val Acc: 0.8422 | F1: 0.8373\n",
      "Epoch [8/300] | Train Loss: 0.3311, Train Acc: 0.8406 | Val Loss: 0.3318, Val Acc: 0.8390 | F1: 0.8269\n",
      "Epoch [9/300] | Train Loss: 0.3293, Train Acc: 0.8414 | Val Loss: 0.3343, Val Acc: 0.8382 | F1: 0.8258\n",
      "Epoch [10/300] | Train Loss: 0.3249, Train Acc: 0.8430 | Val Loss: 0.3340, Val Acc: 0.8400 | F1: 0.8349\n",
      "Epoch [11/300] | Train Loss: 0.3238, Train Acc: 0.8439 | Val Loss: 0.3268, Val Acc: 0.8434 | F1: 0.8385\n",
      "Epoch [12/300] | Train Loss: 0.3234, Train Acc: 0.8444 | Val Loss: 0.3223, Val Acc: 0.8427 | F1: 0.8313\n",
      "Epoch [13/300] | Train Loss: 0.3203, Train Acc: 0.8435 | Val Loss: 0.3245, Val Acc: 0.8402 | F1: 0.8291\n",
      "Epoch [14/300] | Train Loss: 0.3181, Train Acc: 0.8470 | Val Loss: 0.3208, Val Acc: 0.8432 | F1: 0.8368\n",
      "Epoch [15/300] | Train Loss: 0.3153, Train Acc: 0.8472 | Val Loss: 0.3266, Val Acc: 0.8467 | F1: 0.8419\n",
      "Epoch [16/300] | Train Loss: 0.3155, Train Acc: 0.8475 | Val Loss: 0.3229, Val Acc: 0.8390 | F1: 0.8320\n",
      "Epoch [17/300] | Train Loss: 0.3144, Train Acc: 0.8490 | Val Loss: 0.3266, Val Acc: 0.8460 | F1: 0.8343\n",
      "Epoch [18/300] | Train Loss: 0.3123, Train Acc: 0.8504 | Val Loss: 0.3221, Val Acc: 0.8469 | F1: 0.8420\n",
      "Epoch [19/300] | Train Loss: 0.3120, Train Acc: 0.8482 | Val Loss: 0.3150, Val Acc: 0.8508 | F1: 0.8422\n",
      "Epoch [20/300] | Train Loss: 0.3103, Train Acc: 0.8492 | Val Loss: 0.3371, Val Acc: 0.8301 | F1: 0.8239\n",
      "Epoch [21/300] | Train Loss: 0.3100, Train Acc: 0.8513 | Val Loss: 0.3167, Val Acc: 0.8512 | F1: 0.8464\n",
      "Epoch [22/300] | Train Loss: 0.3081, Train Acc: 0.8514 | Val Loss: 0.3178, Val Acc: 0.8510 | F1: 0.8460\n",
      "Epoch [23/300] | Train Loss: 0.3077, Train Acc: 0.8511 | Val Loss: 0.3306, Val Acc: 0.8401 | F1: 0.8350\n",
      "Epoch [24/300] | Train Loss: 0.3077, Train Acc: 0.8496 | Val Loss: 0.3171, Val Acc: 0.8480 | F1: 0.8377\n",
      "Epoch [25/300] | Train Loss: 0.3067, Train Acc: 0.8506 | Val Loss: 0.3160, Val Acc: 0.8436 | F1: 0.8345\n",
      "Epoch [26/300] | Train Loss: 0.3054, Train Acc: 0.8516 | Val Loss: 0.3492, Val Acc: 0.8369 | F1: 0.8245\n",
      "Epoch [27/300] | Train Loss: 0.3037, Train Acc: 0.8525 | Val Loss: 0.3264, Val Acc: 0.8405 | F1: 0.8310\n",
      "Epoch [28/300] | Train Loss: 0.3064, Train Acc: 0.8519 | Val Loss: 0.3212, Val Acc: 0.8487 | F1: 0.8436\n",
      "Epoch [29/300] | Train Loss: 0.3039, Train Acc: 0.8525 | Val Loss: 0.3298, Val Acc: 0.8447 | F1: 0.8400\n",
      "Epoch [30/300] | Train Loss: 0.3037, Train Acc: 0.8524 | Val Loss: 0.3211, Val Acc: 0.8471 | F1: 0.8386\n",
      "Epoch [31/300] | Train Loss: 0.3042, Train Acc: 0.8519 | Val Loss: 0.3259, Val Acc: 0.8462 | F1: 0.8411\n",
      "Epoch [32/300] | Train Loss: 0.3032, Train Acc: 0.8530 | Val Loss: 0.3222, Val Acc: 0.8436 | F1: 0.8331\n",
      "Epoch [33/300] | Train Loss: 0.3028, Train Acc: 0.8539 | Val Loss: 0.3179, Val Acc: 0.8494 | F1: 0.8445\n",
      "Epoch [34/300] | Train Loss: 0.3030, Train Acc: 0.8536 | Val Loss: 0.3242, Val Acc: 0.8516 | F1: 0.8466\n",
      "Epoch [35/300] | Train Loss: 0.3023, Train Acc: 0.8534 | Val Loss: 0.3253, Val Acc: 0.8476 | F1: 0.8421\n",
      "Epoch [36/300] | Train Loss: 0.3015, Train Acc: 0.8541 | Val Loss: 0.3286, Val Acc: 0.8457 | F1: 0.8406\n",
      "Epoch [37/300] | Train Loss: 0.2999, Train Acc: 0.8551 | Val Loss: 0.3212, Val Acc: 0.8471 | F1: 0.8422\n",
      "Epoch [38/300] | Train Loss: 0.3027, Train Acc: 0.8551 | Val Loss: 0.3665, Val Acc: 0.8269 | F1: 0.8134\n",
      "Epoch [39/300] | Train Loss: 0.3007, Train Acc: 0.8549 | Val Loss: 0.3243, Val Acc: 0.8414 | F1: 0.8356\n",
      "Epoch [40/300] | Train Loss: 0.3031, Train Acc: 0.8536 | Val Loss: 0.3186, Val Acc: 0.8516 | F1: 0.8467\n",
      "Epoch [41/300] | Train Loss: 0.3005, Train Acc: 0.8547 | Val Loss: 0.3144, Val Acc: 0.8520 | F1: 0.8470\n",
      "Epoch [42/300] | Train Loss: 0.3015, Train Acc: 0.8556 | Val Loss: 0.3249, Val Acc: 0.8506 | F1: 0.8456\n",
      "Epoch [43/300] | Train Loss: 0.3002, Train Acc: 0.8549 | Val Loss: 0.3167, Val Acc: 0.8508 | F1: 0.8391\n",
      "Epoch [44/300] | Train Loss: 0.2997, Train Acc: 0.8554 | Val Loss: 0.3429, Val Acc: 0.8480 | F1: 0.8386\n",
      "Epoch [45/300] | Train Loss: 0.3013, Train Acc: 0.8538 | Val Loss: 0.3249, Val Acc: 0.8514 | F1: 0.8462\n",
      "Epoch [46/300] | Train Loss: 0.3013, Train Acc: 0.8548 | Val Loss: 0.3241, Val Acc: 0.8493 | F1: 0.8441\n",
      "Epoch [47/300] | Train Loss: 0.2992, Train Acc: 0.8555 | Val Loss: 0.3219, Val Acc: 0.8505 | F1: 0.8415\n",
      "Epoch [48/300] | Train Loss: 0.2990, Train Acc: 0.8556 | Val Loss: 0.3273, Val Acc: 0.8503 | F1: 0.8403\n",
      "Epoch [49/300] | Train Loss: 0.3011, Train Acc: 0.8547 | Val Loss: 0.3309, Val Acc: 0.8450 | F1: 0.8320\n",
      "Epoch [50/300] | Train Loss: 0.3017, Train Acc: 0.8547 | Val Loss: 0.3288, Val Acc: 0.8443 | F1: 0.8302\n",
      "Epoch [51/300] | Train Loss: 0.2998, Train Acc: 0.8546 | Val Loss: 0.3308, Val Acc: 0.8520 | F1: 0.8416\n",
      "Epoch [52/300] | Train Loss: 0.3003, Train Acc: 0.8550 | Val Loss: 0.3322, Val Acc: 0.8485 | F1: 0.8435\n",
      "Epoch [53/300] | Train Loss: 0.3007, Train Acc: 0.8550 | Val Loss: 0.3137, Val Acc: 0.8502 | F1: 0.8454\n",
      "Epoch [54/300] | Train Loss: 0.3004, Train Acc: 0.8558 | Val Loss: 0.3219, Val Acc: 0.8493 | F1: 0.8412\n",
      "Epoch [55/300] | Train Loss: 0.2988, Train Acc: 0.8553 | Val Loss: 0.3160, Val Acc: 0.8503 | F1: 0.8400\n",
      "Epoch [56/300] | Train Loss: 0.2995, Train Acc: 0.8557 | Val Loss: 0.3300, Val Acc: 0.8436 | F1: 0.8375\n",
      "Epoch [57/300] | Train Loss: 0.3018, Train Acc: 0.8538 | Val Loss: 0.3211, Val Acc: 0.8458 | F1: 0.8411\n",
      "Epoch [58/300] | Train Loss: 0.3024, Train Acc: 0.8554 | Val Loss: 0.3217, Val Acc: 0.8506 | F1: 0.8455\n",
      "Epoch [59/300] | Train Loss: 0.3006, Train Acc: 0.8552 | Val Loss: 0.3220, Val Acc: 0.8429 | F1: 0.8308\n",
      "Epoch [60/300] | Train Loss: 0.3017, Train Acc: 0.8541 | Val Loss: 0.3340, Val Acc: 0.8489 | F1: 0.8441\n",
      "Epoch [61/300] | Train Loss: 0.2997, Train Acc: 0.8564 | Val Loss: 0.3298, Val Acc: 0.8494 | F1: 0.8444\n",
      "Epoch [62/300] | Train Loss: 0.2998, Train Acc: 0.8538 | Val Loss: 0.3221, Val Acc: 0.8510 | F1: 0.8417\n",
      "Epoch [63/300] | Train Loss: 0.2992, Train Acc: 0.8551 | Val Loss: 0.3222, Val Acc: 0.8489 | F1: 0.8438\n",
      "Epoch [64/300] | Train Loss: 0.3002, Train Acc: 0.8549 | Val Loss: 0.3376, Val Acc: 0.8494 | F1: 0.8446\n",
      "Epoch [65/300] | Train Loss: 0.3017, Train Acc: 0.8546 | Val Loss: 0.3366, Val Acc: 0.8487 | F1: 0.8426\n",
      "Epoch [66/300] | Train Loss: 0.3005, Train Acc: 0.8548 | Val Loss: 0.3192, Val Acc: 0.8525 | F1: 0.8476\n",
      "Epoch [67/300] | Train Loss: 0.3006, Train Acc: 0.8544 | Val Loss: 0.3382, Val Acc: 0.8468 | F1: 0.8350\n",
      "Epoch [68/300] | Train Loss: 0.3006, Train Acc: 0.8548 | Val Loss: 0.3310, Val Acc: 0.8450 | F1: 0.8329\n",
      "Epoch [69/300] | Train Loss: 0.3014, Train Acc: 0.8542 | Val Loss: 0.3414, Val Acc: 0.8500 | F1: 0.8450\n",
      "Epoch [70/300] | Train Loss: 0.3019, Train Acc: 0.8537 | Val Loss: 0.3332, Val Acc: 0.8535 | F1: 0.8486\n",
      "Epoch [71/300] | Train Loss: 0.3008, Train Acc: 0.8553 | Val Loss: 0.3353, Val Acc: 0.8469 | F1: 0.8361\n",
      "Epoch [72/300] | Train Loss: 0.3017, Train Acc: 0.8554 | Val Loss: 0.3513, Val Acc: 0.8381 | F1: 0.8295\n",
      "Epoch [73/300] | Train Loss: 0.2998, Train Acc: 0.8561 | Val Loss: 0.3254, Val Acc: 0.8480 | F1: 0.8374\n",
      "Epoch [74/300] | Train Loss: 0.3033, Train Acc: 0.8547 | Val Loss: 0.3331, Val Acc: 0.8488 | F1: 0.8401\n",
      "Epoch [75/300] | Train Loss: 0.3009, Train Acc: 0.8541 | Val Loss: 0.3370, Val Acc: 0.8495 | F1: 0.8378\n",
      "Epoch [76/300] | Train Loss: 0.3023, Train Acc: 0.8557 | Val Loss: 0.3377, Val Acc: 0.8478 | F1: 0.8424\n",
      "Epoch [77/300] | Train Loss: 0.3014, Train Acc: 0.8556 | Val Loss: 0.3367, Val Acc: 0.8516 | F1: 0.8464\n",
      "Epoch [78/300] | Train Loss: 0.3005, Train Acc: 0.8554 | Val Loss: 0.3250, Val Acc: 0.8515 | F1: 0.8422\n",
      "Epoch [79/300] | Train Loss: 0.3015, Train Acc: 0.8553 | Val Loss: 0.3215, Val Acc: 0.8526 | F1: 0.8476\n",
      "Epoch [80/300] | Train Loss: 0.3017, Train Acc: 0.8547 | Val Loss: 0.3297, Val Acc: 0.8500 | F1: 0.8448\n",
      "Epoch [81/300] | Train Loss: 0.3034, Train Acc: 0.8547 | Val Loss: 0.3234, Val Acc: 0.8525 | F1: 0.8474\n",
      "Epoch [82/300] | Train Loss: 0.3011, Train Acc: 0.8551 | Val Loss: 0.3294, Val Acc: 0.8497 | F1: 0.8447\n",
      "Epoch [83/300] | Train Loss: 0.3011, Train Acc: 0.8558 | Val Loss: 0.3200, Val Acc: 0.8523 | F1: 0.8474\n",
      "Epoch [84/300] | Train Loss: 0.2993, Train Acc: 0.8553 | Val Loss: 0.3401, Val Acc: 0.8487 | F1: 0.8378\n",
      "Epoch [85/300] | Train Loss: 0.3001, Train Acc: 0.8553 | Val Loss: 0.3209, Val Acc: 0.8516 | F1: 0.8465\n",
      "Epoch [86/300] | Train Loss: 0.2994, Train Acc: 0.8563 | Val Loss: 0.3318, Val Acc: 0.8483 | F1: 0.8398\n",
      "Epoch [87/300] | Train Loss: 0.3023, Train Acc: 0.8564 | Val Loss: 0.3268, Val Acc: 0.8550 | F1: 0.8456\n",
      "Epoch [88/300] | Train Loss: 0.2999, Train Acc: 0.8557 | Val Loss: 0.3536, Val Acc: 0.8445 | F1: 0.8352\n",
      "Epoch [89/300] | Train Loss: 0.3007, Train Acc: 0.8554 | Val Loss: 0.3397, Val Acc: 0.8499 | F1: 0.8451\n",
      "Epoch [90/300] | Train Loss: 0.3011, Train Acc: 0.8566 | Val Loss: 0.3492, Val Acc: 0.8428 | F1: 0.8323\n",
      "Epoch [91/300] | Train Loss: 0.3038, Train Acc: 0.8540 | Val Loss: 0.3375, Val Acc: 0.8481 | F1: 0.8379\n",
      "Epoch [92/300] | Train Loss: 0.3007, Train Acc: 0.8562 | Val Loss: 0.3194, Val Acc: 0.8520 | F1: 0.8471\n",
      "Epoch [93/300] | Train Loss: 0.3022, Train Acc: 0.8548 | Val Loss: 0.3217, Val Acc: 0.8459 | F1: 0.8374\n",
      "Epoch [94/300] | Train Loss: 0.3027, Train Acc: 0.8551 | Val Loss: 0.3226, Val Acc: 0.8493 | F1: 0.8371\n",
      "Epoch [95/300] | Train Loss: 0.3007, Train Acc: 0.8565 | Val Loss: 0.3207, Val Acc: 0.8505 | F1: 0.8451\n",
      "Epoch [96/300] | Train Loss: 0.3017, Train Acc: 0.8554 | Val Loss: 0.3319, Val Acc: 0.8515 | F1: 0.8467\n",
      "Epoch [97/300] | Train Loss: 0.3039, Train Acc: 0.8559 | Val Loss: 0.3316, Val Acc: 0.8469 | F1: 0.8365\n",
      "Epoch [98/300] | Train Loss: 0.3026, Train Acc: 0.8551 | Val Loss: 0.3495, Val Acc: 0.8508 | F1: 0.8400\n",
      "Epoch [99/300] | Train Loss: 0.3039, Train Acc: 0.8535 | Val Loss: 0.3313, Val Acc: 0.8462 | F1: 0.8363\n",
      "Epoch [100/300] | Train Loss: 0.3026, Train Acc: 0.8547 | Val Loss: 0.3273, Val Acc: 0.8518 | F1: 0.8416\n",
      "Epoch [101/300] | Train Loss: 0.3041, Train Acc: 0.8540 | Val Loss: 0.3361, Val Acc: 0.8532 | F1: 0.8447\n",
      "Epoch [102/300] | Train Loss: 0.3029, Train Acc: 0.8537 | Val Loss: 0.3324, Val Acc: 0.8487 | F1: 0.8439\n",
      "Epoch [103/300] | Train Loss: 0.3026, Train Acc: 0.8541 | Val Loss: 0.3471, Val Acc: 0.8451 | F1: 0.8391\n",
      "Epoch [104/300] | Train Loss: 0.3042, Train Acc: 0.8545 | Val Loss: 0.3361, Val Acc: 0.8504 | F1: 0.8454\n",
      "Epoch [105/300] | Train Loss: 0.3020, Train Acc: 0.8547 | Val Loss: 0.3532, Val Acc: 0.8358 | F1: 0.8295\n",
      "Epoch [106/300] | Train Loss: 0.3042, Train Acc: 0.8552 | Val Loss: 0.3285, Val Acc: 0.8434 | F1: 0.8288\n",
      "Epoch [107/300] | Train Loss: 0.3019, Train Acc: 0.8552 | Val Loss: 0.3153, Val Acc: 0.8522 | F1: 0.8461\n",
      "Epoch [108/300] | Train Loss: 0.3019, Train Acc: 0.8547 | Val Loss: 0.3355, Val Acc: 0.8464 | F1: 0.8323\n",
      "Epoch [109/300] | Train Loss: 0.3043, Train Acc: 0.8539 | Val Loss: 0.3452, Val Acc: 0.8495 | F1: 0.8397\n",
      "Epoch [110/300] | Train Loss: 0.3028, Train Acc: 0.8540 | Val Loss: 0.5173, Val Acc: 0.8053 | F1: 0.7800\n",
      "Epoch [111/300] | Train Loss: 0.3011, Train Acc: 0.8566 | Val Loss: 0.3271, Val Acc: 0.8507 | F1: 0.8410\n",
      "Epoch [112/300] | Train Loss: 0.3027, Train Acc: 0.8557 | Val Loss: 0.3313, Val Acc: 0.8500 | F1: 0.8452\n",
      "Epoch [113/300] | Train Loss: 0.3014, Train Acc: 0.8554 | Val Loss: 0.3323, Val Acc: 0.8543 | F1: 0.8492\n",
      "Epoch [114/300] | Train Loss: 0.3036, Train Acc: 0.8555 | Val Loss: 0.3207, Val Acc: 0.8528 | F1: 0.8436\n",
      "Epoch [115/300] | Train Loss: 0.3037, Train Acc: 0.8553 | Val Loss: 0.3325, Val Acc: 0.8504 | F1: 0.8455\n",
      "Epoch [116/300] | Train Loss: 0.3065, Train Acc: 0.8541 | Val Loss: 0.3209, Val Acc: 0.8531 | F1: 0.8445\n",
      "Epoch [117/300] | Train Loss: 0.3025, Train Acc: 0.8551 | Val Loss: 0.3254, Val Acc: 0.8507 | F1: 0.8425\n",
      "Epoch [118/300] | Train Loss: 0.3024, Train Acc: 0.8552 | Val Loss: 0.3384, Val Acc: 0.8518 | F1: 0.8413\n",
      "Epoch [119/300] | Train Loss: 0.3041, Train Acc: 0.8547 | Val Loss: 0.3375, Val Acc: 0.8435 | F1: 0.8320\n",
      "Epoch [120/300] | Train Loss: 0.3031, Train Acc: 0.8545 | Val Loss: 0.3280, Val Acc: 0.8525 | F1: 0.8420\n",
      "Epoch [121/300] | Train Loss: 0.3026, Train Acc: 0.8554 | Val Loss: 0.3263, Val Acc: 0.8522 | F1: 0.8423\n",
      "Epoch [122/300] | Train Loss: 0.3067, Train Acc: 0.8544 | Val Loss: 0.3242, Val Acc: 0.8523 | F1: 0.8470\n",
      "Epoch [123/300] | Train Loss: 0.3040, Train Acc: 0.8558 | Val Loss: 0.3358, Val Acc: 0.8489 | F1: 0.8433\n",
      "Epoch [124/300] | Train Loss: 0.3045, Train Acc: 0.8553 | Val Loss: 0.3571, Val Acc: 0.8386 | F1: 0.8303\n",
      "Epoch [125/300] | Train Loss: 0.3022, Train Acc: 0.8571 | Val Loss: 0.3434, Val Acc: 0.8342 | F1: 0.8186\n",
      "Epoch [126/300] | Train Loss: 0.3053, Train Acc: 0.8541 | Val Loss: 0.3400, Val Acc: 0.8526 | F1: 0.8475\n",
      "Epoch [127/300] | Train Loss: 0.3058, Train Acc: 0.8543 | Val Loss: 0.3224, Val Acc: 0.8522 | F1: 0.8444\n",
      "Epoch [128/300] | Train Loss: 0.3048, Train Acc: 0.8551 | Val Loss: 0.3153, Val Acc: 0.8504 | F1: 0.8451\n",
      "Epoch [129/300] | Train Loss: 0.3038, Train Acc: 0.8551 | Val Loss: 0.3544, Val Acc: 0.8358 | F1: 0.8252\n",
      "Epoch [130/300] | Train Loss: 0.3052, Train Acc: 0.8536 | Val Loss: 0.3354, Val Acc: 0.8527 | F1: 0.8420\n",
      "Epoch [131/300] | Train Loss: 0.3032, Train Acc: 0.8546 | Val Loss: 0.3459, Val Acc: 0.8528 | F1: 0.8477\n",
      "Epoch [132/300] | Train Loss: 0.3045, Train Acc: 0.8544 | Val Loss: 0.3431, Val Acc: 0.8500 | F1: 0.8451\n",
      "Epoch [133/300] | Train Loss: 0.3081, Train Acc: 0.8535 | Val Loss: 0.3306, Val Acc: 0.8509 | F1: 0.8417\n",
      "Epoch [134/300] | Train Loss: 0.3065, Train Acc: 0.8543 | Val Loss: 0.3325, Val Acc: 0.8524 | F1: 0.8448\n",
      "Epoch [135/300] | Train Loss: 0.3077, Train Acc: 0.8543 | Val Loss: 0.3298, Val Acc: 0.8474 | F1: 0.8425\n",
      "Epoch [136/300] | Train Loss: 0.3066, Train Acc: 0.8547 | Val Loss: 0.3447, Val Acc: 0.8518 | F1: 0.8471\n",
      "Epoch [137/300] | Train Loss: 0.3071, Train Acc: 0.8545 | Val Loss: 0.3455, Val Acc: 0.8520 | F1: 0.8469\n",
      "Epoch [138/300] | Train Loss: 0.3040, Train Acc: 0.8550 | Val Loss: 0.3321, Val Acc: 0.8490 | F1: 0.8377\n",
      "Epoch [139/300] | Train Loss: 0.3082, Train Acc: 0.8540 | Val Loss: 0.3367, Val Acc: 0.8491 | F1: 0.8415\n",
      "Epoch [140/300] | Train Loss: 0.3039, Train Acc: 0.8553 | Val Loss: 0.3462, Val Acc: 0.8529 | F1: 0.8440\n",
      "Epoch [141/300] | Train Loss: 0.3094, Train Acc: 0.8539 | Val Loss: 0.3459, Val Acc: 0.8510 | F1: 0.8438\n",
      "Epoch [142/300] | Train Loss: 0.3067, Train Acc: 0.8529 | Val Loss: 0.3427, Val Acc: 0.8483 | F1: 0.8371\n",
      "Epoch [143/300] | Train Loss: 0.3090, Train Acc: 0.8528 | Val Loss: 0.3266, Val Acc: 0.8482 | F1: 0.8384\n",
      "Epoch [144/300] | Train Loss: 0.3089, Train Acc: 0.8534 | Val Loss: 0.3344, Val Acc: 0.8490 | F1: 0.8437\n",
      "Epoch [145/300] | Train Loss: 0.3066, Train Acc: 0.8538 | Val Loss: 0.3289, Val Acc: 0.8508 | F1: 0.8458\n",
      "Epoch [146/300] | Train Loss: 0.3106, Train Acc: 0.8523 | Val Loss: 0.4166, Val Acc: 0.8121 | F1: 0.7999\n",
      "Epoch [147/300] | Train Loss: 0.3059, Train Acc: 0.8541 | Val Loss: 0.3246, Val Acc: 0.8496 | F1: 0.8413\n",
      "Epoch [148/300] | Train Loss: 0.3101, Train Acc: 0.8534 | Val Loss: 0.3263, Val Acc: 0.8503 | F1: 0.8410\n",
      "Epoch [149/300] | Train Loss: 0.3045, Train Acc: 0.8549 | Val Loss: 0.3383, Val Acc: 0.8539 | F1: 0.8447\n",
      "Epoch [150/300] | Train Loss: 0.3083, Train Acc: 0.8537 | Val Loss: 0.3295, Val Acc: 0.8478 | F1: 0.8392\n",
      "Epoch [151/300] | Train Loss: 0.3098, Train Acc: 0.8520 | Val Loss: 0.3562, Val Acc: 0.8389 | F1: 0.8336\n",
      "Epoch [152/300] | Train Loss: 0.3077, Train Acc: 0.8542 | Val Loss: 0.3280, Val Acc: 0.8510 | F1: 0.8420\n",
      "Epoch [153/300] | Train Loss: 0.3086, Train Acc: 0.8530 | Val Loss: 0.3316, Val Acc: 0.8463 | F1: 0.8336\n",
      "Epoch [154/300] | Train Loss: 0.3071, Train Acc: 0.8547 | Val Loss: 0.3360, Val Acc: 0.8476 | F1: 0.8429\n",
      "Epoch [155/300] | Train Loss: 0.3089, Train Acc: 0.8546 | Val Loss: 0.3222, Val Acc: 0.8528 | F1: 0.8463\n",
      "Epoch [156/300] | Train Loss: 0.3108, Train Acc: 0.8536 | Val Loss: 0.3232, Val Acc: 0.8489 | F1: 0.8394\n",
      "Epoch [157/300] | Train Loss: 0.3067, Train Acc: 0.8532 | Val Loss: 0.3488, Val Acc: 0.8469 | F1: 0.8358\n",
      "Epoch [158/300] | Train Loss: 0.3111, Train Acc: 0.8530 | Val Loss: 0.3264, Val Acc: 0.8502 | F1: 0.8455\n",
      "Epoch [159/300] | Train Loss: 0.3076, Train Acc: 0.8539 | Val Loss: 0.3346, Val Acc: 0.8462 | F1: 0.8339\n",
      "Epoch [160/300] | Train Loss: 0.3098, Train Acc: 0.8538 | Val Loss: 0.3265, Val Acc: 0.8475 | F1: 0.8419\n",
      "Epoch [161/300] | Train Loss: 0.3091, Train Acc: 0.8538 | Val Loss: 0.3363, Val Acc: 0.8477 | F1: 0.8427\n",
      "Epoch [162/300] | Train Loss: 0.3113, Train Acc: 0.8524 | Val Loss: 0.3378, Val Acc: 0.8474 | F1: 0.8348\n",
      "Epoch [163/300] | Train Loss: 0.3085, Train Acc: 0.8516 | Val Loss: 0.3293, Val Acc: 0.8503 | F1: 0.8395\n",
      "Epoch [164/300] | Train Loss: 0.3079, Train Acc: 0.8547 | Val Loss: 0.3245, Val Acc: 0.8488 | F1: 0.8400\n",
      "Epoch [165/300] | Train Loss: 0.3087, Train Acc: 0.8533 | Val Loss: 0.3352, Val Acc: 0.8466 | F1: 0.8419\n",
      "Epoch [166/300] | Train Loss: 0.3113, Train Acc: 0.8518 | Val Loss: 0.3357, Val Acc: 0.8537 | F1: 0.8487\n",
      "Epoch [167/300] | Train Loss: 0.3097, Train Acc: 0.8530 | Val Loss: 0.3672, Val Acc: 0.8365 | F1: 0.8307\n",
      "Epoch [168/300] | Train Loss: 0.3113, Train Acc: 0.8520 | Val Loss: 0.3308, Val Acc: 0.8450 | F1: 0.8311\n",
      "Epoch [169/300] | Train Loss: 0.3116, Train Acc: 0.8522 | Val Loss: 0.3171, Val Acc: 0.8517 | F1: 0.8426\n",
      "Epoch [170/300] | Train Loss: 0.3095, Train Acc: 0.8525 | Val Loss: 0.3272, Val Acc: 0.8508 | F1: 0.8430\n",
      "Epoch [171/300] | Train Loss: 0.3080, Train Acc: 0.8539 | Val Loss: 0.3210, Val Acc: 0.8487 | F1: 0.8439\n",
      "Epoch [172/300] | Train Loss: 0.3138, Train Acc: 0.8523 | Val Loss: 0.3216, Val Acc: 0.8495 | F1: 0.8449\n",
      "Epoch [173/300] | Train Loss: 0.3111, Train Acc: 0.8511 | Val Loss: 0.3607, Val Acc: 0.8390 | F1: 0.8228\n",
      "Epoch [174/300] | Train Loss: 0.3121, Train Acc: 0.8530 | Val Loss: 0.3362, Val Acc: 0.8503 | F1: 0.8433\n",
      "Epoch [175/300] | Train Loss: 0.3128, Train Acc: 0.8522 | Val Loss: 0.3328, Val Acc: 0.8480 | F1: 0.8425\n",
      "Epoch [176/300] | Train Loss: 0.3086, Train Acc: 0.8524 | Val Loss: 0.3309, Val Acc: 0.8441 | F1: 0.8390\n",
      "Epoch [177/300] | Train Loss: 0.3119, Train Acc: 0.8514 | Val Loss: 0.3375, Val Acc: 0.8493 | F1: 0.8393\n",
      "Epoch [178/300] | Train Loss: 0.3103, Train Acc: 0.8518 | Val Loss: 0.3576, Val Acc: 0.8356 | F1: 0.8296\n",
      "Epoch [179/300] | Train Loss: 0.3118, Train Acc: 0.8516 | Val Loss: 0.3199, Val Acc: 0.8557 | F1: 0.8498\n",
      "Epoch [180/300] | Train Loss: 0.3107, Train Acc: 0.8531 | Val Loss: 0.3395, Val Acc: 0.8493 | F1: 0.8384\n",
      "Epoch [181/300] | Train Loss: 0.3115, Train Acc: 0.8522 | Val Loss: 0.3443, Val Acc: 0.8430 | F1: 0.8379\n",
      "Epoch [182/300] | Train Loss: 0.3095, Train Acc: 0.8528 | Val Loss: 0.3282, Val Acc: 0.8441 | F1: 0.8356\n",
      "Epoch [183/300] | Train Loss: 0.3108, Train Acc: 0.8526 | Val Loss: 0.3257, Val Acc: 0.8510 | F1: 0.8414\n",
      "Epoch [184/300] | Train Loss: 0.3100, Train Acc: 0.8531 | Val Loss: 0.3385, Val Acc: 0.8523 | F1: 0.8419\n",
      "Epoch [185/300] | Train Loss: 0.3140, Train Acc: 0.8510 | Val Loss: 0.3356, Val Acc: 0.8437 | F1: 0.8307\n",
      "Epoch [186/300] | Train Loss: 0.3124, Train Acc: 0.8527 | Val Loss: 0.3384, Val Acc: 0.8493 | F1: 0.8442\n",
      "Epoch [187/300] | Train Loss: 0.3173, Train Acc: 0.8510 | Val Loss: 0.3341, Val Acc: 0.8502 | F1: 0.8454\n",
      "Epoch [188/300] | Train Loss: 0.3128, Train Acc: 0.8514 | Val Loss: 0.3257, Val Acc: 0.8483 | F1: 0.8436\n",
      "Epoch [189/300] | Train Loss: 0.3171, Train Acc: 0.8498 | Val Loss: 0.3241, Val Acc: 0.8453 | F1: 0.8406\n",
      "Epoch [190/300] | Train Loss: 0.3185, Train Acc: 0.8498 | Val Loss: 0.3252, Val Acc: 0.8494 | F1: 0.8396\n",
      "Epoch [191/300] | Train Loss: 0.3136, Train Acc: 0.8510 | Val Loss: 0.3168, Val Acc: 0.8502 | F1: 0.8453\n",
      "Epoch [192/300] | Train Loss: 0.3138, Train Acc: 0.8509 | Val Loss: 0.3391, Val Acc: 0.8467 | F1: 0.8419\n",
      "Epoch [193/300] | Train Loss: 0.3115, Train Acc: 0.8521 | Val Loss: 0.3493, Val Acc: 0.8480 | F1: 0.8350\n",
      "Epoch [194/300] | Train Loss: 0.3109, Train Acc: 0.8526 | Val Loss: 0.3375, Val Acc: 0.8474 | F1: 0.8423\n",
      "Epoch [195/300] | Train Loss: 0.3133, Train Acc: 0.8525 | Val Loss: 0.3305, Val Acc: 0.8476 | F1: 0.8367\n",
      "Epoch [196/300] | Train Loss: 0.3136, Train Acc: 0.8514 | Val Loss: 0.3364, Val Acc: 0.8496 | F1: 0.8446\n",
      "Epoch [197/300] | Train Loss: 0.3147, Train Acc: 0.8530 | Val Loss: 0.3261, Val Acc: 0.8502 | F1: 0.8450\n",
      "Epoch [198/300] | Train Loss: 0.3120, Train Acc: 0.8532 | Val Loss: 0.3483, Val Acc: 0.8329 | F1: 0.8234\n",
      "Epoch [199/300] | Train Loss: 0.3147, Train Acc: 0.8514 | Val Loss: 0.3204, Val Acc: 0.8519 | F1: 0.8468\n",
      "Epoch [200/300] | Train Loss: 0.3160, Train Acc: 0.8518 | Val Loss: 0.3719, Val Acc: 0.8415 | F1: 0.8332\n",
      "Epoch [201/300] | Train Loss: 0.3124, Train Acc: 0.8510 | Val Loss: 0.3274, Val Acc: 0.8516 | F1: 0.8429\n",
      "Epoch [202/300] | Train Loss: 0.3164, Train Acc: 0.8520 | Val Loss: 0.3240, Val Acc: 0.8458 | F1: 0.8334\n",
      "Epoch [203/300] | Train Loss: 0.3179, Train Acc: 0.8505 | Val Loss: 0.3266, Val Acc: 0.8471 | F1: 0.8424\n",
      "Epoch [204/300] | Train Loss: 0.3175, Train Acc: 0.8510 | Val Loss: 0.3276, Val Acc: 0.8472 | F1: 0.8422\n",
      "Epoch [205/300] | Train Loss: 0.3145, Train Acc: 0.8513 | Val Loss: 0.3311, Val Acc: 0.8458 | F1: 0.8324\n",
      "Epoch [206/300] | Train Loss: 0.3203, Train Acc: 0.8495 | Val Loss: 0.3333, Val Acc: 0.8460 | F1: 0.8348\n",
      "Epoch [207/300] | Train Loss: 0.3195, Train Acc: 0.8505 | Val Loss: 0.3300, Val Acc: 0.8454 | F1: 0.8352\n",
      "Epoch [208/300] | Train Loss: 0.3126, Train Acc: 0.8515 | Val Loss: 0.3262, Val Acc: 0.8521 | F1: 0.8470\n",
      "Epoch [209/300] | Train Loss: 0.3180, Train Acc: 0.8510 | Val Loss: 0.3296, Val Acc: 0.8484 | F1: 0.8437\n",
      "Epoch [210/300] | Train Loss: 0.3193, Train Acc: 0.8505 | Val Loss: 0.3497, Val Acc: 0.8401 | F1: 0.8345\n",
      "Epoch [211/300] | Train Loss: 0.3138, Train Acc: 0.8522 | Val Loss: 0.3200, Val Acc: 0.8505 | F1: 0.8407\n",
      "Epoch [212/300] | Train Loss: 0.3155, Train Acc: 0.8517 | Val Loss: 0.3237, Val Acc: 0.8475 | F1: 0.8371\n",
      "Epoch [213/300] | Train Loss: 0.3171, Train Acc: 0.8509 | Val Loss: 0.3294, Val Acc: 0.8505 | F1: 0.8457\n",
      "Epoch [214/300] | Train Loss: 0.3171, Train Acc: 0.8514 | Val Loss: 0.3205, Val Acc: 0.8520 | F1: 0.8439\n",
      "Epoch [215/300] | Train Loss: 0.3200, Train Acc: 0.8509 | Val Loss: 0.3298, Val Acc: 0.8488 | F1: 0.8440\n",
      "Epoch [216/300] | Train Loss: 0.3208, Train Acc: 0.8480 | Val Loss: 0.3482, Val Acc: 0.8468 | F1: 0.8420\n",
      "Epoch [217/300] | Train Loss: 0.3189, Train Acc: 0.8505 | Val Loss: 0.3284, Val Acc: 0.8456 | F1: 0.8404\n",
      "Epoch [218/300] | Train Loss: 0.3209, Train Acc: 0.8491 | Val Loss: 0.3261, Val Acc: 0.8510 | F1: 0.8419\n",
      "Epoch [219/300] | Train Loss: 0.3179, Train Acc: 0.8500 | Val Loss: 0.3337, Val Acc: 0.8506 | F1: 0.8417\n",
      "Epoch [220/300] | Train Loss: 0.3239, Train Acc: 0.8494 | Val Loss: 0.3269, Val Acc: 0.8495 | F1: 0.8446\n",
      "Epoch [221/300] | Train Loss: 0.3206, Train Acc: 0.8491 | Val Loss: 0.3326, Val Acc: 0.8444 | F1: 0.8331\n",
      "Epoch [222/300] | Train Loss: 0.3187, Train Acc: 0.8486 | Val Loss: 0.3385, Val Acc: 0.8477 | F1: 0.8429\n",
      "Epoch [223/300] | Train Loss: 0.3200, Train Acc: 0.8491 | Val Loss: 0.3444, Val Acc: 0.8485 | F1: 0.8436\n",
      "Epoch [224/300] | Train Loss: 0.3207, Train Acc: 0.8498 | Val Loss: 0.3327, Val Acc: 0.8497 | F1: 0.8449\n",
      "Epoch [225/300] | Train Loss: 0.3241, Train Acc: 0.8486 | Val Loss: 0.3431, Val Acc: 0.8380 | F1: 0.8312\n",
      "Epoch [226/300] | Train Loss: 0.3210, Train Acc: 0.8493 | Val Loss: 0.3432, Val Acc: 0.8454 | F1: 0.8341\n",
      "Epoch [227/300] | Train Loss: 0.3244, Train Acc: 0.8480 | Val Loss: 0.3430, Val Acc: 0.8478 | F1: 0.8411\n",
      "Epoch [228/300] | Train Loss: 0.3227, Train Acc: 0.8489 | Val Loss: 0.3549, Val Acc: 0.8468 | F1: 0.8367\n",
      "Epoch [229/300] | Train Loss: 0.3220, Train Acc: 0.8492 | Val Loss: 0.4244, Val Acc: 0.8363 | F1: 0.8237\n",
      "Epoch [230/300] | Train Loss: 0.3237, Train Acc: 0.8480 | Val Loss: 0.3277, Val Acc: 0.8502 | F1: 0.8402\n",
      "Epoch [231/300] | Train Loss: 0.3229, Train Acc: 0.8484 | Val Loss: 0.3272, Val Acc: 0.8451 | F1: 0.8329\n",
      "Epoch [232/300] | Train Loss: 0.3216, Train Acc: 0.8478 | Val Loss: 0.3347, Val Acc: 0.8488 | F1: 0.8441\n",
      "Epoch [233/300] | Train Loss: 0.3238, Train Acc: 0.8487 | Val Loss: 0.3624, Val Acc: 0.8249 | F1: 0.8145\n",
      "Epoch [234/300] | Train Loss: 0.3243, Train Acc: 0.8479 | Val Loss: 0.3215, Val Acc: 0.8496 | F1: 0.8393\n",
      "Epoch [235/300] | Train Loss: 0.3265, Train Acc: 0.8472 | Val Loss: 0.3275, Val Acc: 0.8477 | F1: 0.8375\n",
      "Epoch [236/300] | Train Loss: 0.3235, Train Acc: 0.8477 | Val Loss: 0.3305, Val Acc: 0.8485 | F1: 0.8432\n",
      "Epoch [237/300] | Train Loss: 0.3226, Train Acc: 0.8497 | Val Loss: 0.3287, Val Acc: 0.8382 | F1: 0.8307\n",
      "Epoch [238/300] | Train Loss: 0.3224, Train Acc: 0.8490 | Val Loss: 0.3474, Val Acc: 0.8490 | F1: 0.8444\n",
      "Epoch [239/300] | Train Loss: 0.3240, Train Acc: 0.8478 | Val Loss: 0.3346, Val Acc: 0.8435 | F1: 0.8321\n",
      "Epoch [240/300] | Train Loss: 0.3223, Train Acc: 0.8490 | Val Loss: 0.3416, Val Acc: 0.8466 | F1: 0.8358\n",
      "Epoch [241/300] | Train Loss: 0.3267, Train Acc: 0.8465 | Val Loss: 0.3225, Val Acc: 0.8465 | F1: 0.8363\n",
      "Epoch [242/300] | Train Loss: 0.3235, Train Acc: 0.8480 | Val Loss: 0.3346, Val Acc: 0.8411 | F1: 0.8253\n",
      "Epoch [243/300] | Train Loss: 0.3213, Train Acc: 0.8479 | Val Loss: 0.3223, Val Acc: 0.8500 | F1: 0.8425\n",
      "Epoch [244/300] | Train Loss: 0.3253, Train Acc: 0.8466 | Val Loss: 0.3403, Val Acc: 0.8388 | F1: 0.8318\n",
      "Epoch [245/300] | Train Loss: 0.3282, Train Acc: 0.8479 | Val Loss: 0.3479, Val Acc: 0.8462 | F1: 0.8414\n",
      "Epoch [246/300] | Train Loss: 0.3281, Train Acc: 0.8455 | Val Loss: 0.3448, Val Acc: 0.8415 | F1: 0.8291\n",
      "Epoch [247/300] | Train Loss: 0.3265, Train Acc: 0.8460 | Val Loss: 0.3273, Val Acc: 0.8475 | F1: 0.8428\n",
      "Epoch [248/300] | Train Loss: 0.3228, Train Acc: 0.8484 | Val Loss: 0.3297, Val Acc: 0.8476 | F1: 0.8430\n",
      "Epoch [249/300] | Train Loss: 0.3264, Train Acc: 0.8460 | Val Loss: 0.3246, Val Acc: 0.8455 | F1: 0.8340\n",
      "Epoch [250/300] | Train Loss: 0.3284, Train Acc: 0.8453 | Val Loss: 0.3421, Val Acc: 0.8370 | F1: 0.8253\n",
      "Epoch [251/300] | Train Loss: 0.3278, Train Acc: 0.8469 | Val Loss: 0.3450, Val Acc: 0.8462 | F1: 0.8371\n",
      "Epoch [252/300] | Train Loss: 0.3255, Train Acc: 0.8477 | Val Loss: 0.4142, Val Acc: 0.8402 | F1: 0.8298\n",
      "Epoch [253/300] | Train Loss: 0.3340, Train Acc: 0.8462 | Val Loss: 0.3352, Val Acc: 0.8421 | F1: 0.8294\n",
      "Epoch [254/300] | Train Loss: 0.3286, Train Acc: 0.8451 | Val Loss: 0.3333, Val Acc: 0.8472 | F1: 0.8368\n",
      "Epoch [255/300] | Train Loss: 0.3279, Train Acc: 0.8467 | Val Loss: 0.3308, Val Acc: 0.8474 | F1: 0.8370\n",
      "Epoch [256/300] | Train Loss: 0.3281, Train Acc: 0.8461 | Val Loss: 0.3279, Val Acc: 0.8502 | F1: 0.8423\n",
      "Epoch [257/300] | Train Loss: 0.3261, Train Acc: 0.8463 | Val Loss: 0.3272, Val Acc: 0.8455 | F1: 0.8335\n",
      "Epoch [258/300] | Train Loss: 0.3297, Train Acc: 0.8452 | Val Loss: 0.4571, Val Acc: 0.8205 | F1: 0.7952\n",
      "Epoch [259/300] | Train Loss: 0.3267, Train Acc: 0.8460 | Val Loss: 0.3318, Val Acc: 0.8465 | F1: 0.8417\n",
      "Epoch [260/300] | Train Loss: 0.3283, Train Acc: 0.8465 | Val Loss: 0.3650, Val Acc: 0.8357 | F1: 0.8300\n",
      "Epoch [261/300] | Train Loss: 0.3271, Train Acc: 0.8480 | Val Loss: 0.3336, Val Acc: 0.8408 | F1: 0.8359\n",
      "Epoch [262/300] | Train Loss: 0.3261, Train Acc: 0.8469 | Val Loss: 0.3602, Val Acc: 0.8203 | F1: 0.7876\n",
      "Epoch [263/300] | Train Loss: 0.3263, Train Acc: 0.8472 | Val Loss: 0.3311, Val Acc: 0.8455 | F1: 0.8336\n",
      "Epoch [264/300] | Train Loss: 0.3268, Train Acc: 0.8463 | Val Loss: 0.3260, Val Acc: 0.8434 | F1: 0.8279\n",
      "Epoch [265/300] | Train Loss: 0.3284, Train Acc: 0.8470 | Val Loss: 0.3341, Val Acc: 0.8416 | F1: 0.8371\n",
      "Epoch [266/300] | Train Loss: 0.3277, Train Acc: 0.8442 | Val Loss: 0.3241, Val Acc: 0.8472 | F1: 0.8401\n",
      "Epoch [267/300] | Train Loss: 0.3300, Train Acc: 0.8446 | Val Loss: 0.3305, Val Acc: 0.8447 | F1: 0.8330\n",
      "Epoch [268/300] | Train Loss: 0.3296, Train Acc: 0.8458 | Val Loss: 0.3226, Val Acc: 0.8488 | F1: 0.8438\n",
      "Epoch [269/300] | Train Loss: 0.3264, Train Acc: 0.8478 | Val Loss: 0.3375, Val Acc: 0.8414 | F1: 0.8363\n",
      "Epoch [270/300] | Train Loss: 0.3336, Train Acc: 0.8428 | Val Loss: 0.3264, Val Acc: 0.8442 | F1: 0.8395\n",
      "Epoch [271/300] | Train Loss: 0.3294, Train Acc: 0.8447 | Val Loss: 0.3239, Val Acc: 0.8451 | F1: 0.8393\n",
      "Epoch [272/300] | Train Loss: 0.3325, Train Acc: 0.8454 | Val Loss: 0.3294, Val Acc: 0.8413 | F1: 0.8272\n",
      "Epoch [273/300] | Train Loss: 0.3287, Train Acc: 0.8460 | Val Loss: 0.3427, Val Acc: 0.8371 | F1: 0.8232\n",
      "Epoch [274/300] | Train Loss: 0.3266, Train Acc: 0.8458 | Val Loss: 0.3288, Val Acc: 0.8466 | F1: 0.8399\n",
      "Epoch [275/300] | Train Loss: 0.3298, Train Acc: 0.8467 | Val Loss: 0.3268, Val Acc: 0.8468 | F1: 0.8354\n",
      "Epoch [276/300] | Train Loss: 0.3319, Train Acc: 0.8461 | Val Loss: 0.3301, Val Acc: 0.8469 | F1: 0.8418\n",
      "Epoch [277/300] | Train Loss: 0.3279, Train Acc: 0.8460 | Val Loss: 0.3245, Val Acc: 0.8436 | F1: 0.8320\n",
      "Epoch [278/300] | Train Loss: 0.3283, Train Acc: 0.8452 | Val Loss: 0.3241, Val Acc: 0.8471 | F1: 0.8424\n",
      "Epoch [279/300] | Train Loss: 0.3274, Train Acc: 0.8476 | Val Loss: 0.3395, Val Acc: 0.8487 | F1: 0.8400\n",
      "Epoch [280/300] | Train Loss: 0.3324, Train Acc: 0.8465 | Val Loss: 0.3251, Val Acc: 0.8473 | F1: 0.8426\n",
      "Epoch [281/300] | Train Loss: 0.3337, Train Acc: 0.8463 | Val Loss: 0.3273, Val Acc: 0.8454 | F1: 0.8408\n",
      "Epoch [282/300] | Train Loss: 0.3295, Train Acc: 0.8457 | Val Loss: 0.3324, Val Acc: 0.8495 | F1: 0.8443\n",
      "Epoch [283/300] | Train Loss: 0.3332, Train Acc: 0.8465 | Val Loss: 0.3237, Val Acc: 0.8487 | F1: 0.8404\n",
      "Epoch [284/300] | Train Loss: 0.3259, Train Acc: 0.8472 | Val Loss: 0.3359, Val Acc: 0.8455 | F1: 0.8330\n",
      "Epoch [285/300] | Train Loss: 0.3288, Train Acc: 0.8452 | Val Loss: 0.3988, Val Acc: 0.8395 | F1: 0.8353\n",
      "Epoch [286/300] | Train Loss: 0.3260, Train Acc: 0.8473 | Val Loss: 0.3410, Val Acc: 0.8455 | F1: 0.8403\n",
      "Epoch [287/300] | Train Loss: 0.3270, Train Acc: 0.8459 | Val Loss: 0.3326, Val Acc: 0.8481 | F1: 0.8432\n",
      "Epoch [288/300] | Train Loss: 0.3317, Train Acc: 0.8456 | Val Loss: 0.3345, Val Acc: 0.8489 | F1: 0.8382\n",
      "Epoch [289/300] | Train Loss: 0.3287, Train Acc: 0.8487 | Val Loss: 0.3485, Val Acc: 0.8341 | F1: 0.8236\n",
      "Epoch [290/300] | Train Loss: 0.3249, Train Acc: 0.8483 | Val Loss: 0.3376, Val Acc: 0.8458 | F1: 0.8361\n",
      "Epoch [291/300] | Train Loss: 0.3323, Train Acc: 0.8464 | Val Loss: 0.3549, Val Acc: 0.8455 | F1: 0.8405\n",
      "Epoch [292/300] | Train Loss: 0.3266, Train Acc: 0.8468 | Val Loss: 0.3482, Val Acc: 0.8388 | F1: 0.8297\n",
      "Epoch [293/300] | Train Loss: 0.3322, Train Acc: 0.8455 | Val Loss: 0.3295, Val Acc: 0.8469 | F1: 0.8418\n",
      "Epoch [294/300] | Train Loss: 0.3278, Train Acc: 0.8475 | Val Loss: 0.3423, Val Acc: 0.8456 | F1: 0.8411\n",
      "Epoch [295/300] | Train Loss: 0.3403, Train Acc: 0.8442 | Val Loss: 0.3394, Val Acc: 0.8424 | F1: 0.8377\n",
      "Epoch [296/300] | Train Loss: 0.3345, Train Acc: 0.8435 | Val Loss: 0.3410, Val Acc: 0.8431 | F1: 0.8383\n",
      "Epoch [297/300] | Train Loss: 0.3408, Train Acc: 0.8407 | Val Loss: 0.3602, Val Acc: 0.8443 | F1: 0.8389\n",
      "Epoch [298/300] | Train Loss: 0.3341, Train Acc: 0.8443 | Val Loss: 0.3365, Val Acc: 0.8382 | F1: 0.8288\n",
      "Epoch [299/300] | Train Loss: 0.3332, Train Acc: 0.8459 | Val Loss: 0.3269, Val Acc: 0.8528 | F1: 0.8467\n",
      "Epoch [300/300] | Train Loss: 0.3380, Train Acc: 0.8422 | Val Loss: 0.3435, Val Acc: 0.8413 | F1: 0.8302\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 300\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "    all_preds, all_labels = [], []     # NEW\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            val_loss += loss.item() * X_batch.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            val_correct += (preds == y_batch).sum().item()\n",
    "            val_total += y_batch.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    val_loss /= val_total\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    # Extra Metrics\n",
    "    precision = precision_score(all_labels, all_preds, average=\"macro\")\n",
    "    recall = recall_score(all_labels, all_preds, average=\"macro\")\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "    # Store metrics\n",
    "    history[\"epoch\"].append(epoch+1)\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    history[\"precision\"].append(precision)\n",
    "    history[\"recall\"].append(recall)\n",
    "    history[\"f1\"].append(f1)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] | \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} | \"\n",
    "          f\"F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ed369f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9381    0.8993    0.9183      6000\n",
      "           1     0.7903    0.5073    0.6179      4948\n",
      "           2     0.6582    0.8752    0.7513      5359\n",
      "           3     0.9906    0.9888    0.9897      6000\n",
      "           4     0.9841    0.9288    0.9556      5589\n",
      "           5     0.8687    0.9524    0.9086      5993\n",
      "\n",
      "    accuracy                         0.8684     33889\n",
      "   macro avg     0.8717    0.8586    0.8569     33889\n",
      "weighted avg     0.8769    0.8684    0.8651     33889\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_val, y_val in test_loader:\n",
    "        X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "        outputs = model(X_val)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_val.cpu().numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429748d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66f13bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(history)\n",
    "df.to_csv(\"../metrics/torch/rclnet-multiheadselfattn.csv\", index=False)\n",
    "torch.save(model.state_dict(), \"../artifacts/torch/rclnet-MULTI_cuda.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
